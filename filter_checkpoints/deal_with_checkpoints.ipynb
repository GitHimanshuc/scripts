{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import shutil\n",
    "import h5py\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_lm(l,m,df):\n",
    "  L_max = int(np.sqrt(df.attrs['Extents'][1]/2))\n",
    "  if m >= 0:\n",
    "    idx = L_max*l +m\n",
    "  else:\n",
    "    idx = L_max*l + np.abs(m) + L_max**2\n",
    "  return idx\n",
    "\n",
    "def pow_in_l(l,df):\n",
    "  # For each l compute the power by taking L2 norms over all m\n",
    "  indices_for_m = [get_lm(l,m,df) for m in range(-l,l+1)]\n",
    "  L2_normed = np.linalg.norm(df[indices_for_m], axis=1)\n",
    "  return L2_normed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_spec_into_pd(file_path: Path):\n",
    "\n",
    "    with file_path.open(\"r\") as f:\n",
    "        file_content = f.read()\n",
    "\n",
    "    # Extract metadata\n",
    "    time_value = float(re.search(r\"Time\\[0\\]\\s*=\\s*([\\d.]+)\", file_content).group(1))\n",
    "    extents = tuple(\n",
    "        map(int, re.search(r\"Extents\\s*=\\s*\\((\\d+),(\\d+)\\)\", file_content).groups())\n",
    "    )\n",
    "\n",
    "    # Extract data rows\n",
    "    data_rows = re.findall(r\"\\(:,(\\d+)\\):\\s*([\\d\\s\\.,eE+-]+)\", file_content)\n",
    "\n",
    "    # Convert data into a list of lists\n",
    "    data = {}\n",
    "    for row in data_rows:\n",
    "        index = int(row[0])\n",
    "        values = list(map(float, row[1].split(\",\")))\n",
    "        data[index] = values\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Add metadata as attributes (optional)\n",
    "    df.attrs[\"Time\"] = time_value\n",
    "    df.attrs[\"Extents\"] = extents\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_phy_into_pd(file_path: Path):\n",
    "\n",
    "    with file_path.open(\"r\") as f:\n",
    "        file_content = f.read()\n",
    "\n",
    "    # Extract metadata\n",
    "    time_value = float(re.search(r\"Time\\[0\\]\\s*=\\s*([\\d.]+)\", file_content).group(1))\n",
    "    extents = tuple(\n",
    "        map(\n",
    "            int,\n",
    "            re.search(r\"Extents\\s*=\\s*\\((\\d+),(\\d+),(\\d+)\\)\", file_content).groups(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Extract data rows\n",
    "    data_rows = re.findall(r\"\\(:,(\\d+),(\\d+)\\):\\s*(.*)\", file_content)\n",
    "\n",
    "    # Convert data into a list of lists\n",
    "    data = {}\n",
    "    for row in data_rows:\n",
    "        l = int(row[0])\n",
    "        m = int(row[1])\n",
    "        values = list(map(float, row[2].split(\",\")))\n",
    "        data[f\"{l},{m}\"] = values\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Add metadata as attributes (optional)\n",
    "    df.attrs[\"Time\"] = time_value\n",
    "    df.attrs[\"Extents\"] = extents\n",
    "\n",
    "    return df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, Optional, Union\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "\n",
    "def estimate_noise_level(\n",
    "    coefficients: ArrayLike, window_size: int = 5, derivative_threshold: float = 0.1\n",
    ") -> Tuple[Optional[float], Optional[np.ndarray]]:\n",
    "    # Input validation\n",
    "    coefficients = np.asarray(coefficients)\n",
    "    if coefficients.size == 0:\n",
    "        raise ValueError(\"Coefficients array cannot be empty\")\n",
    "    if window_size < 2 or window_size > len(coefficients):\n",
    "        raise ValueError(\"Invalid window size\")\n",
    "\n",
    "    # Take absolute values of coefficients\n",
    "    coeff_abs = np.abs(coefficients)\n",
    "\n",
    "    # Compute moving average of log coefficients\n",
    "    log_coeffs = np.log10(coeff_abs + 1e-16)  # Add small number to avoid log(0)\n",
    "    moving_avg = np.convolve(\n",
    "        log_coeffs, np.ones(window_size) / window_size, mode=\"valid\"\n",
    "    )\n",
    "\n",
    "    # Find where the derivative becomes small\n",
    "    derivatives = np.diff(moving_avg)\n",
    "    plateau_idx = np.where(np.abs(derivatives) < derivative_threshold)[0]\n",
    "\n",
    "    # Group consecutive indices to find the longest plateau\n",
    "    if len(plateau_idx) > 0:\n",
    "        # Find gaps in plateau indices\n",
    "        gaps = np.diff(plateau_idx) > 1\n",
    "        # Get start indices of each group\n",
    "        group_starts = np.concatenate([[0], np.where(gaps)[0] + 1])\n",
    "        # Get end indices of each group\n",
    "        group_ends = np.concatenate([np.where(gaps)[0], [len(plateau_idx) - 1]])\n",
    "        # Find the longest group\n",
    "        longest_group = np.argmax(group_ends - group_starts)\n",
    "        start_idx = plateau_idx[group_starts[longest_group]]\n",
    "        end_idx = plateau_idx[group_ends[longest_group]] + 1\n",
    "\n",
    "        # Calculate noise level from the longest plateau\n",
    "        noise_level = 10 ** np.mean(log_coeffs[start_idx:end_idx])\n",
    "        # Return noise indices adjusted for the window size effect\n",
    "        noise_indices = np.arange(\n",
    "            start_idx, min(end_idx + window_size, len(coefficients))\n",
    "        )\n",
    "\n",
    "        return noise_level, noise_indices\n",
    "\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def estimate_noise_level(\n",
    "    coefficients: ArrayLike, window_size: int = 5, derivative_threshold: float = 0.1\n",
    ") -> Tuple[Optional[float], Optional[np.ndarray]]:\n",
    "    coeff_abs = np.abs(coefficients)\n",
    "\n",
    "    log_coeffs = np.log10(coeff_abs + 1e-16)  # Add small number to avoid log(0)\n",
    "    moving_avg = np.convolve(\n",
    "        log_coeffs, np.ones(window_size) / window_size, mode=\"valid\"\n",
    "    )\n",
    "    derivatives = np.diff(moving_avg)\n",
    "    plateau_idx = np.where(np.abs(derivatives) < derivative_threshold)[0]\n",
    "\n",
    "    # Group consecutive indices to find the longest plateau\n",
    "    if len(plateau_idx) > 0:\n",
    "        # Find gaps in plateau indices\n",
    "        gaps = np.diff(plateau_idx) > 1\n",
    "        # Get start indices of each group\n",
    "        group_starts = np.concatenate([[0], np.where(gaps)[0] + 1])\n",
    "        # Get end indices of each group\n",
    "        group_ends = np.concatenate([np.where(gaps)[0], [len(plateau_idx) - 1]])\n",
    "        # Find the longest group\n",
    "        longest_group = np.argmax(group_ends - group_starts)\n",
    "        start_idx = plateau_idx[group_starts[longest_group]]\n",
    "        end_idx = plateau_idx[group_ends[longest_group]] + 1\n",
    "\n",
    "        # Calculate noise level from the longest plateau\n",
    "        noise_level = 10 ** np.mean(log_coeffs[start_idx:end_idx])\n",
    "        # Return noise indices adjusted for the window size effect\n",
    "        noise_indices = np.arange(\n",
    "            start_idx, min(end_idx + window_size, len(coefficients))\n",
    "        )\n",
    "\n",
    "        return noise_level, noise_indices\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_h5_file(file_path: str) -> Dict[str, Any]:\n",
    "    def read_group(group) -> Dict[str, Any]:\n",
    "        result = {}\n",
    "        # Read all datasets in current group\n",
    "        for name, item in group.items():\n",
    "            if isinstance(item, h5py.Dataset):\n",
    "                # Convert dataset to numpy array\n",
    "                result[name] = item[()]\n",
    "            elif isinstance(item, h5py.Group):\n",
    "                # Recursively read nested group\n",
    "                result[name] = read_group(item)\n",
    "        return result\n",
    "\n",
    "    # Check if file exists\n",
    "    if not Path(file_path).exists():\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist\")\n",
    "    try:\n",
    "        with h5py.File(file_path, \"r\") as f:\n",
    "            # Read all contents\n",
    "            data = {}\n",
    "            # Read main groups\n",
    "            for group_name in [\"InitGridHi\", \"InitHhatt\", \"kappa\", \"psi\"]:\n",
    "                if group_name in f:\n",
    "                    data[group_name] = read_group(f[group_name])\n",
    "        return data\n",
    "\n",
    "    except OSError as e:\n",
    "        raise OSError(f\"Error reading HDF5 file: {str(e)}\")\n",
    "\n",
    "\n",
    "def read_h5_file_dump_tensors(file_path: str) -> Dict[str, Any]:\n",
    "    def read_group(group) -> Dict[str, Any]:\n",
    "        result = {}\n",
    "        # Read all datasets in current group\n",
    "        for name, item in group.items():\n",
    "            if isinstance(item, h5py.Dataset):\n",
    "                # Convert dataset to numpy array\n",
    "                result[name] = item[()]\n",
    "            elif isinstance(item, h5py.Group):\n",
    "                # Recursively read nested group\n",
    "                result[name] = read_group(item)\n",
    "            else:\n",
    "                print(name, item)\n",
    "        for name, item in group.attrs.items():\n",
    "            result[name] = item\n",
    "        return result\n",
    "\n",
    "    # Check if file exists\n",
    "    if not Path(file_path).exists():\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist\")\n",
    "    try:\n",
    "        with h5py.File(file_path, \"r\") as f:\n",
    "            # Read all contents\n",
    "            data = {}\n",
    "            # Read main groups\n",
    "            for name, item in f.items():\n",
    "                data[name] = read_group(f[name])\n",
    "            # # Read main groups\n",
    "            # for group_name in [\"InitGridHi\", \"InitHhatt\", \"kappa\", \"psi\"]:\n",
    "            #     if group_name in f:\n",
    "            #         data[group_name] = read_group(f[group_name])\n",
    "        return data\n",
    "\n",
    "    except OSError as e:\n",
    "        raise OSError(f\"Error reading HDF5 file: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "def are_dicts_equal(\n",
    "    dict1: Dict[str, Any], dict2: Dict[str, Any], rtol: float = 1e-5\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Compare two nested dictionaries that may contain NumPy arrays.\n",
    "\n",
    "    Args:\n",
    "        dict1: First dictionary to compare\n",
    "        dict2: Second dictionary to compare\n",
    "        rtol: Relative tolerance for NumPy array comparison (default: 1e-5)\n",
    "\n",
    "    Returns:\n",
    "        bool: True if dictionaries are equal, False otherwise\n",
    "\n",
    "    Examples:\n",
    "        >>> d1 = {'a': {'b': np.array([1, 2, 3])}}\n",
    "        >>> d2 = {'a': {'b': np.array([1, 2, 3])}}\n",
    "        >>> are_dicts_equal(d1, d2)\n",
    "        True\n",
    "    \"\"\"\n",
    "    # Check if both inputs are dictionaries\n",
    "    if not isinstance(dict1, dict) or not isinstance(dict2, dict):\n",
    "        return False\n",
    "\n",
    "    # Check if they have the same keys\n",
    "    if dict1.keys() != dict2.keys():\n",
    "        return False\n",
    "\n",
    "    # Compare each key-value pair\n",
    "    for key in dict1:\n",
    "        val1, val2 = dict1[key], dict2[key]\n",
    "\n",
    "        # Handle numpy arrays\n",
    "        if isinstance(val1, np.ndarray) or isinstance(val2, np.ndarray):\n",
    "            if not (isinstance(val1, np.ndarray) and isinstance(val2, np.ndarray)):\n",
    "                return False\n",
    "            if val1.shape != val2.shape:\n",
    "                return False\n",
    "            if not np.allclose(val1, val2, rtol=rtol):\n",
    "                return False\n",
    "\n",
    "        # Handle nested dictionaries\n",
    "        elif isinstance(val1, dict):\n",
    "            if not are_dicts_equal(val1, val2, rtol):\n",
    "                return False\n",
    "\n",
    "        # Handle other types\n",
    "        elif val1 != val2:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def mean_diff_h5(file_1:Path, file_2:Path):\n",
    "    data1 = read_h5_file(file_1)\n",
    "    data2 = read_h5_file(file_2)\n",
    "\n",
    "    diff_dict = {}\n",
    "    for var in data1.keys():\n",
    "        # diff_dict[var] = {}\n",
    "        for component in data1[var][\"Step000000\"].keys():\n",
    "            diff_dict[f\"{var}_{component}\"] = np.mean(np.abs(data1[var][\"Step000000\"][component] - data2[var][\"Step000000\"][component]))\n",
    "\n",
    "\n",
    "    return diff_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdomain = 'SphereC0'\n",
    "file_path = Path(f\"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/test_fake_kappa/checkpoints/14012_workdir/Vars2_{subdomain}.h5\")\n",
    "file_path = Path(f\"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/test_fake_kappa/checkpoints/14012_workdir/filtered_checkpoint_test/Vars2_SphereC0.h5\")\n",
    "data = read_h5_file_dump_tensors(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['diff_kappa_psi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdomain = 'SphereC0'\n",
    "check_pts = Path(\"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_FK_14012_C_13/Ev/Lev3_AA/Run/Checkpoints/14012\")\n",
    "check_pts = Path(\"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_FK_9443_All/Ev/Lev3_AA/Run/Checkpoints/9443\")\n",
    "# check_pts = Path(\"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_fil_buff0_14012/Ev/Lev3_AA/Run/Checkpoints/14012\")\n",
    "file_1 = Path(f\"{check_pts}/Cp-VarsGr_{subdomain}.h5\")\n",
    "file_2 = Path(f\"{check_pts}_original/Cp-VarsGr_{subdomain}.h5\")\n",
    "\n",
    "# data1 = read_h5_file(file_1)\n",
    "# data2 = read_h5_file(file_2)\n",
    "\n",
    "mean_diff_h5(file_1,file_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1 = Path(\"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_5517_6/Ev/Lev3_AA/Run/Checkpoints/5517/Cp-VarsGr_SphereC0.h5\")\n",
    "file_2 = Path(\"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_5517_6/Ev/Lev3_AA/Run/Checkpoints/5517_original/Cp-VarsGr_SphereC0.h5\")\n",
    "\n",
    "# data1 = read_h5_file(file_1)\n",
    "# data2 = read_h5_file(file_2)\n",
    "\n",
    "mean_diff_h5(file_1,file_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_data_from_h5(input_file, output_file):\n",
    "    # The idea is to just copy the data and not mess with the attributes because that causes issues with the reader\n",
    "    with h5py.File(output_file, \"r+\") as outfile, h5py.File(input_file) as infile:\n",
    "        # Level 1: Root level items\n",
    "        for key1, item1 in outfile.items():\n",
    "            if isinstance(item1, h5py.Group):\n",
    "                # Level 2: First nested level\n",
    "                for key2, item2 in item1.items():\n",
    "                    if isinstance(item2, h5py.Group):\n",
    "                        # Level 3: Second nested level\n",
    "                        for key3, item3 in item2.items():\n",
    "                            if isinstance(item3, h5py.Dataset):\n",
    "                                outfile[key1][key2][key3][()] = infile[key1][key2][key3]\n",
    "                            else:\n",
    "                                raise ValueError(f\"Unexpected item type: {type(item3)}\")\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unexpected item type: {type(item2)}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected item type: {type(item1)}\")\n",
    "\n",
    "\n",
    "# input_file = \"/workspaces/spec/Tests/BlackBoxTests/GeneralizedHarmonicExamples/BBHLong/not_tracked/checkpoint_observer/data/Cp-VarsGr_SphereA0.h5\"\n",
    "# output_file = \"/workspaces/spec/Tests/BlackBoxTests/GeneralizedHarmonicExamples/BBHLong/not_tracked/checkpoint_observer/data/Cp-VarsGr_SphereA0_copy.h5\"\n",
    "# copy_and_modify_h5file(input_file, output_file, data_dict['SphereA0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = Path(\"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/del/1294/Cp-VarsGr_SphereC0.h5\")\n",
    "output_path_original = Path(\"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/del/5517/Cp-VarsGr_SphereC0.h5\")\n",
    "\n",
    "output_path = output_path_original.parent/\"Cp-VarsGr_SphereC0_changed.h5\"\n",
    "shutil.copy(output_path_original,output_path)\n",
    "\n",
    "before = mean_diff_h5(input_path,output_path)\n",
    "\n",
    "replace_data_from_h5(input_path,output_path)\n",
    "\n",
    "after = mean_diff_h5(input_path,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before,after"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sxs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
