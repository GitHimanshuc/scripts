{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c8af81-0cf2-4fb2-a7c4-1ec16bcb5fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "plt.style.use('ggplot')\n",
    "# plt.rcParams[\"figure.figsize\"] = (12,10)\n",
    "plt.rcParams[\"figure.figsize\"] = (10,8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef9670f4",
   "metadata": {},
   "source": [
    "# Code to read Horizons.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2f2254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Bh_pandas(h5_dir):\n",
    "    # Empty dataframe\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # List of all the vars in the h5 file\n",
    "    var_list = []\n",
    "    h5_dir.visit(var_list.append)\n",
    "    \n",
    "    \n",
    "    for var in var_list:\n",
    "        # This means there is no time column\n",
    "        # print(f\"{var} : {h5_dir[var].shape}\")\n",
    "        if df.shape == (0,0):\n",
    "            # data[:,0] is time and then we have the data\n",
    "            data = h5_dir[var]\n",
    "            \n",
    "            # vars[:-4] to remove the .dat at the end\n",
    "            col_names = make_col_names(var[:-4],data.shape[1]-1)\n",
    "            col_names.append('t')\n",
    "            # Reverse the list so that we get [\"t\",\"var_name\"]\n",
    "            col_names.reverse()            \n",
    "            append_to_df(data[:],col_names,df)\n",
    "            \n",
    "        else:\n",
    "            data = h5_dir[var]\n",
    "            col_names = make_col_names(var[:-4],data.shape[1]-1)         \n",
    "            append_to_df(data[:,1:],col_names,df)\n",
    "            \n",
    "    return df\n",
    "\n",
    "def append_to_df(data,col_names,df):\n",
    "    for i,col_name in enumerate(col_names):\n",
    "        df[col_name] = data[:,i]\n",
    "        \n",
    "def make_col_names(val_name:str,val_size:int):\n",
    "    col_names = []\n",
    "    if val_size == 1:\n",
    "        col_names.append(val_name)\n",
    "    else:\n",
    "        for i in range(val_size):\n",
    "            col_names.append(val_name+f\"_{i}\")\n",
    "    return col_names\n",
    "\n",
    "\n",
    "def horizon_to_pandas(horizon_path:Path):\n",
    "    assert(horizon_path.exists())\n",
    "    df_dict = {}\n",
    "    with h5py.File(horizon_path,'r') as hf:\n",
    "        # Not all horizon files may have AhC\n",
    "        for key in hf.keys():\n",
    "            df_dict[key[:-4]] = make_Bh_pandas(hf[key])\n",
    "\n",
    "    return df_dict\n",
    "\n",
    "def read_horizon_across_Levs(path_list:List[Path]):\n",
    "    df_listAB = []\n",
    "    df_listC = []\n",
    "    final_dict = {}\n",
    "    for path in path_list:\n",
    "        df_lev = horizon_to_pandas(path)\n",
    "        # Either [AhA,AhB] or [AhA,AhB,AhC]\n",
    "        if len(df_lev.keys()) > 1:\n",
    "            df_listAB.append(df_lev)\n",
    "        # Either [AhC] or [AhA,AhB,AhC]\n",
    "        if (len(df_lev.keys()) == 1) or (len(df_lev.keys()) ==3):\n",
    "            df_listC.append(df_lev)\n",
    "    if len(df_listAB)==1:\n",
    "        # There was only one lev\n",
    "        final_dict = df_listAB[0]\n",
    "    else:\n",
    "        final_dict[\"AhA\"] = pd.concat([df[\"AhA\"] for df in df_listAB])\n",
    "        final_dict[\"AhB\"] = pd.concat([df[\"AhB\"] for df in df_listAB])\n",
    "        if len(df_listC) > 0:\n",
    "            final_dict[\"AhC\"] = pd.concat([df[\"AhC\"] for df in df_listC])       \n",
    "    \n",
    "    return final_dict\n",
    "\n",
    "def moving_average(array,avg_len):\n",
    "    return np.convolve(array,np.ones(avg_len))/avg_len\n",
    "    \n",
    "def moving_average_valid(array,avg_len):\n",
    "    return np.convolve(array,np.ones(avg_len),'valid')/avg_len\n",
    "\n",
    "def plot_graph_for_runs(runs_data_dict, x_axis, y_axis, minT, maxT, save_path=None, moving_avg_len=0, plot_fun = lambda x,y,label : plt.plot(x,y,label=label)):\n",
    "\n",
    "  minT_indx_list={}\n",
    "  maxT_indx_list={}\n",
    "  \n",
    "  for run_name in runs_data_dict.keys():\n",
    "    minT_indx_list[run_name] = len(runs_data_dict[run_name][x_axis][runs_data_dict[run_name][x_axis] < minT])\n",
    "    maxT_indx_list[run_name] = len(runs_data_dict[run_name][x_axis][runs_data_dict[run_name][x_axis] < maxT])\n",
    "\n",
    "  if moving_avg_len == 0:\n",
    "\n",
    "    for run_name in runs_data_dict.keys():\n",
    "      x_data = runs_data_dict[run_name][x_axis][minT_indx_list[run_name]:maxT_indx_list[run_name]]\n",
    "      y_data = runs_data_dict[run_name][y_axis][minT_indx_list[run_name]:maxT_indx_list[run_name]]\n",
    "      plot_fun(x_data, y_data,run_name)\n",
    "\n",
    "    plt.xlabel(x_axis)\n",
    "    plt.ylabel(y_axis)\n",
    "    title = \"\\\"\" +  y_axis+\"\\\" vs \\\"\"+x_axis+\"\\\"\"\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "  else:\n",
    "    for run_name in runs_data_dict.keys():\n",
    "      x_data = runs_data_dict[run_name][x_axis][minT_indx_list[run_name] + moving_avg_len-1:maxT_indx_list[run_name]]\n",
    "      y_data = moving_average_valid(runs_data_dict[run_name][y_axis][minT_indx_list[run_name]:maxT_indx_list[run_name]], moving_avg_len)\n",
    "      plot_fun(x_data, y_data,run_name)\n",
    "\n",
    "    plt.xlabel(x_axis)\n",
    "    plt.ylabel(y_axis)\n",
    "    title = \"\\\"\" + y_axis+ \"\\\" vs \\\"\" + x_axis + \"\\\"  \" + f\"avg_window_len={moving_avg_len}\"\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "  \n",
    "  if save_path is not None:\n",
    "    fig_x_label = x_axis.replace(\"/\",\"_\").replace(\".\",\"_\")\n",
    "    fig_y_label = y_axis.replace(\"/\",\"_\").replace(\".\",\"_\")\n",
    "    save_file_name = f\"{fig_y_label}_vs_{fig_x_label}_minT={minT}_maxT={maxT}_moving_avg_len={moving_avg_len}\"\n",
    "    for run_name in runs_data_dict.keys():\n",
    "      save_file_name = save_file_name + \"__\" + run_name\n",
    "\n",
    "    plt.savefig(save_path+save_file_name)\n",
    "\n",
    "def load_data_from_levs(base_path:Path, runs_path:Dict[str,Path]):\n",
    "  data_dict = {}\n",
    "  for run_name in runs_path.keys():\n",
    "    path_list = list(base_path.glob(runs_path[run_name]))\n",
    "    print(path_list)\n",
    "    data_dict[run_name] = read_horizon_across_Levs(path_list)\n",
    "  return data_dict\n",
    "\n",
    "def flatten_dict(horizon_data_dict:Dict[str,pd.DataFrame]) -> Dict[str,pd.DataFrame] :\n",
    "  flattened_data = {}\n",
    "  for run_name in horizon_data_dict.keys():\n",
    "      for horizons in horizon_data_dict[run_name]:\n",
    "          flattened_data[run_name+\"_\"+horizons] = horizon_data_dict[run_name][horizons]\n",
    "          # print(run_name+\"_\"+horizons)\n",
    "  return flattened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03186d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_to_plot = {}\n",
    "base_path = Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs\")\n",
    "runs_to_plot[\"76_ngd_master_mr1_50_3000\"] =  \"76_ngd_master_mr1_50_3000/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "runs_to_plot[\"76_ngd_master_mr1_200_3000\"] =  \"76_ngd_master_mr1_200_3000/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"77_gd_Kerr_q1\"] =  \"77_gd_Kerr_q1/Ev/Lev1_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"77_gd_Kerr_q3\"] =  \"77_gd_Kerr_q3/Ev/Lev1_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"77_gd_Kerr_q1_Kerr\"] =  \"77_gd_Kerr_q1/Ev_Kerr/Lev1_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"77_gd_Kerr_q3_Kerr\"] =  \"77_gd_Kerr_q3/Ev_Kerr/Lev1_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"78_ngd_master_mr1\"] =  \"78_ngd_master_mr1/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "runs_to_plot[\"79_ngd_master_mr1_1000_3000\"] =  \"79_ngd_master_mr1_1000_3000/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"79_ngd_master_mr1_200_3000\"] =  \"79_ngd_master_mr1_200_3000/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"80_ngd_master_mr1_100\"] =  \"80_ngd_master_mr1_100/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"80_ngd_master_mr1_50\"] =  \"80_ngd_master_mr1_50/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"80_ngd_master_mr1_10\"] =  \"80_ngd_master_mr1_10/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"80_ngd_master_mr1_5\"] =  \"80_ngd_master_mr1_5/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"80_ngd_master_mr1_300\"] =  \"80_ngd_master_mr1_300/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"81_gd_Kerr_q3_0_9_0__0_0_0\"] =  \"81_gd_Kerr_q3_0_9_0__0_0_0/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"81_gd_DH_q3_0_9_0__0_0_0\"] =  \"81_gd_DH_q3_0_9_0__0_0_0/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"82_ngd_master_mr1_50_3000_DH_to_DH\"] =  \"82_ngd_master_mr1_50_3000_DH_to_DH/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"83_ngd_master_mr1_200_3000_no_eps\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"83_ngd_master_mr1_200_3000_no_eps_no_lsr\"] =  \"83_ngd_master_mr1_200_3000_no_eps_no_lsr/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"84_gd_KerrI_3000_200\"] =  \"84_gd_KerrI_3000_200/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"84_gd_DH_3000_200\"] =  \"84_gd_DH_3000_200/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"83_no_eps_Ev_wrong_evolution\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev_wrong_evolution/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "runs_to_plot[\"83_no_eps_Ev_pow2\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev_pow2/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "runs_to_plot[\"83_no_eps_Ev_pow6\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev_pow6/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"83_no_eps_Ev_tanh15\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev_tanh15/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"83_no_eps_Ev_tanh7\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev_tanh7/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"83_no_eps_Ev_tanh7_lsr_correct_evolution\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev_tanh7_lsr_correct_evolution/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "\n",
    "data_dict = load_data_from_levs(base_path, runs_to_plot)\n",
    "data_dict = flatten_dict(data_dict)\n",
    "data_dict[list(data_dict.keys())[0]].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468fdaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_avg_len = 0\n",
    "save_path = None\n",
    "\n",
    "x_axis = 't'\n",
    "# y_axis = 'ArealMass'\n",
    "# y_axis = 'ChristodoulouMass'\n",
    "# y_axis = 'CoordCenterInertial_0'\n",
    "# y_axis = 'CoordCenterInertial_1'\n",
    "# y_axis = 'CoordCenterInertial_2'\n",
    "# y_axis = 'DimensionfulInertialSpin_0'\n",
    "# y_axis = 'DimensionfulInertialSpin_1'\n",
    "# y_axis = 'DimensionfulInertialSpin_2'\n",
    "y_axis = 'DimensionfulInertialSpinMag'\n",
    "# y_axis = 'SpinFromShape_0'\n",
    "# y_axis = 'SpinFromShape_1'\n",
    "# y_axis = 'SpinFromShape_2'\n",
    "# y_axis = 'SpinFromShape_3'\n",
    "# y_axis = 'chiInertial_0'\n",
    "# y_axis = 'chiInertial_1'\n",
    "# y_axis = 'chiInertial_2'\n",
    "# y_axis = 'chiMagInertial'\n",
    "\n",
    "\n",
    "\n",
    "# moving_avg_len=25\n",
    "minT = 2500\n",
    "maxT = 5000\n",
    "\n",
    "plot_fun = lambda x,y,label : plt.plot(x,y,label=label)\n",
    "# plot_fun = lambda x,y,label : plt.semilogy(x,y,label=label)\n",
    "# plot_fun = lambda x,y,label : plt.loglog(x,y,label=label)\n",
    "# plot_fun = lambda x,y,label : plt.scatter(x,y,label=label)\n",
    "# save_path = \"/panfs/ds09/sxs/himanshu/scripts/report/not_tracked/temp2/\"\n",
    "\n",
    "filtered_dict = {}\n",
    "allowed_horizons = [\"AhA\"]\n",
    "for horizons in allowed_horizons:\n",
    "  for runs_keys in data_dict.keys():\n",
    "    if horizons in runs_keys:\n",
    "      filtered_dict[runs_keys] = data_dict[runs_keys]\n",
    "\n",
    "with plt.style.context('default'):\n",
    "  plt.rcParams[\"figure.figsize\"] = (12,10)\n",
    "  plt.rcParams[\"figure.autolayout\"] = True\n",
    "  plot_graph_for_runs(filtered_dict, x_axis, y_axis, minT, maxT, save_path=save_path, moving_avg_len=moving_avg_len, plot_fun=plot_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/83_ngd_master_mr1_200_3000_no_eps/Ev_pow2\")\n",
    "base_path = Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/83_ngd_master_mr1_200_3000_no_eps/Ev_pow6\")\n",
    "# base_path = Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/76_ngd_master_mr1_50_3000/Ev\")\n",
    "# base_path = Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/76_ngd_master_mr1_200_3000/Ev\")\n",
    "# base_path = Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/79_ngd_master_mr1_200_3000/Ev\")\n",
    "# base_path = Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/79_ngd_master_mr1_1000_3000/Ev\")\n",
    "file_pattern = \"Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "path_list = list(base_path.glob(file_pattern))\n",
    "path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f8e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_horizon_across_Levs(path_list)\n",
    "print(df.keys())\n",
    "df[\"AhA\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c26f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 't'\n",
    "y = \"SpinFromShape_2\"\n",
    "plt.semilogy(df['AhA'][x],df['AhA'][y],label=\"AhA\")\n",
    "plt.semilogy(df['AhB'][x],df['AhB'][y],label=\"AhB\")\n",
    "# plt.plot(df['AhC'][x],df['AhC'][y],label=\"AhC\")\n",
    "plt.xlabel(x)\n",
    "plt.ylabel(y)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dbd1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda67ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df['AhC'][x],df['AhC'][y],label=\"AhC\")\n",
    "plt.xlabel(x)\n",
    "plt.ylabel(y)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e3329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(df[\"AhA\"].describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
