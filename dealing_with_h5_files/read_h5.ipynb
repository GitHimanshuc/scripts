{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c8af81-0cf2-4fb2-a7c4-1ec16bcb5fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "plt.style.use('ggplot')\n",
    "# plt.rcParams[\"figure.figsize\"] = (12,10)\n",
    "plt.rcParams[\"figure.figsize\"] = (10,8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef9670f4",
   "metadata": {},
   "source": [
    "# Code to read Horizons.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2f2254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Bh_pandas(h5_dir):\n",
    "    # Empty dataframe\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # List of all the vars in the h5 file\n",
    "    var_list = []\n",
    "    h5_dir.visit(var_list.append)\n",
    "    \n",
    "    \n",
    "    for var in var_list:\n",
    "        # This means there is no time column\n",
    "        # print(f\"{var} : {h5_dir[var].shape}\")\n",
    "        if df.shape == (0,0):\n",
    "            # data[:,0] is time and then we have the data\n",
    "            data = h5_dir[var]\n",
    "            \n",
    "            # vars[:-4] to remove the .dat at the end\n",
    "            col_names = make_col_names(var[:-4],data.shape[1]-1)\n",
    "            col_names.append('t')\n",
    "            # Reverse the list so that we get [\"t\",\"var_name\"]\n",
    "            col_names.reverse()            \n",
    "            append_to_df(data[:],col_names,df)\n",
    "            \n",
    "        else:\n",
    "            data = h5_dir[var]\n",
    "            col_names = make_col_names(var[:-4],data.shape[1]-1)         \n",
    "            append_to_df(data[:,1:],col_names,df)\n",
    "            \n",
    "    return df\n",
    "\n",
    "def append_to_df(data,col_names,df):\n",
    "    for i,col_name in enumerate(col_names):\n",
    "        df[col_name] = data[:,i]\n",
    "        \n",
    "def make_col_names(val_name:str,val_size:int):\n",
    "    col_names = []\n",
    "    if val_size == 1:\n",
    "        col_names.append(val_name)\n",
    "    else:\n",
    "        for i in range(val_size):\n",
    "            col_names.append(val_name+f\"_{i}\")\n",
    "    return col_names\n",
    "\n",
    "\n",
    "def horizon_to_pandas(horizon_path:Path):\n",
    "    assert(horizon_path.exists())\n",
    "    df_dict = {}\n",
    "    with h5py.File(horizon_path,'r') as hf:\n",
    "        # Not all horizon files may have AhC\n",
    "        for key in hf.keys():\n",
    "            df_dict[key[:-4]] = make_Bh_pandas(hf[key])\n",
    "\n",
    "    return df_dict\n",
    "\n",
    "def read_horizon_across_Levs(path_list:List[Path]):\n",
    "    df_listAB = []\n",
    "    df_listC = []\n",
    "    final_dict = {}\n",
    "    for path in path_list:\n",
    "        df_lev = horizon_to_pandas(path)\n",
    "        # Either [AhA,AhB] or [AhA,AhB,AhC]\n",
    "        if len(df_lev.keys()) > 1:\n",
    "            df_listAB.append(df_lev)\n",
    "        # Either [AhC] or [AhA,AhB,AhC]\n",
    "        if (len(df_lev.keys()) == 1) or (len(df_lev.keys()) ==3):\n",
    "            df_listC.append(df_lev)\n",
    "    if len(df_listAB)==1:\n",
    "        # There was only one lev\n",
    "        final_dict = df_listAB[0]\n",
    "    else:\n",
    "        final_dict[\"AhA\"] = pd.concat([df[\"AhA\"] for df in df_listAB])\n",
    "        final_dict[\"AhB\"] = pd.concat([df[\"AhB\"] for df in df_listAB])\n",
    "        if len(df_listC) > 0:\n",
    "            final_dict[\"AhC\"] = pd.concat([df[\"AhC\"] for df in df_listC])       \n",
    "    \n",
    "    return final_dict\n",
    "\n",
    "def moving_average(array,avg_len):\n",
    "    return np.convolve(array,np.ones(avg_len))/avg_len\n",
    "    \n",
    "def moving_average_valid(array,avg_len):\n",
    "    return np.convolve(array,np.ones(avg_len),'valid')/avg_len\n",
    "\n",
    "def plot_graph_for_runs(runs_data_dict, x_axis, y_axis, minT, maxT, save_path=None, moving_avg_len=0, plot_fun = lambda x,y,label : plt.plot(x,y,label=label)):\n",
    "\n",
    "  minT_indx_list={}\n",
    "  maxT_indx_list={}\n",
    "  \n",
    "  for run_name in runs_data_dict.keys():\n",
    "    minT_indx_list[run_name] = len(runs_data_dict[run_name][x_axis][runs_data_dict[run_name][x_axis] < minT])\n",
    "    maxT_indx_list[run_name] = len(runs_data_dict[run_name][x_axis][runs_data_dict[run_name][x_axis] < maxT])\n",
    "\n",
    "  if moving_avg_len == 0:\n",
    "\n",
    "    for run_name in runs_data_dict.keys():\n",
    "      x_data = runs_data_dict[run_name][x_axis][minT_indx_list[run_name]:maxT_indx_list[run_name]]\n",
    "      y_data = runs_data_dict[run_name][y_axis][minT_indx_list[run_name]:maxT_indx_list[run_name]]\n",
    "      plot_fun(x_data, y_data,run_name)\n",
    "\n",
    "    plt.xlabel(x_axis)\n",
    "    plt.ylabel(y_axis)\n",
    "    title = \"\\\"\" +  y_axis+\"\\\" vs \\\"\"+x_axis+\"\\\"\"\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "  else:\n",
    "    for run_name in runs_data_dict.keys():\n",
    "      x_data = runs_data_dict[run_name][x_axis][minT_indx_list[run_name] + moving_avg_len-1:maxT_indx_list[run_name]]\n",
    "      y_data = moving_average_valid(runs_data_dict[run_name][y_axis][minT_indx_list[run_name]:maxT_indx_list[run_name]], moving_avg_len)\n",
    "      plot_fun(x_data, y_data,run_name)\n",
    "\n",
    "    plt.xlabel(x_axis)\n",
    "    plt.ylabel(y_axis)\n",
    "    title = \"\\\"\" + y_axis+ \"\\\" vs \\\"\" + x_axis + \"\\\"  \" + f\"avg_window_len={moving_avg_len}\"\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "  \n",
    "  if save_path is not None:\n",
    "    fig_x_label = x_axis.replace(\"/\",\"_\").replace(\".\",\"_\")\n",
    "    fig_y_label = y_axis.replace(\"/\",\"_\").replace(\".\",\"_\")\n",
    "    save_file_name = f\"{fig_y_label}_vs_{fig_x_label}_minT={minT}_maxT={maxT}_moving_avg_len={moving_avg_len}\"\n",
    "    for run_name in runs_data_dict.keys():\n",
    "      save_file_name = save_file_name + \"__\" + run_name\n",
    "\n",
    "    plt.savefig(save_path+save_file_name)\n",
    "\n",
    "def load_data_from_levs(base_path:Path, runs_path:Dict[str,Path]):\n",
    "  data_dict = {}\n",
    "  for run_name in runs_path.keys():\n",
    "    path_list = list(base_path.glob(runs_path[run_name]))\n",
    "    print(path_list)\n",
    "    data_dict[run_name] = read_horizon_across_Levs(path_list)\n",
    "  return data_dict\n",
    "\n",
    "def flatten_dict(horizon_data_dict:Dict[str,pd.DataFrame]) -> Dict[str,pd.DataFrame] :\n",
    "  flattened_data = {}\n",
    "  for run_name in horizon_data_dict.keys():\n",
    "      for horizons in horizon_data_dict[run_name]:\n",
    "          flattened_data[run_name+\"_\"+horizons] = horizon_data_dict[run_name][horizons]\n",
    "          # print(run_name+\"_\"+horizons)\n",
    "  return flattened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03186d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_to_plot = {}\n",
    "base_path = Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs\")\n",
    "runs_to_plot[\"76_ngd_master_mr1_50_3000\"] =  \"76_ngd_master_mr1_50_3000/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "runs_to_plot[\"76_ngd_master_mr1_200_3000\"] =  \"76_ngd_master_mr1_200_3000/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"77_gd_Kerr_q1\"] =  \"77_gd_Kerr_q1/Ev/Lev1_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"77_gd_Kerr_q3\"] =  \"77_gd_Kerr_q3/Ev/Lev1_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"77_gd_Kerr_q1_Kerr\"] =  \"77_gd_Kerr_q1/Ev_Kerr/Lev1_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"77_gd_Kerr_q3_Kerr\"] =  \"77_gd_Kerr_q3/Ev_Kerr/Lev1_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"78_ngd_master_mr1\"] =  \"78_ngd_master_mr1/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "runs_to_plot[\"79_ngd_master_mr1_1000_3000\"] =  \"79_ngd_master_mr1_1000_3000/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"79_ngd_master_mr1_200_3000\"] =  \"79_ngd_master_mr1_200_3000/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"80_ngd_master_mr1_100\"] =  \"80_ngd_master_mr1_100/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"80_ngd_master_mr1_50\"] =  \"80_ngd_master_mr1_50/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"80_ngd_master_mr1_10\"] =  \"80_ngd_master_mr1_10/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"80_ngd_master_mr1_5\"] =  \"80_ngd_master_mr1_5/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"80_ngd_master_mr1_300\"] =  \"80_ngd_master_mr1_300/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"81_gd_Kerr_q3_0_9_0__0_0_0\"] =  \"81_gd_Kerr_q3_0_9_0__0_0_0/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"81_gd_DH_q3_0_9_0__0_0_0\"] =  \"81_gd_DH_q3_0_9_0__0_0_0/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"82_ngd_master_mr1_50_3000_DH_to_DH\"] =  \"82_ngd_master_mr1_50_3000_DH_to_DH/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"83_ngd_master_mr1_200_3000_no_eps\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"83_ngd_master_mr1_200_3000_no_eps_no_lsr\"] =  \"83_ngd_master_mr1_200_3000_no_eps_no_lsr/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"84_gd_KerrI_3000_200\"] =  \"84_gd_KerrI_3000_200/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"84_gd_DH_3000_200\"] =  \"84_gd_DH_3000_200/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"83_no_eps_Ev_wrong_evolution\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev_wrong_evolution/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "runs_to_plot[\"83_no_eps_Ev_pow2\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev_pow2/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "runs_to_plot[\"83_no_eps_Ev_pow6\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev_pow6/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"83_no_eps_Ev_tanh15\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev_tanh15/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"83_no_eps_Ev_tanh7\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev_tanh7/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"83_no_eps_Ev_tanh7_lsr_correct_evolution\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev_tanh7_lsr_correct_evolution/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "\n",
    "data_dict = load_data_from_levs(base_path, runs_to_plot)\n",
    "data_dict = flatten_dict(data_dict)\n",
    "data_dict[list(data_dict.keys())[0]].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468fdaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_avg_len = 0\n",
    "save_path = None\n",
    "\n",
    "x_axis = 't'\n",
    "# y_axis = 'ArealMass'\n",
    "# y_axis = 'ChristodoulouMass'\n",
    "# y_axis = 'CoordCenterInertial_0'\n",
    "# y_axis = 'CoordCenterInertial_1'\n",
    "# y_axis = 'CoordCenterInertial_2'\n",
    "# y_axis = 'DimensionfulInertialSpin_0'\n",
    "# y_axis = 'DimensionfulInertialSpin_1'\n",
    "# y_axis = 'DimensionfulInertialSpin_2'\n",
    "y_axis = 'DimensionfulInertialSpinMag'\n",
    "# y_axis = 'SpinFromShape_0'\n",
    "# y_axis = 'SpinFromShape_1'\n",
    "# y_axis = 'SpinFromShape_2'\n",
    "# y_axis = 'SpinFromShape_3'\n",
    "# y_axis = 'chiInertial_0'\n",
    "# y_axis = 'chiInertial_1'\n",
    "# y_axis = 'chiInertial_2'\n",
    "# y_axis = 'chiMagInertial'\n",
    "\n",
    "\n",
    "\n",
    "# moving_avg_len=25\n",
    "minT = 2500\n",
    "maxT = 5000\n",
    "\n",
    "plot_fun = lambda x,y,label : plt.plot(x,y,label=label)\n",
    "# plot_fun = lambda x,y,label : plt.semilogy(x,y,label=label)\n",
    "# plot_fun = lambda x,y,label : plt.loglog(x,y,label=label)\n",
    "# plot_fun = lambda x,y,label : plt.scatter(x,y,label=label)\n",
    "# save_path = \"/panfs/ds09/sxs/himanshu/scripts/report/not_tracked/temp2/\"\n",
    "\n",
    "filtered_dict = {}\n",
    "allowed_horizons = [\"AhA\"]\n",
    "for horizons in allowed_horizons:\n",
    "  for runs_keys in data_dict.keys():\n",
    "    if horizons in runs_keys:\n",
    "      filtered_dict[runs_keys] = data_dict[runs_keys]\n",
    "\n",
    "with plt.style.context('default'):\n",
    "  plt.rcParams[\"figure.figsize\"] = (12,10)\n",
    "  plt.rcParams[\"figure.autolayout\"] = True\n",
    "  plot_graph_for_runs(filtered_dict, x_axis, y_axis, minT, maxT, save_path=save_path, moving_avg_len=moving_avg_len, plot_fun=plot_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065718c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/83_ngd_master_mr1_200_3000_no_eps/Ev_pow2\")\n",
    "base_path = Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/83_ngd_master_mr1_200_3000_no_eps/Ev_pow6\")\n",
    "# base_path = Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/76_ngd_master_mr1_50_3000/Ev\")\n",
    "# base_path = Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/76_ngd_master_mr1_200_3000/Ev\")\n",
    "# base_path = Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/79_ngd_master_mr1_200_3000/Ev\")\n",
    "# base_path = Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/79_ngd_master_mr1_1000_3000/Ev\")\n",
    "file_pattern = \"Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "path_list = list(base_path.glob(file_pattern))\n",
    "path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f8e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_horizon_across_Levs(path_list)\n",
    "print(df.keys())\n",
    "df[\"AhA\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c26f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 't'\n",
    "y = \"SpinFromShape_2\"\n",
    "plt.semilogy(df['AhA'][x],df['AhA'][y],label=\"AhA\")\n",
    "plt.semilogy(df['AhB'][x],df['AhB'][y],label=\"AhB\")\n",
    "# plt.plot(df['AhC'][x],df['AhC'][y],label=\"AhC\")\n",
    "plt.xlabel(x)\n",
    "plt.ylabel(y)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dbd1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda67ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df['AhC'][x],df['AhC'][y],label=\"AhC\")\n",
    "plt.xlabel(x)\n",
    "plt.ylabel(y)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e3329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(df[\"AhA\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3a7cfb",
   "metadata": {},
   "source": [
    "# Read profiler results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a97c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f58a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = Path(\"AdjustGridExtents.h5\")\n",
    "# file_path = Path(\"CachedH5Info.h5\")\n",
    "# file_path = Path(\"ControlNthDeriv.h5\")\n",
    "file_path = Path(\"FilterDiagnostics.h5\")\n",
    "# file_path = Path(\"IncProfiler.h5\")\n",
    "# file_path = Path(\"MemProfiler.h5\")\n",
    "# file_path = Path(\"OrbitDiagnostics.h5\")\n",
    "# file_path = Path(\"PowerDiagnostics.h5\")\n",
    "# file_path = Path(\"Profiler.h5\")\n",
    "# file_path = Path(\"ProjectedCon.h5\")\n",
    "# file_path = Path(\"RhsExpense.h5\")\n",
    "file_path = Path(\"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/cce_bondi/Lev3_R0200/Lev3_R0200.h5\")\n",
    "# file_path = Path(\"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev3_AC/Run/GW2/BondiCceR0258.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d3ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(file_path,'r') as f:\n",
    "    names = []\n",
    "    f.visit(names.append)\n",
    "    f.visit(print)\n",
    "    data = np.array(f['Beta.dat'])\n",
    "    # print(np.array(data),np.array(data).shape)\n",
    "\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3b0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(Path(\"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35\").glob(f\"Ev/Lev4_??/Run/GW2/BondiCceR0100.h5\"))\n",
    "a = a+list(Path(\"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35\").glob(f\"Ev/Lev4_Ringdown/Lev4_??/Run/GW2/BondiCceR0100.h5\"))\n",
    "a.sort()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf93eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e125d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(file_path,'r') as f:\n",
    "    steps = set()\n",
    "    procs = set()\n",
    "    names = []\n",
    "    f.visit(names.append)\n",
    "    for name in names:\n",
    "      step = name.split('.')[0][4:]\n",
    "      steps.add(step)\n",
    "      if 'Proc' in name:\n",
    "         procs.add(name.split('/')[-1][4:-4])\n",
    "\n",
    "    dict_list = []\n",
    "    for step in steps:\n",
    "       for proc in procs:\n",
    "          data = f[f'Step{step}.dir/Proc{proc}.txt'][0].decode()\n",
    "\n",
    "          lines = data.split(\"\\n\")\n",
    "          time = float((lines[0].split(\"=\")[-1])[:-1])\n",
    "\n",
    "          curr_dict = {\n",
    "             \"t(M)\": time,\n",
    "             \"step\": step,\n",
    "             \"proc\": proc\n",
    "          }\n",
    "          # Find where the columns end\n",
    "          a = lines[4]\n",
    "          event_end = a.find(\"Event\")+5\n",
    "          cum_end = a.find(\"cum(%)\")+6\n",
    "          exc_end = a.find(\"exc(%)\")+6\n",
    "          inc_end = a.find(\"inc(%)\")+6\n",
    "\n",
    "          for line in lines[6:-2]:\n",
    "            Event = line[:event_end].strip()\n",
    "            cum = float(line[event_end:cum_end].strip())\n",
    "            exc = float(line[cum_end:exc_end].strip())\n",
    "            inc = float(line[exc_end:inc_end].strip())\n",
    "            N = int(line[inc_end:].strip())\n",
    "            # print(a)\n",
    "            # a = line.split(\"  \")\n",
    "            # Event,cum,exc,inc,N = [i.strip() for i in a if i!= '']\n",
    "            curr_dict[f'{Event}_cum'] = cum\n",
    "            curr_dict[f'{Event}_exc'] = exc\n",
    "            curr_dict[f'{Event}_inc'] = inc\n",
    "            curr_dict[f'{Event}_N'] = N\n",
    "\n",
    "          dict_list.append(curr_dict)\n",
    "\n",
    "\n",
    "print(steps,procs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5497349",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3adc79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = data.split(\"\\n\")\n",
    "for line in lines[6:-2]:\n",
    "  a = line.split(\"  \")\n",
    "  Event,cum,exc,inc,N = [i.strip() for i in a if i!= '']\n",
    "  print(Event,cum,exc,inc,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d71e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Event,cum,exc,inc,N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0566b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "float(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e054820",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"                                                            Event    cum(%)    exc(%)    inc(%)         N\"\n",
    "a.find(\"cum(%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24449d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4580b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_end = a.find(\"Event\")+5\n",
    "cum_end = a.find(\"cum(%)\")+6\n",
    "exc_end = a.find(\"exc(%)\")+6\n",
    "inc_end = a.find(\"inc(%)\")+6\n",
    "event = a[:event_end].strip()\n",
    "cum = float(a[event_end:cum_end].strip())\n",
    "exc = float(a[event_end:exc_end].strip())\n",
    "inc = float(a[event_end:inc_end].strip())\n",
    "N = int(a[inc_end:].strip())\n",
    "print(event,cum,exc,inc,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e465c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[event_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c832f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2233704",
   "metadata": {},
   "source": [
    "## Parsing speed test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d34e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path(\"/groups/sxs/hchaudha/scripts/dealing_with_h5_files/del/extracted-./PowerDiagnostics/SphereA0.dir/Bf0I1_ConvergenceFactor.dat\")\n",
    "file_path = Path(\"/groups/sxs/hchaudha/spec_runs/2_SpKS_q1_sA_0_0_9_sB_0_0_9_d15/Ev/Lev3_AB/Run/ConstraintNorms/GhCe_Linf.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72216c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text_file_to_csv(input_file):\n",
    "    # Read the file and extract lines\n",
    "    with open(input_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Extract column names\n",
    "    column_names = [line.strip().split('= ')[1] for line in lines if line.startswith('#') and \"=\" in line]\n",
    "    \n",
    "    # Extract data lines\n",
    "    data_lines = [line.strip() for line in lines if not line.startswith('#') and line.strip()]\n",
    "\n",
    "    # Convert data lines to a list of lists\n",
    "    data = [list(map(float, line.split())) for line in data_lines]\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_text_file_to_csv_np(input_file):\n",
    "    # Read the file and extract lines\n",
    "    with open(input_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Extract column names\n",
    "    column_names = [line.strip().split('= ')[1] for line in lines if line.startswith('#') and \"=\" in line]\n",
    "    print(column_names)\n",
    "    # Extract data lines\n",
    "    data_lines = np.genfromtxt(input_file,comments=\"#\",delimiter='    ')\n",
    "\n",
    "    return data_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cf6427",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path(\"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L6/Ev/Lev6_AD/Run/TStepperDiag.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fda99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "np.genfromtxt(file_path,comments=\"#\",delimiter='  ',dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8714fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "parse_text_file_to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988e5a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_text_file_to_csv_np(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99965434",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_text_file_to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3d35e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.genfromtxt(file_path,comments=\"#\",delimiter='  ',dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4690ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "pd.read_csv(file_path,sep=\"\\s+\",comment=\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f0795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_column_names_dat_file(file_path):\n",
    "  cols_names = []\n",
    "  with open(file_path,'r') as f:\n",
    "      lines = f.readlines()\n",
    "      for line in lines:\n",
    "        if \"#\" not in line:\n",
    "          break\n",
    "        elif \"=\" in line:\n",
    "          cols_names.append(line.split('=')[-1][1:-1].strip())\n",
    "        else:\n",
    "          continue\n",
    "  return cols_names\n",
    "\n",
    "def read_dat_file(file_path):\n",
    "  cols_names = find_column_names_dat_file(file_path)\n",
    "  return pd.read_csv(file_path,sep=\"\\s+\",comment=\"#\",names=cols_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8694f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "AdjustGridExtents_path = Path(\"/groups/sxs/hchaudha/scripts/dealing_with_h5_files/del/extracted-./AdjustGridExtents\")\n",
    "CachedH5Info_path = Path(\"/groups/sxs/hchaudha/scripts/dealing_with_h5_files/del/extracted-./CachedH5Info\")\n",
    "ControlNthDeriv_path = Path(\"/groups/sxs/hchaudha/scripts/dealing_with_h5_files/del/extracted-./ControlNthDeriv\")\n",
    "FilterDiagnostics_path = Path(\"/groups/sxs/hchaudha/scripts/dealing_with_h5_files/del/extracted-./FilterDiagnostics\")\n",
    "IncProfiler_path = Path(\"/groups/sxs/hchaudha/scripts/dealing_with_h5_files/del/extracted-./IncProfiler\")\n",
    "MemProfiler_path = Path(\"/groups/sxs/hchaudha/scripts/dealing_with_h5_files/del/extracted-./MemProfiler\")\n",
    "OrbitDiagnostics_path = Path(\"/groups/sxs/hchaudha/scripts/dealing_with_h5_files/del/extracted-./OrbitDiagnostics\")\n",
    "PowerDiagnostics_path = Path(\"/groups/sxs/hchaudha/scripts/dealing_with_h5_files/del/extracted-./PowerDiagnostics\")\n",
    "Profiler_path = Path(\"/groups/sxs/hchaudha/scripts/dealing_with_h5_files/del/extracted-./Profiler\")\n",
    "ProjectedCon_path = Path(\"/groups/sxs/hchaudha/scripts/dealing_with_h5_files/del/extracted-./ProjectedCon\")\n",
    "RhsExpense_path = Path(\"/groups/sxs/hchaudha/scripts/dealing_with_h5_files/del/extracted-./RhsExpense\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2b5771",
   "metadata": {},
   "source": [
    "## Read various h5 files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3299d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_dir_and_dat_files(main_folder:Path):\n",
    "  # Returns [dir_names], [dat_file_names]\n",
    "  dirs = []\n",
    "  dat_files = []\n",
    "  for path in main_folder.iterdir():\n",
    "    if \".dir\" in path.name:\n",
    "      dirs.append(path)\n",
    "    elif \".dat\" in path.name:\n",
    "      dat_files.append(path)\n",
    "\n",
    "  return dirs,dat_files\n",
    "\n",
    "\n",
    "def pad_tuple(var,required_size):\n",
    "  # pad_tuple(('a'),4) # ('a', '', '', '')\n",
    "  # pad_tuple('a',4) # ('a', '', '', '')\n",
    "  # pad_tuple((1,2),4) # (1, 2, '', '')\n",
    "  # pad_tuple(('1','2'),4) # ('1', '2', '', '')\n",
    "  list_sized = ['' for i in range(required_size)]\n",
    "  if isinstance(var,tuple):\n",
    "    if len(var) > required_size:\n",
    "      raise Exception(f\"Length of {var}={len(var)} is larger than the {required_size=}\")\n",
    "    for i,val in enumerate(var):\n",
    "      list_sized[i] = val\n",
    "  elif isinstance(var, str):\n",
    "    list_sized[0] = var\n",
    "  else:\n",
    "    raise ValueError(f\"{var} is of type {type(var)}. Only string and tuples are supported.\")\n",
    "  return tuple(list_sized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0ed274",
   "metadata": {},
   "source": [
    "#### Profiler.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf814a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_profiler_old(file_name):\n",
    "  with h5py.File(file_name,'r') as f:\n",
    "    steps = set()\n",
    "    procs = set()\n",
    "    names = []\n",
    "    f.visit(names.append)\n",
    "    for name in names:\n",
    "      step = name.split('.')[0][4:]\n",
    "      steps.add(step)\n",
    "      if 'Proc' in name:\n",
    "        procs.add(name.split('/')[-1][4:-4])\n",
    "\n",
    "    dict_list = []\n",
    "    for step in steps:\n",
    "      for proc in procs:\n",
    "        data = f[f'Step{step}.dir/Proc{proc}.txt'][0].decode()\n",
    "\n",
    "        lines = data.split(\"\\n\")\n",
    "        time = float((lines[0].split(\"=\")[-1])[:-1])\n",
    "\n",
    "        curr_dict = {\n",
    "            \"t(M)\": time,\n",
    "            \"step\": step,\n",
    "            \"proc\": proc\n",
    "        }\n",
    "        # Find where the columns end\n",
    "        a = lines[4]\n",
    "        event_end = a.find(\"Event\")+5\n",
    "        cum_end = a.find(\"cum(%)\")+6\n",
    "        exc_end = a.find(\"exc(%)\")+6\n",
    "        inc_end = a.find(\"inc(%)\")+6\n",
    "\n",
    "        for line in lines[6:-2]:\n",
    "          Event = line[:event_end].strip()\n",
    "          cum = float(line[event_end:cum_end].strip())\n",
    "          exc = float(line[cum_end:exc_end].strip())\n",
    "          inc = float(line[exc_end:inc_end].strip())\n",
    "          N = int(line[inc_end:].strip())\n",
    "          # print(a)\n",
    "          # a = line.split(\"  \")\n",
    "          # Event,cum,exc,inc,N = [i.strip() for i in a if i!= '']\n",
    "          curr_dict[f'{Event}_cum'] = cum\n",
    "          curr_dict[f'{Event}_exc'] = exc\n",
    "          curr_dict[f'{Event}_inc'] = inc\n",
    "          curr_dict[f'{Event}_N'] = N\n",
    "\n",
    "        dict_list.append(curr_dict)\n",
    "  return pd.DataFrame(dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1940c6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_profiler_multiindex(folder_path:Path):\n",
    "  dir_paths,dat_paths = list_all_dir_and_dat_files(folder_path)\n",
    "  steps = set()\n",
    "  # Get step names\n",
    "  for dir in dir_paths:\n",
    "    step = dir.name.split('.')[0][4:]\n",
    "    steps.add(step)\n",
    "\n",
    "  procs = set()\n",
    "  # Get the proc names\n",
    "  for txt in dir_paths[0].iterdir():\n",
    "    if \".txt\" in txt.name and \"Summary\" not in txt.name:\n",
    "      procs.add(txt.name[4:-4])\n",
    "\n",
    "  dict_list = []\n",
    "  col_names = set()\n",
    "  row_names = []\n",
    "  for step in steps:\n",
    "    for proc in procs:\n",
    "      txt_file_path = folder_path/f'Step{step}.dir/Proc{proc}.txt'\n",
    "\n",
    "      with txt_file_path.open(\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "      time = float((lines[0].split(\"=\")[-1])[:-2])\n",
    "\n",
    "      curr_dict = {\n",
    "          \"time\": time,\n",
    "          \"step\": step,\n",
    "          \"proc\": proc\n",
    "      }\n",
    "\n",
    "      # Find where the columns end\n",
    "      a = lines[4]\n",
    "      event_end = a.find(\"Event\")+5\n",
    "      cum_end = a.find(\"cum(%)\")+6\n",
    "      exc_end = a.find(\"exc(%)\")+6\n",
    "      inc_end = a.find(\"inc(%)\")+6\n",
    "\n",
    "      row_names.append((str(proc),str(time)))\n",
    "\n",
    "      for line in lines[6:-2]:\n",
    "        Event = line[:event_end].strip()\n",
    "        cum = float(line[event_end:cum_end].strip())\n",
    "        exc = float(line[cum_end:exc_end].strip())\n",
    "        inc = float(line[exc_end:inc_end].strip())\n",
    "        N = int(line[inc_end:].strip())\n",
    "        # print(a)\n",
    "        # a = line.split(\"  \")\n",
    "        # Event,cum,exc,inc,N = [i.strip() for i in a if i!= '']\n",
    "        col_names.add(Event)\n",
    "        curr_dict[(\"cum\",Event)] = cum\n",
    "        curr_dict[(\"exc\",Event)] = exc\n",
    "        curr_dict[(\"inc\",Event)] = inc\n",
    "        curr_dict[(\"N\",Event)] = N\n",
    "\n",
    "      dict_list.append(curr_dict)\n",
    "\n",
    "  # Multi index rows\n",
    "  index = pd.MultiIndex.from_tuples(row_names, names=[\"proc\",\"t(M)\"])\n",
    "  df = pd.DataFrame(dict_list,index=index)\n",
    "  \n",
    "  # Multi index cols\n",
    "  multi_index_columns = [(k if isinstance(k, tuple) else (k, '')) for k in df.columns]\n",
    "  df.columns = pd.MultiIndex.from_tuples(multi_index_columns)\n",
    "  df.columns.names = ['metric', 'process']\n",
    "\n",
    "  # data.xs('24', level=\"proc\")['N']\n",
    "  # data.xs('0.511442', level=\"t(M)\")['cum']\n",
    "  # data.xs(('0','0.511442'),level=('proc','t(M)'))\n",
    "  # data.xs('cum',level='metric',axis=1) = data['cum']\n",
    "  # data.xs('MPI::MPreduceAdd(MV<double>)',level='process',axis=1)\n",
    "  # data[data['time']<50]\n",
    "  # data[data['time']<50]['cum'].xs('0',level='proc')['MPI::MPreduceAdd(MV<double>)']\n",
    "  return df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ff6a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_profiler_multiindex(folder_path:Path):\n",
    "  dir_paths,dat_paths = list_all_dir_and_dat_files(folder_path)\n",
    "  steps = set()\n",
    "  # Get step names\n",
    "  for dir in dir_paths:\n",
    "    step = dir.name.split('.')[0][4:]\n",
    "    steps.add(step)\n",
    "\n",
    "  procs = set()\n",
    "  # Get the proc names\n",
    "  for txt in dir_paths[0].iterdir():\n",
    "    if \".txt\" in txt.name and \"Summary\" not in txt.name:\n",
    "      procs.add(txt.name[4:-4])\n",
    "\n",
    "  dict_list = []\n",
    "  col_names = set()\n",
    "  row_names = []\n",
    "  for step in steps:\n",
    "    for proc in procs:\n",
    "      txt_file_path = folder_path/f'Step{step}.dir/Proc{proc}.txt'\n",
    "\n",
    "      with txt_file_path.open(\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "      time = float((lines[0].split(\"=\")[-1])[:-2])\n",
    "\n",
    "      curr_dict = {\n",
    "          \"time\": time,\n",
    "          \"step\": step,\n",
    "          \"proc\": proc\n",
    "      }\n",
    "\n",
    "      # Find where the columns end\n",
    "      a = lines[4]\n",
    "      event_end = a.find(\"Event\")+5\n",
    "      cum_end = a.find(\"cum(%)\")+6\n",
    "      exc_end = a.find(\"exc(%)\")+6\n",
    "      inc_end = a.find(\"inc(%)\")+6\n",
    "\n",
    "      # row_names.append((str(proc),str(time)))\n",
    "\n",
    "      for line in lines[6:-2]:\n",
    "        Event = line[:event_end].strip()\n",
    "        cum = float(line[event_end:cum_end].strip())\n",
    "        exc = float(line[cum_end:exc_end].strip())\n",
    "        inc = float(line[exc_end:inc_end].strip())\n",
    "        N = int(line[inc_end:].strip())\n",
    "        # print(a)\n",
    "        # a = line.split(\"  \")\n",
    "        # Event,cum,exc,inc,N = [i.strip() for i in a if i!= '']\n",
    "        col_names.add(Event)\n",
    "        curr_dict[(\"cum\",Event,str(proc))] = cum\n",
    "        curr_dict[(\"exc\",Event,str(proc))] = exc\n",
    "        curr_dict[(\"inc\",Event,str(proc))] = inc\n",
    "        curr_dict[(\"N\",Event,str(proc))] = N\n",
    "\n",
    "      dict_list.append(curr_dict)\n",
    "\n",
    "  df = pd.DataFrame(dict_list)\n",
    "  \n",
    "  # Multi index cols\n",
    "  multi_index_columns = [pad_tuple(k,3) for k in df.columns]\n",
    "  df.columns = pd.MultiIndex.from_tuples(multi_index_columns)\n",
    "  df.columns.names = ['metric', 'process', 'procs']\n",
    "\n",
    "  # data.xs('24', level=\"proc\")['N']\n",
    "  # data.xs('0.511442', level=\"t(M)\")['cum']\n",
    "  # data.xs(('0','0.511442'),level=('proc','t(M)'))\n",
    "  # data.xs('cum',level='metric',axis=1) = data['cum']\n",
    "  # data.xs('MPI::MPreduceAdd(MV<double>)',level='process',axis=1)\n",
    "  # data[data['time']<50]\n",
    "  # data[data['time']<50]['cum'].xs('0',level='proc')['MPI::MPreduceAdd(MV<double>)']\n",
    "  return df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86848a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6336373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4ed0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_profiler_multiindex(Profiler_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449c98f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.xs(('exc','24'),level=('metric','procs'),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a86200",
   "metadata": {},
   "source": [
    "#### AdjustGridExtent.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4376d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_AdjustGridExtent_multiindex(folder_path:Path):\n",
    "  dir_paths,_ = list_all_dir_and_dat_files(folder_path)\n",
    "  domains = set()\n",
    "  # Get domain names\n",
    "  for dir in dir_paths:\n",
    "    domains.add(dir.name[:-4])\n",
    "\n",
    "    _,dat_paths = list_all_dir_and_dat_files(dir)\n",
    "    \n",
    "\n",
    "  return domains\n",
    "\n",
    "read_AdjustGridExtent_multiindex(AdjustGridExtents_path)\n",
    "  # procs = set()\n",
    "  # # Get the proc names\n",
    "  # for txt in dir_paths[0].iterdir():\n",
    "  #   if \".txt\" in txt.name and \"Summary\" not in txt.name:\n",
    "  #     procs.add(txt.name[4:-4])\n",
    "\n",
    "  # dict_list = []\n",
    "  # col_names = set()\n",
    "  # row_names = []\n",
    "  # for step in steps:\n",
    "  #   for proc in procs:\n",
    "  #     txt_file_path = folder_path/f'Step{step}.dir/Proc{proc}.txt'\n",
    "\n",
    "  #     with txt_file_path.open(\"r\") as f:\n",
    "  #       lines = f.readlines()\n",
    "\n",
    "  #     time = float((lines[0].split(\"=\")[-1])[:-2])\n",
    "\n",
    "  #     curr_dict = {\n",
    "  #         \"time\": time,\n",
    "  #         \"step\": step,\n",
    "  #         \"proc\": proc\n",
    "  #     }\n",
    "\n",
    "  #     # Find where the columns end\n",
    "  #     a = lines[4]\n",
    "  #     event_end = a.find(\"Event\")+5\n",
    "  #     cum_end = a.find(\"cum(%)\")+6\n",
    "  #     exc_end = a.find(\"exc(%)\")+6\n",
    "  #     inc_end = a.find(\"inc(%)\")+6\n",
    "\n",
    "  #     row_names.append((str(proc),str(time)))\n",
    "\n",
    "  #     for line in lines[6:-2]:\n",
    "  #       Event = line[:event_end].strip()\n",
    "  #       cum = float(line[event_end:cum_end].strip())\n",
    "  #       exc = float(line[cum_end:exc_end].strip())\n",
    "  #       inc = float(line[exc_end:inc_end].strip())\n",
    "  #       N = int(line[inc_end:].strip())\n",
    "  #       # print(a)\n",
    "  #       # a = line.split(\"  \")\n",
    "  #       # Event,cum,exc,inc,N = [i.strip() for i in a if i!= '']\n",
    "  #       col_names.add(Event)\n",
    "  #       curr_dict[(\"cum\",Event)] = cum\n",
    "  #       curr_dict[(\"exc\",Event)] = exc\n",
    "  #       curr_dict[(\"inc\",Event)] = inc\n",
    "  #       curr_dict[(\"N\",Event)] = N\n",
    "\n",
    "  #     dict_list.append(curr_dict)\n",
    "\n",
    "  # # Multi index rows\n",
    "  # index = pd.MultiIndex.from_tuples(row_names, names=[\"proc\",\"t(M)\"])\n",
    "  # df = pd.DataFrame(dict_list,index=index)\n",
    "  \n",
    "  # # Multi index cols\n",
    "  # multi_index_columns = [(k if isinstance(k, tuple) else (k, '')) for k in df.columns]\n",
    "  # df.columns = pd.MultiIndex.from_tuples(multi_index_columns)\n",
    "  # df.columns.names = ['metric', 'process']\n",
    "\n",
    "  # data.xs('24', level=\"proc\")['N']\n",
    "  # data.xs('0.511442', level=\"t(M)\")['cum']\n",
    "  # data.xs(('0','0.511442'),level=('proc','t(M)'))\n",
    "  # data.xs('cum',level='metric',axis=1) = data['cum']\n",
    "  # data.xs('MPI::MPreduceAdd(MV<double>)',level='process',axis=1)\n",
    "  # return df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba9755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03ff995b",
   "metadata": {},
   "source": [
    "# Checkpoint files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4186d181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "def read_h5_file(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Read HDF5 file and return its contents in a structured dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the HDF5 file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Nested dictionary containing all the datasets from the file\n",
    "    \"\"\"\n",
    "    \n",
    "    def read_group(group) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Recursively read groups and datasets.\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        \n",
    "        # Read all datasets in current group\n",
    "        for name, item in group.items():\n",
    "            if isinstance(item, h5py.Dataset):\n",
    "                # Convert dataset to numpy array\n",
    "                result[name] = item[()]\n",
    "            elif isinstance(item, h5py.Group):\n",
    "                # Recursively read nested group\n",
    "                result[name] = read_group(item)\n",
    "                \n",
    "        return result\n",
    "\n",
    "    # Check if file exists\n",
    "    if not Path(file_path).exists():\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist\")\n",
    "\n",
    "    try:\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            # Read all contents\n",
    "            data = {}\n",
    "            \n",
    "            # Read main groups\n",
    "            for group_name in ['InitGridHi', 'InitHhatt', 'kappa', 'psi']:\n",
    "                if group_name in f:\n",
    "                    data[group_name] = read_group(f[group_name])\n",
    "                    \n",
    "        return data\n",
    "    \n",
    "    except OSError as e:\n",
    "        raise OSError(f\"Error reading HDF5 file: {str(e)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb2b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = Path(\"/groups/sxs/hchaudha/scripts/dealing_with_h5_files/del/4882/Cp-VarsGr_SphereA0.h5\")\n",
    "data = read_h5_file(file_path)\n",
    "\n",
    "# Example of accessing data\n",
    "print(\"Available groups:\", list(data.keys()))\n",
    "\n",
    "# Example: accessing coordinates if they exist\n",
    "if 'InitGridHi' in data and 'Step000000' in data['InitGridHi']:\n",
    "    coords = data['InitGridHi']['Step000000']\n",
    "    if 'x' in coords:\n",
    "        print(\"\\nShape of x coordinates:\", coords['x'].shape)\n",
    "        \n",
    "# Example: accessing kappa components\n",
    "if 'kappa' in data and 'Step000000' in data['kappa']:\n",
    "    kappa_data = data['kappa']['Step000000']\n",
    "    print(\"\\nAvailable kappa components:\", list(kappa_data.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a04a4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['psi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2758b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['psi']['Step000000']['tt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f284b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['psi']['Step000000']['xx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3006bf",
   "metadata": {},
   "source": [
    "# CCE vol data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7746681",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"/groups/sxs/hchaudha/spec_runs/single_bh_CCE/runs/obs_vol_data/rad_2500/red_cce.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ded74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_vars = set()\n",
    "max_val = {}\n",
    "R_pts = 0\n",
    "with h5py.File(data_path,'r') as f:\n",
    "    names = []\n",
    "    f.visit(names.append)\n",
    "    # print(f['Cce/VolumeData/BondiBeta/CompactifiedRadius_0.dat'].shape)\n",
    "    for name in names:\n",
    "        # if 'VolumeData/' in name and 'CompactifiedRadius' not in name:\n",
    "        #     vol_vars.add(name.split('/')[-1])\n",
    "        #     print(name)\n",
    "        if 'CompactifiedRadius' in name:\n",
    "            # max_val[name] = np.max(np.abs(f[name][0,1:]),axis=1)\n",
    "            max_val[name] = f[name][0,1:]\n",
    "            print(name)\n",
    "\n",
    "    max_val['Cce/VolumeData/InertialRetardedTime.dat'] = np.array(f['Cce/VolumeData/InertialRetardedTime.dat'])\n",
    "    max_val['Cce/VolumeData/OneMinusY.dat'] = np.array(f['Cce/VolumeData/OneMinusY.dat'])\n",
    "\n",
    "    # data = f['Cce/VolumeData/W/CompactifiedRadius_0.dat'][()]\n",
    "\n",
    "# vol_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e556e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(h5path:Path):\n",
    "    with h5py.File(h5path,'r') as f:\n",
    "        names = []\n",
    "        f.visit(names.append)\n",
    "    var_names = set()\n",
    "    num_comp_rad = 0\n",
    "    for name in names:\n",
    "        if 'VolumeData/' in name and 'CompactifiedRadius' not in name:\n",
    "            var_names.add(name.split('/')[-1])\n",
    "        if 'VolumeData/' in name and 'CompactifiedRadius' in name:\n",
    "            num_comp_rad = max(num_comp_rad,int(name.split(\"CompactifiedRadius_\")[-1].split('.')[0]))\n",
    "\n",
    "    return var_names,num_comp_rad\n",
    "\n",
    "var_names,num_comp_rad = get_info(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a27d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_all_comp_rad(h5_datapath:Path, var_name:str, comp_rad_list:list, time_slice:slice , red_func=np.linalg.norm):\n",
    "    with h5py.File(h5_datapath,'r') as f:\n",
    "        data = {}\n",
    "        for comp_rad in comp_rad_list:\n",
    "            curr_data = f[f'Cce/VolumeData/{var_name}/CompactifiedRadius_{comp_rad}.dat'][time_slice,1:]\n",
    "            data[comp_rad] = red_func(curr_data,axis=1)\n",
    "        t = f[f'Cce/VolumeData/{var_name}/CompactifiedRadius_0.dat'][time_slice,0]\n",
    "    return t,data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d005cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"/groups/sxs/hchaudha/spec_runs/single_bh_CCE/runs/obs_vol_data/rad_0100/red_cce.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46bb302",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_name = 'Q'\n",
    "t,var_data = get_data_all_comp_rad(data_path, var_name, range(num_comp_rad), slice(0,-1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802e3545",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_data[0]\n",
    "\n",
    "for comp_rad in var_data:\n",
    "    plt.plot(t,var_data[comp_rad],label=f'CompactifiedRadius_{comp_rad}')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel(f'L2({var_name})')\n",
    "plt.title(\"Extraction radius: \" + str(data_path).split(\"/\")[-2][4:] + \"M\")\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02269563",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"/groups/sxs/hchaudha/spec_runs/single_bh_CCE/runs/obs_vol_data/rad_0100/red_cce.h5\")\n",
    "data_path = Path(\"/groups/sxs/hchaudha/spec_runs/single_bh_CCE/runs/obs_vol_data/rad_0100_IC/red_cce.h5\")\n",
    "data_path = Path(\"/groups/sxs/hchaudha/spec_runs/single_bh_CCE/runs/obs_vol_data/rad_0100_ZNS/red_cce.h5\")\n",
    "# data_path = Path(\"/groups/sxs/hchaudha/spec_runs/single_bh_CCE/runs/obs_vol_data/rad_0100_NIR/red_cce.h5\")\n",
    "with h5py.File(data_path,'r') as f:\n",
    "    for var in [\"BondiBeta\", \"Du(J)\", \"DuRDividedByR\", \"Dy(BondiBeta)\", \"Dy(Du(J))\", \"Dy(Dy(BondiBeta))\", \"Dy(Dy(Du(J)))\", \"Dy(Dy(J))\", \"Dy(Dy(Q))\", \"Dy(Dy(U))\", \"Dy(Dy(W))\", \"Dy(H)\", \"Dy(J)\", \"Dy(Q)\", \"Dy(U)\", \"Dy(W)\", \"EthRDividedByR\", \"H\", \"J\", \"Psi0\", \"Psi1\", \"Q\", \"R\", \"U\", \"W\"]:\n",
    "    # for var in ['J','Q','H','U','W','BondiBeta']:\n",
    "        max_val = np.max(np.abs(f[f'Cce/VolumeData/{var}/CompactifiedRadius_0.dat'][0,1:]))\n",
    "        # print(var,max_val)\n",
    "        print(f\"{var:<30} {max_val}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1560e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa780f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val['Cce/VolumeData/InertialRetardedTime.dat'][:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd12c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "2/max_val['Cce/VolumeData/OneMinusY.dat'][0,1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b963bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in max_val.keys():\n",
    "    if \"_0\" in i:\n",
    "        print(i,np.max(max_val[i]),np.min(max_val[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de825a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(max_val['Cce/VolumeData/W/CompactifiedRadius_0.dat'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9a4933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(Path(\"/groups/sxs/hchaudha/scripts/dealing_with_h5_files/max_val_rad_0500.pkl\"), 'wb') as f:\n",
    "#     pickle.dump(max_val, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b28b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = data[:,0]\n",
    "max_val = np.max(np.abs(data[:,1:]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d2b8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t,max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84db601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sxs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
