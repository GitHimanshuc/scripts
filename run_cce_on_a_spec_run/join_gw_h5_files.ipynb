{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "spec_home=\"/home/hchaudha/spec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_list_to_str_list(data):\n",
    "  copy_data = deepcopy(data)\n",
    "  for i,val in enumerate(copy_data):\n",
    "    copy_data[i] = str(copy_data[i])\n",
    "  return copy_data\n",
    "\n",
    "def path_dict_to_str_dict(data):\n",
    "  copy_data = deepcopy(data)\n",
    "  for key in copy_data:\n",
    "    if isinstance(copy_data[key],dict):\n",
    "      copy_data[key] = path_dict_to_str_dict(copy_data[key])\n",
    "    if isinstance(copy_data[key],Path):\n",
    "      copy_data[key] = str(copy_data[key])\n",
    "    if isinstance(copy_data[key],list):\n",
    "      copy_data[key] = path_list_to_str_list(copy_data[key])\n",
    "\n",
    "  return copy_data\n",
    "\n",
    "def sort_paths_by_postfix(paths):\n",
    "  # Define a helper function to extract the postfix for sorting\n",
    "  def extract_postfix(path):\n",
    "      # Split the path by '/' and get the last segment\n",
    "      # Then get the part after the last underscore '_'\n",
    "      if isinstance(path,Path):\n",
    "        return str(path).split('/')[-1].split('_')[-1]\n",
    "      else:\n",
    "        return path.split('/')[-1].split('_')[-1]\n",
    "\n",
    "  # Sort the paths using the postfix as key\n",
    "  sorted_paths = sorted(paths, key=extract_postfix)\n",
    "  return sorted_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_paths_dict = {}\n",
    "lev_paths_dict[\"high_accuracy_L02_master\"] = {\n",
    "  \"Ev_path\": \"/groups/sxs/hchaudha/spec_runs/Lev01_test/old_ode_tol/high_accuracy_L35_master/Ev\",\n",
    "  \"save_path\": \"/groups/sxs/hchaudha/spec_runs/Lev01_test/old_ode_tol/high_accuracy_L35_master/Ev/GW_data\",\n",
    "  \"Lev0\":[ ],\n",
    "  \"Lev1\":[ ],\n",
    "  \"Lev2\":[ ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_paths_dict\n",
    "with open(\"./del.json\",'w') as f:\n",
    "  json.dump(lev_paths_dict,f,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"Lev5_big_gaussian\"\n",
    "Ev_path = Path(f\"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/{run_name}\")\n",
    "save_path = Ev_path/\"GW_data\"\n",
    "\n",
    "lev_paths_dict[run_name] = {\n",
    "  \"Ev_path\":Ev_path,\n",
    "  \"save_path\":save_path,\n",
    "}\n",
    "\n",
    "lev_paths = list(Ev_path.glob(\"Lev?_??\"))\n",
    "levs = set()\n",
    "for paths in lev_paths:\n",
    "  levs.add(str(paths.stem)[3])\n",
    "\n",
    "for lev in levs:\n",
    "  lev_paths = []\n",
    "  lev_paths = lev_paths + list(Ev_path.glob(f\"Lev{lev}_??\"))\n",
    "  lev_paths = lev_paths + list(Ev_path.glob(f\"Lev{lev}_Ringdown/Lev{lev}_??\"))\n",
    "  lev_paths.sort()\n",
    "  lev_paths_dict[run_name][f\"Lev{lev}\"] = path_list_to_str_list(lev_paths)\n",
    "\n",
    "# save_path.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "with (save_path/\"paths.json\").open('w') as f:\n",
    "  json.dump(path_dict_to_str_dict(lev_paths_dict),f,indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write paths of levs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_joinH5_command(save_folder:Path, path_list:list[Path], output_file_name:str, write_scripts_only=False):\n",
    "  file_list_str = \"\"\n",
    "  for file_path in path_list:\n",
    "    file_list_str += f\" {file_path}\"\n",
    "\n",
    "  command = f\"cd {save_folder} && {spec_home}/Support/bin/JoinH5 -o {output_file_name} {file_list_str}\"\n",
    "  with open(save_folder/\"joinH5_commands.sh\",'w') as f:\n",
    "    submit_file =\\\n",
    "f\"\"\"#!/bin/bash -\n",
    "# run JoinH5\n",
    "{command}\n",
    "\"\"\"\n",
    "    f.writelines(submit_file)\n",
    "\n",
    "  if not write_scripts_only:\n",
    "    status = subprocess.run(command, capture_output=True, shell=True, text=True)\n",
    "    if status.returncode == 0:\n",
    "      print(f\"Succesfully saved joined h5 file {output_file_name} in {save_folder}\")\n",
    "    else:\n",
    "      sys.exit(\n",
    "          f\"JoinH5 failed in {save_folder} with error: \\n {status.stderr}\")\n",
    "\n",
    "def get_GW_data_for_levs(lev_path_list:list[Path]):\n",
    "  # NOTE! No sorting is done in this function. It is important that the lev_path_list is in the correct order which will be passed to the JoinH5\n",
    "  GW_data = dict()\n",
    "  bondi_radii_list = set()\n",
    "  # This uses the first path to get the list of the radii, which could be different from the list of radii in the subsequent folders in case fo checkpoints restarts\n",
    "  for bondi_path in list(lev_path_list[0].glob(\"Run/GW2/Bondi*.h5\")):\n",
    "    bondi_radii_list.add(bondi_path.stem[-4:])\n",
    "\n",
    "  GW_data[f\"bondi_radii_list\"] = list(bondi_radii_list)\n",
    "  GW_data[f\"PhiMinus_FiniteRadii_CodeUnits\"] = []\n",
    "  GW_data[f\"PhiPlus_FiniteRadii_CodeUnits\"] = []\n",
    "  GW_data[f\"RawStrahlkorperIntegrals\"] = []\n",
    "  GW_data[f\"rh_FiniteRadii_CodeUnits\"] = []\n",
    "  GW_data[f\"rPsi4_FiniteRadii_CodeUnits\"] = []\n",
    "  for bondi_radii in bondi_radii_list:\n",
    "    GW_data[f\"BondiCceR{bondi_radii}\"] = []\n",
    "\n",
    "  def append_if_exists(path_list:list[Path],path:Path):\n",
    "    if path.exists():\n",
    "      path_list.append(path)\n",
    "    else:\n",
    "      print(f\"{path=} does not exist. Skipping it!\")\n",
    "\n",
    "  for lev_path in lev_path_list:\n",
    "    append_if_exists(GW_data[f\"PhiMinus_FiniteRadii_CodeUnits\"],Path(f\"{lev_path}/Run/GW2/PhiMinus_FiniteRadii_CodeUnits.h5\"))\n",
    "    append_if_exists(GW_data[f\"PhiPlus_FiniteRadii_CodeUnits\"],Path(f\"{lev_path}/Run/GW2/PhiPlus_FiniteRadii_CodeUnits.h5\"))\n",
    "    append_if_exists(GW_data[f\"RawStrahlkorperIntegrals\"],Path(f\"{lev_path}/Run/GW2/RawStrahlkorperIntegrals.h5\"))\n",
    "    append_if_exists(GW_data[f\"rh_FiniteRadii_CodeUnits\"],Path(f\"{lev_path}/Run/GW2/rh_FiniteRadii_CodeUnits.h5\"))\n",
    "    append_if_exists(GW_data[f\"rPsi4_FiniteRadii_CodeUnits\"],Path(f\"{lev_path}/Run/GW2/rPsi4_FiniteRadii_CodeUnits.h5\"))\n",
    "    for bondi_radii in bondi_radii_list:\n",
    "      append_if_exists(GW_data[f\"BondiCceR{bondi_radii}\"],Path(f\"{lev_path}/Run/GW2/BondiCceR{bondi_radii}.h5\"))\n",
    "\n",
    "  return GW_data\n",
    "\n",
    "def make_config_file(BoundaryDataPath: Path,\n",
    "                     InputSavePath: Path = None) -> Path :\n",
    "        \n",
    "    if InputSavePath is None:\n",
    "        InputSavePath = BoundaryDataPath.parent/\"cce.yaml\"\n",
    "    assert(InputSavePath.parent.exists())\n",
    "\n",
    "    config_file=\\\n",
    "f\"\"\"\n",
    "# Distributed under the MIT License.\n",
    "# See LICENSE.txt for details.\n",
    "\n",
    "# This block is used by testing and the SpECTRE command line interface.\n",
    "Executable: CharacteristicExtract\n",
    "Testing:\n",
    "  Check: parse\n",
    "  Priority: High\n",
    "\n",
    "---\n",
    "Evolution:\n",
    "  InitialTimeStep: 0.25\n",
    "  InitialSlabSize: 10.0\n",
    "\n",
    "ResourceInfo:\n",
    "  AvoidGlobalProc0: false\n",
    "  Singletons: Auto\n",
    "\n",
    "Observers:\n",
    "  VolumeFileName: \"vol_{str(InputSavePath.stem)}\"\n",
    "  ReductionFileName: \"red_{str(InputSavePath.stem)}\"\n",
    "\n",
    "EventsAndTriggers:\n",
    "  # Write the CCE time step every Slab. A Slab is a fixed length of simulation\n",
    "  # time and is not influenced by the dynamically adjusted step size.\n",
    "  - Trigger:\n",
    "      Slabs:\n",
    "        EvenlySpaced:\n",
    "          Offset: 0\n",
    "          Interval: 1\n",
    "    Events:\n",
    "      - ObserveTimeStep:\n",
    "          # The output is written into the \"ReductionFileName\" HDF5 file under\n",
    "          # \"/SubfileName.dat\"\n",
    "          SubfileName: CceTimeStep\n",
    "          PrintTimeToTerminal: true\n",
    "\n",
    "Cce:\n",
    "  Evolution:\n",
    "    TimeStepper:\n",
    "      AdamsBashforth:\n",
    "        Order: 3 # Going to higher order doesn't seem necessary for CCE\n",
    "    StepChoosers:\n",
    "      - Constant: 0.1 # Don't take steps bigger than 0.1M\n",
    "      - Increase:\n",
    "          Factor: 2\n",
    "      - ErrorControl(SwshVars):\n",
    "          AbsoluteTolerance: 1e-8\n",
    "          RelativeTolerance: 1e-6\n",
    "          # These factors control how much the time step is changed at once.\n",
    "          MaxFactor: 2\n",
    "          MinFactor: 0.25\n",
    "          # How close to the \"perfect\" time step we take. Since the \"perfect\"\n",
    "          # value assumes a linear system, we need some safety factor since our\n",
    "          # system is nonlinear, and also so that we reduce how often we retake\n",
    "          # time steps.\n",
    "          SafetyFactor: 0.9\n",
    "      - ErrorControl(CoordVars):\n",
    "          AbsoluteTolerance: 1e-8\n",
    "          RelativeTolerance: 1e-7\n",
    "          # These factors control how much the time step is changed at once.\n",
    "          MaxFactor: 2\n",
    "          MinFactor: 0.25\n",
    "          # How close to the \"perfect\" time step we take. Since the \"perfect\"\n",
    "          # value assumes a linear system, we need some safety factor since our\n",
    "          # system is nonlinear, and also so that we reduce how often we retake\n",
    "          # time steps.\n",
    "          SafetyFactor: 0.9\n",
    "\n",
    "  # The number of angular modes used by the CCE evolution. This must be larger\n",
    "  # than ObservationLMax. We always use all of the m modes for the LMax since\n",
    "  # using fewer m modes causes aliasing-driven instabilities.\n",
    "  LMax: 20\n",
    "  # Probably don't need more than 15 radial grid points, but could increase\n",
    "  # up to ~20\n",
    "  NumberOfRadialPoints: 15\n",
    "  # The maximum ell we use for writing waveform output. While CCE can dump\n",
    "  # more, you should be cautious with higher modes since mode mixing, truncation\n",
    "  # error, and systematic numerical effects can have significant contamination\n",
    "  # in these modes.\n",
    "  ObservationLMax: 8\n",
    "\n",
    "  InitializeJ:\n",
    "    # To see what other J-initialization procedures are available, comment\n",
    "    # out this group of options and do, e.g. \"Blah:\" The code will print\n",
    "    # an error message with the available options and a help string.\n",
    "    # More details can be found at spectre-code.org.\n",
    "    ConformalFactor:\n",
    "      AngularCoordTolerance: 1e-13\n",
    "      MaxIterations: 1000 # Do extra iterations in case we improve.\n",
    "      RequireConvergence: False # Often don't converge to 1e-13, but that's fine\n",
    "      OptimizeL0Mode: True\n",
    "      UseBetaIntegralEstimate: False\n",
    "      ConformalFactorIterationHeuristic: SpinWeight1CoordPerturbation\n",
    "      UseInputModes: False\n",
    "      InputModes: []\n",
    "\n",
    "  StartTime: Auto\n",
    "  EndTime: Auto\n",
    "  ExtractionRadius: Auto\n",
    "\n",
    "  BoundaryDataFilename: {BoundaryDataPath.name}\n",
    "  H5IsBondiData: True\n",
    "  H5Interpolator:\n",
    "    BarycentricRationalSpanInterpolator:\n",
    "      MinOrder: 10\n",
    "      MaxOrder: 10\n",
    "  FixSpecNormalization: False\n",
    "\n",
    "  H5LookaheadTimes: 10000\n",
    "\n",
    "  Filtering:\n",
    "    RadialFilterHalfPower: 64\n",
    "    RadialFilterAlpha: 35.0\n",
    "    FilterLMax: 18\n",
    "\n",
    "  ScriInterpOrder: 5\n",
    "  ScriOutputDensity: 1\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    with InputSavePath.open('w') as f:\n",
    "        f.writelines(config_file)\n",
    "\n",
    "    return InputSavePath\n",
    "\n",
    "def make_submit_file(save_folder_path:Path,cce_input_file_path:Path, CCE_Executable_path:Path,write_scripts_only=False):\n",
    "  submit_script=\\\n",
    "f\"\"\"#!/bin/bash -\n",
    "#SBATCH -J CCE_{save_folder_path.stem}             # Job Name\n",
    "#SBATCH -o CCE.stdout                 # Output file name\n",
    "#SBATCH -e CCE.stderr                 # Error file name\n",
    "#SBATCH -n 4                          # Number of cores\n",
    "#SBATCH -p expansion                  # Queue name\n",
    "#SBATCH --ntasks-per-node 4           # number of MPI ranks per node\n",
    "#SBATCH -t 24:0:00   # Run time\n",
    "#SBATCH -A sxs                # Account name\n",
    "#SBATCH --no-requeue\n",
    "#SBATCH --reservation=sxs_standing\n",
    "#SBATCH --constraint=skylake\n",
    "\n",
    "# Go to the correct folder with the boundary data\n",
    "cd {save_folder_path}\n",
    "\n",
    "# run CCE\n",
    "{CCE_Executable_path} --input-file ./{cce_input_file_path.name}\n",
    "\"\"\"\n",
    "  submit_script_path = save_folder_path/\"submit.sh\"\n",
    "  submit_script_path.write_text(submit_script)\n",
    "\n",
    "  if not write_scripts_only:\n",
    "    command = f\"cd {save_folder_path} && qsub {submit_script_path}\"\n",
    "    status = subprocess.run(command, capture_output=True, shell=True, text=True)\n",
    "    if status.returncode == 0:\n",
    "      print(f\"Succesfully submitted {submit_script_path}\\n{status.stdout}\")\n",
    "    else:\n",
    "      sys.exit(\n",
    "          f\"Job submission failed for {submit_script_path} with error: \\n{status.stdout} \\n{status.stderr}\")\n",
    "\n",
    "def serialize_and_dump_dict(data_dict:dict, save_path:Path):\n",
    "  def serialize(data):\n",
    "      \"\"\"Recursively serialize Path objects and convert sets to lists.\"\"\"\n",
    "      if isinstance(data, dict):\n",
    "          return {key: serialize(value) for key, value in data.items()}\n",
    "      elif isinstance(data, list):\n",
    "          return [serialize(item) for item in data]\n",
    "      elif isinstance(data, set):\n",
    "          return list(serialize(item) for item in data)  # Convert set to list\n",
    "      elif isinstance(data, Path):\n",
    "          return str(data)  # Convert Path to string\n",
    "      return data  # Return unmodified if it's not a Path, dict, list, or set\n",
    "\n",
    "  # Serialize the data\n",
    "  serialized_data = serialize(data_dict)\n",
    "\n",
    "  # Write the serialized data to a JSON file\n",
    "  with open(save_path, 'w') as json_file:\n",
    "      json.dump(serialized_data, json_file, indent=2)\n",
    "\n",
    "def write_GW_data_dict(Ev_path:Path, CCE_Executable_path:Path, Ev_lev_path_glob:str=None, save_path:Path=None, previous_levs_list:list[Path]=None,write_scripts_only:bool=False):\n",
    "\n",
    "  if save_path is None:\n",
    "    save_path = Ev_path/\"GW_data\"\n",
    "  # Create save folder\n",
    "  save_path.mkdir()\n",
    "\n",
    "  if previous_levs_list is None:\n",
    "    has_previous_levs = False\n",
    "    previous_levs_list = []\n",
    "  else:\n",
    "  # If there are previous levs then check that the paths are correct\n",
    "    has_previous_levs = True\n",
    "    for previous_lev_path in previous_levs_list:\n",
    "      if not previous_lev_path.exists():\n",
    "        raise Exception(f\"{previous_lev_path=} does not exist!\")\n",
    "\n",
    "  GW_data_dict= {\n",
    "    \"Ev_path\":Ev_path,\n",
    "    \"Ev_lev_path_glob\":Ev_lev_path_glob,\n",
    "    \"save_path\":save_path,\n",
    "    \"has_previous_levs\": has_previous_levs,\n",
    "  }\n",
    "\n",
    "  # Find all the levs in the Ev folder, if Ev_lev_path_glob is not None then use it for globbing\n",
    "  if Ev_lev_path_glob is None:\n",
    "    Ev_lev_path_glob = \"Lev?_??\"\n",
    "\n",
    "  Ev_lev_paths = list(Ev_path.glob(Ev_lev_path_glob))\n",
    "  Ev_ringdown_lev_paths = list(Ev_path.glob(\"Lev?_Ringdown/\"+Ev_lev_path_glob))\n",
    "  levs = set()\n",
    "  for paths in Ev_lev_paths:\n",
    "    levs.add(str(paths.stem)[3])\n",
    "\n",
    "  if has_previous_levs and len(levs)>1:\n",
    "    raise Exception(f\"{previous_levs_set=} is given, use Ev_lev_path_glob to select one specific lev in the {Ev_path=}\")\n",
    "  \n",
    "  if has_previous_levs:\n",
    "    # Find all the levs in the previous_lev_list\n",
    "    # NOTE! Right now only one lev is supported in the previous_lev_list and that requires that Ev_lev_path_glob selects that Lev\n",
    "    previous_levs_set = set()\n",
    "    for path in previous_levs_list:\n",
    "      previous_levs_set.add(str(path.stem)[3])\n",
    "\n",
    "    if len(previous_levs_set)>1:\n",
    "      # User gave multiple levs in the preivous lev list, this is not supported directly!\n",
    "      raise Exception(f\"{previous_levs_list=} has more than one lev {previous_levs_set=} which is not supported!\")\n",
    "    \n",
    "    if levs != previous_levs_set:\n",
    "      raise Exception(f\"Levs globbed from {Ev_path=}:\\n {levs=} \\n are different from the levs given in the {previous_levs_set=}:\\n {previous_levs_set=}\")\n",
    "\n",
    "  GW_data_dict['levs'] = list(levs)\n",
    "  for lev in levs:\n",
    "    # Add the lev paths in increasing time order\n",
    "    GW_data_dict[f'Lev{lev}_dir_list'] = sort_paths_by_postfix(Ev_lev_paths)\n",
    "    GW_data_dict[f'Lev{lev}_dir_list'] = GW_data_dict[f'Lev{lev}_dir_list']+sort_paths_by_postfix(Ev_ringdown_lev_paths)\n",
    "\n",
    "    # Prepend the previous levs if given\n",
    "    if has_previous_levs:\n",
    "      GW_data_dict[f'Lev{lev}_dir_list'] = sort_paths_by_postfix(previous_levs_list)+GW_data_dict[f'Lev{lev}_dir_list']\n",
    "\n",
    "    # Add the GW data for this lev\n",
    "    GW_data_dict[f\"Lev{lev}_GW_data\"] = get_GW_data_for_levs(GW_data_dict[f'Lev{lev}_dir_list'])\n",
    "\n",
    "    # Create folders to save the data\n",
    "    for key in GW_data_dict[f\"Lev{lev}_GW_data\"]:\n",
    "      if key == \"bondi_radii_list\":\n",
    "        continue\n",
    "      else:\n",
    "        save_folder = save_path/key\n",
    "        save_folder.mkdir()\n",
    "        joined_data_name = key+\".h5\" # CCE needs the radius to be bw R and .h5\n",
    "        submit_joinH5_command(save_folder,GW_data_dict[f\"Lev{lev}_GW_data\"][key],joined_data_name,write_scripts_only)\n",
    "\n",
    "        # For bondi cce data create the yaml files and submission scripts for the cce as well\n",
    "        if \"Bondi\" in key:\n",
    "          cce_input_file_path = make_config_file(save_folder/joined_data_name)\n",
    "          make_submit_file(save_folder,cce_input_file_path,CCE_Executable_path,write_scripts_only)\n",
    "\n",
    "    serialize_and_dump_dict(GW_data_dict,save_path/\"GW_data_dict.json\")\n",
    "  return GW_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ev_path = Path(f\"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev5_big_gaussian\")\n",
    "Ev_path = Path(f\"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev5_big_gaussian_constra_200\")\n",
    "previous_levs_list = [\n",
    "        Path(\"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev5_AA\"),\n",
    "        Path(\"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev5_AB\"),\n",
    "        Path(\"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev5_AC\"),\n",
    "        Path(\"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev5_big_gaussian/Lev5_AD\"),\n",
    "      ]\n",
    "CCE_executable = Path(\"/groups/sxs/hchaudha/spec_runs/CCE_stuff/CceExecutables/CharacteristicExtract\")\n",
    "asd = write_GW_data_dict(Ev_path,Ev_lev_path_glob=\"Lev5_?[A-Z]\",previous_levs_list=previous_levs_list,write_scripts_only=True,CCE_Executable_path=CCE_executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_list = [\n",
    "  \"Lev5_big_gaussian\",\n",
    "  \"Lev5_big_gaussian_ah_tol10\",\n",
    "  \"Lev5_big_gaussian_ah_tol100\",\n",
    "  \"Lev5_big_gaussian_constra\",\n",
    "  \"Lev5_big_gaussian_constra_200\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run_name in runs_list:\n",
    "  lev_paths_dict = {}\n",
    "  Ev_path = Path(f\"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/{run_name}\")\n",
    "  save_path = Ev_path/\"GW_data\"\n",
    "\n",
    "  lev_paths_dict[run_name] = {\n",
    "    \"Ev_path\":Ev_path,\n",
    "    \"save_path\":save_path,\n",
    "  }\n",
    "\n",
    "  lev_paths = list(Ev_path.glob(\"Lev?_??\"))\n",
    "  levs = set()\n",
    "  for paths in lev_paths:\n",
    "    levs.add(str(paths.stem)[3])\n",
    "\n",
    "  # save_path.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "  for lev in levs:\n",
    "    lev_paths = []\n",
    "    lev_paths = lev_paths + list(Ev_path.glob(f\"Lev{lev}_??\"))\n",
    "    lev_paths = lev_paths + list(Ev_path.glob(f\"Lev{lev}_Ringdown/Lev{lev}_??\"))\n",
    "    lev_paths.sort()\n",
    "    lev_paths_dict[run_name][f\"Lev{lev}\"] = path_list_to_str_list(lev_paths)\n",
    "\n",
    "  # with (save_path/\"paths.json\").open('w') as f:\n",
    "  #   json.dump(path_dict_to_str_dict(lev_paths_dict),f,indent=2)\n",
    "\n",
    "  with (save_path/\"paths.json\").open('w') as f:\n",
    "    json.dump(path_dict_to_str_dict(lev_paths_dict),f,indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write JoinH5 and write files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_paths_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lev in [0,1,2]:\n",
    "  lev_paths = []\n",
    "  lev_paths = lev_paths + list(Path(f\"/groups/sxs/hchaudha/spec_runs/Lev01_test/old_ode_tol/high_accuracy_L35_master/Ev\").glob(f\"Lev{lev}_??\"))\n",
    "  lev_paths = lev_paths + list(Path(f\"/groups/sxs/hchaudha/spec_runs/Lev01_test/old_ode_tol/high_accuracy_L35_master/Ev\").glob(f\"Lev{lev}_Ringdown/Lev{lev}_??\"))\n",
    "  lev_paths.sort()\n",
    "  path_list_to_str_list(lev_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GW_data = dict()\n",
    "GW_data[\"bondi\"] = list(Ev_path.glob(\"Lev?_??/Run/GW2/Bondi*.h5\"))\n",
    "GW_data[\"PhiMinus_FiniteRadii_CodeUnits\"] = list(Ev_path.glob(\"Lev?_??/Run/GW2/PhiMinus_FiniteRadii_CodeUnits.h5\"))\n",
    "GW_data[\"PhiPlus_FiniteRadii_CodeUnits\"] = list(Ev_path.glob(\"Lev?_??/Run/GW2/PhiPlus_FiniteRadii_CodeUnits.h5\"))\n",
    "GW_data[\"RawStrahlkorperIntegrals\"] = list(Ev_path.glob(\"Lev?_??/Run/GW2/RawStrahlkorperIntegrals.h5\"))\n",
    "GW_data[\"rh_FiniteRadii_CodeUnits\"] = list(Ev_path.glob(\"Lev?_??/Run/GW2/rh_FiniteRadii_CodeUnits.h5\"))\n",
    "GW_data[\"rPsi4_FiniteRadii_CodeUnits\"] = list(Ev_path.glob(\"Lev?_??/Run/GW2/rPsi4_FiniteRadii_CodeUnits.h5\"))\n",
    "\n",
    "for key in GW_data:\n",
    "  GW_data[key].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GW_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sxs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
