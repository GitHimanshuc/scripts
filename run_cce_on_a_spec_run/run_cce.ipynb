{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.integrate as integrate\n",
    "import scipy\n",
    "import sxs\n",
    "import h5py\n",
    "import scri\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import shutil\n",
    "import sys\n",
    "from numba import njit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "from scipy import interpolate\n",
    "import glob\n",
    "from spherical_functions import LM_index as lm\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 10)\n",
    "spec_home = \"/home/himanshu/spec/my_spec\"\n",
    "matplotlib.matplotlib_fname()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to convert the data to scri format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def block_mode_data_to_ylm_timeseries(volume_file: Path, output_dir_name: Path, output_prefix):\n",
    "    def LM_index(L, M):\n",
    "        return 2 * (L**2 + L + M) + 1\n",
    "\n",
    "    with h5py.File(volume_file, 'r') as input_h5:\n",
    "        for i, dataset in enumerate(input_h5):\n",
    "            if \"Version\" in dataset or \"tar.gz\" in dataset:\n",
    "                continue\n",
    "\n",
    "            for variable in input_h5[dataset]:\n",
    "                if i == 0:\n",
    "                    min_time = input_h5[dataset][variable][0, 0]\n",
    "                    max_time = input_h5[dataset][variable][-1, 0]\n",
    "                else:\n",
    "                    if min_time < input_h5[dataset][variable][0, 0]:\n",
    "                        min_time = input_h5[dataset][variable][0, 0]\n",
    "                    if max_time > input_h5[dataset][variable][-1, 0]:\n",
    "                        max_time = input_h5[dataset][variable][-1, 0]\n",
    "\n",
    "        for dataset in input_h5:\n",
    "            if \"Version\" in dataset or \"tar.gz\" in dataset:\n",
    "                continue\n",
    "\n",
    "            for variable in input_h5[dataset]:\n",
    "                idx1 = np.argmin(\n",
    "                    abs(input_h5[dataset][variable][:, 0] - min_time))\n",
    "                idx2 = np.argmin(\n",
    "                    abs(input_h5[dataset][variable][:, 0] - max_time))\n",
    "                data_array = input_h5[dataset][variable][idx1:idx2]\n",
    "\n",
    "                # sort the array according to the time\n",
    "                data_array = data_array[data_array[:, 0].argsort()]\n",
    "                number_of_columns = data_array.shape[1]\n",
    "                L_max = int(np.sqrt((number_of_columns - 1) / 2 - 1))\n",
    "                with h5py.File(output_dir_name + variable[:-4] + output_prefix + \".h5\", 'w') as output_h5:\n",
    "                    for L in range(L_max + 1):\n",
    "                        for M in range(-L, L + 1):\n",
    "                            output_h5.create_dataset(\n",
    "                                \"/Y_l\" + str(L) + \"_m\" + str(M) + \".dat\",\n",
    "                                data=np.append(\n",
    "                                    data_array[:, 0:1],\n",
    "                                    data_array[:, LM_index(\n",
    "                                        L, M):LM_index(L, M) + 2],\n",
    "                                    axis=1))\n",
    "\n",
    "\n",
    "def make_variables_dimensionless(WM, ChMass=None, metadata_filename=None):\n",
    "    if WM.m_is_scaled_out:\n",
    "        raise ValueError(\"Data is already dimensionless!\")\n",
    "    if (ChMass is None and metadata_filename is None):\n",
    "        raise ValueError(\n",
    "            \"Either ChMass OR metadata_filename must be supplied.\")\n",
    "    elif (ChMass is not None and metadata_filename is not None):\n",
    "        raise ValueError(\n",
    "            \"Either ChMass OR metadata_filename must be supplied, but not both.\")\n",
    "\n",
    "    if ChMass is None:\n",
    "        metadata = sxs.metadata.Metadata.from_file(metadata_filename)\n",
    "        mass1 = metadata['reference-mass1']\n",
    "        mass2 = metadata['reference-mass2']\n",
    "        ChMass = float(mass1) + float(mass2)\n",
    "\n",
    "    if WM.dataType in [scri.psi4, scri.psi3, scri.psi2, scri.psi1, scri.psi0]:\n",
    "        unit_scale_factor = (ChMass)**(WM.dataType-4)\n",
    "    elif WM.dataType == scri.h:\n",
    "        unit_scale_factor = 1/ChMass\n",
    "    elif WM.dataType == scri.hdot:\n",
    "        unit_scale_factor = 1.0\n",
    "    else:\n",
    "        raise ValueError(\"DataType not determined.\")\n",
    "\n",
    "    WM.t = WM.t / ChMass\n",
    "    WM.data = WM.data * unit_scale_factor\n",
    "    WM.m_is_scaled_out = True\n",
    "\n",
    "\n",
    "def plot_and_save_bianchi_violations(violation_dict: dict, save_dir: Path):\n",
    "    plt.semilogy(violation_dict['t'], violation_dict['5'], label='5')\n",
    "    plt.semilogy(violation_dict['t'], violation_dict['4'], label='4')\n",
    "    plt.semilogy(violation_dict['t'], violation_dict['3'], label='3')\n",
    "    plt.semilogy(violation_dict['t'], violation_dict['2'], label='2')\n",
    "    plt.semilogy(violation_dict['t'], violation_dict['1'], label='1')\n",
    "    plt.semilogy(violation_dict['t'], violation_dict['0'], label='0')\n",
    "    plt.xlabel('t')\n",
    "    plt.ylabel(\"violations\")\n",
    "    plt.legend()\n",
    "    plt.savefig(save_dir/\"violations.png\")\n",
    "\n",
    "\n",
    "def post_process_jobs(path_dict, output_dir_name=\"extracted_data\"):\n",
    "    cce_scri_data_names = {\n",
    "        'Strain': (scri.h, 'rhOverM'),\n",
    "        'News': (scri.hdot, 'r2News'),\n",
    "        'Psi4': (scri.psi4, 'rMPsi4'),\n",
    "        'Psi3': (scri.psi3, 'r2Psi3'),\n",
    "        'Psi2': (scri.psi2, 'r3Psi2OverM'),\n",
    "        'Psi1': (scri.psi1, 'r4Psi1OverM2'),\n",
    "        'Psi0': (scri.psi0, 'r5Psi0OverM3')\n",
    "    }\n",
    "\n",
    "    bianchi_violations = {}\n",
    "    for bd_data_path in path_dict['boundary_data_paths']:\n",
    "        bd_folder_path = bd_data_path.parent\n",
    "\n",
    "        directory = str(bd_folder_path)+\"/\"\n",
    "        radius = bd_folder_path.stem[-4:]\n",
    "\n",
    "        os.system(f'rm -r {directory}/{output_dir_name} 2> /dev/null')\n",
    "        os.system(f'mkdir {directory}/{output_dir_name}')\n",
    "\n",
    "        block_mode_data_to_ylm_timeseries(bd_data_path,\n",
    "                                          f'{directory}{output_dir_name}/',\n",
    "                                          f'_BondiCce_R{radius}_unprocessed')\n",
    "\n",
    "        variables = {}\n",
    "        for input_h5_file in list(np.sort(glob.glob(f'{directory}/{output_dir_name}/*R{radius}_unprocessed.h5'))):\n",
    "            input_data_name = input_h5_file.split('/')[-1].split('_')[0]\n",
    "            if input_data_name in cce_scri_data_names:\n",
    "                input_data_type = cce_scri_data_names[input_data_name][0]\n",
    "\n",
    "                WM = scri.SpEC.read_from_h5(\n",
    "                    input_h5_file,\n",
    "                    frameType=scri.Inertial,\n",
    "                    dataType=input_data_type,\n",
    "                    r_is_scaled_out=True,\n",
    "                    m_is_scaled_out=False,\n",
    "                )\n",
    "                if os.path.exists(f'{directory}metadata.txt'):\n",
    "                    metadata_filename = f'{directory}metadata.txt'\n",
    "                else:\n",
    "                    metadata_filename = f'{directory}metadata.json'\n",
    "                make_variables_dimensionless(\n",
    "                    WM, metadata_filename=metadata_filename)\n",
    "                WM.t = WM.t - float(radius)\n",
    "                variables[input_data_name] = WM\n",
    "\n",
    "        min_time = variables['Strain'].t[0]\n",
    "        max_time = variables['Strain'].t[-1]\n",
    "        idx = 0\n",
    "        for i, WM_name in enumerate(variables):\n",
    "            WM = variables[WM_name]\n",
    "            if WM.t[0] > min_time and WM.t[-1] < max_time:\n",
    "                min_time = WM.t[0]\n",
    "                max_time = WM.t[-1]\n",
    "                idx = i\n",
    "\n",
    "        t_common = variables[list(variables.keys())[idx]].t\n",
    "        for WM_name in variables:\n",
    "            WM = variables[WM_name]\n",
    "            variables[WM_name] = WM.interpolate(t_common)\n",
    "\n",
    "        for WM_name in variables:\n",
    "            scri.SpEC.file_io.write_to_h5(variables[WM_name],\n",
    "                                          f'{directory}{output_dir_name}/BondiCce_R{radius}.h5')\n",
    "\n",
    "        # remove the unprocessed parts\n",
    "        os.system(f'rm {directory}/{output_dir_name}/*unprocessed.h5')\n",
    "\n",
    "        # compute bianchi violations and save the pickel\n",
    "\n",
    "        abd = scri.SpEC.file_io.create_abd_from_h5(h=f'{directory}{output_dir_name}/rhOverM_BondiCce_R{radius}.h5',\n",
    "                                                   Psi4=f'{directory}{output_dir_name}/rMPsi4_BondiCce_R{radius}.h5',\n",
    "                                                   Psi3=f'{directory}{output_dir_name}/r2Psi3_BondiCce_R{radius}.h5',\n",
    "                                                   Psi2=f'{directory}{output_dir_name}/r3Psi2OverM_BondiCce_R{radius}.h5',\n",
    "                                                   Psi1=f'{directory}{output_dir_name}/r4Psi1OverM2_BondiCce_R{radius}.h5',\n",
    "                                                   Psi0=f'{directory}{output_dir_name}/r5Psi0OverM3_BondiCce_R{radius}.h5',\n",
    "                                                   file_format='SXS')\n",
    "\n",
    "        violations = abd.bondi_violation_norms\n",
    "\n",
    "        # dump the dict as a pickel\n",
    "        violations_dict = {\n",
    "            't': abd.t,\n",
    "            '0': violations[0],\n",
    "            '1': violations[1],\n",
    "            '2': violations[2],\n",
    "            '3': violations[3],\n",
    "            '4': violations[4],\n",
    "            '5': violations[5]\n",
    "        }\n",
    "        with open(f'{directory}bondi_violation_dict.pkl', 'wb') as f:\n",
    "            pickle.dump(violations_dict, f)\n",
    "        plot_and_save_bianchi_violations(violations_dict, bd_folder_path)\n",
    "\n",
    "        total_violations = []\n",
    "        for violation in violations:\n",
    "            total_violations.append(\n",
    "                scipy.integrate.trapezoid(violation, abd.t))\n",
    "            bianchi_violations[str(bd_folder_path.stem)] = total_violations\n",
    "\n",
    "    # Save the bianchi violation dict\n",
    "    with open(f'{directory}bianchi_violations.json', 'w') as f:\n",
    "        json.dump(bianchi_violations, f, indent=2,\n",
    "                  separators=(\",\", \": \"), ensure_ascii=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to deal with cce extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunCCE(CCE_executable: Path, BoundaryDataPath: Path, VolumeFilePostFix: str = None) -> Path:\n",
    "    assert (CCE_executable.exists())\n",
    "    assert (BoundaryDataPath.exists())\n",
    "\n",
    "    if VolumeFilePostFix is None:\n",
    "        VolumeFilePostFix = \"_VolumeData\"\n",
    "\n",
    "    # Input file that will be created\n",
    "    InputSavePath = BoundaryDataPath.parent / \\\n",
    "        (str(BoundaryDataPath.stem)+\".yaml\")\n",
    "\n",
    "    # Create input file\n",
    "    make_config_file(BoundaryDataPath, InputSavePath, VolumeFilePostFix)\n",
    "\n",
    "    command = f\"cd {InputSavePath.parent} && {CCE_executable} +p8 --input-file {InputSavePath}\"\n",
    "    status = subprocess.run(command,\n",
    "                            capture_output=True,\n",
    "                            shell=True,\n",
    "                            text=True)\n",
    "    if status.returncode == 0:\n",
    "        print(f\"Succesfully ran CCE for file {BoundaryDataPath.name}\")\n",
    "    else:\n",
    "        sys.exit(\n",
    "            f\"CCE failed for file {BoundaryDataPath.name} with error: \\n {status.stderr}\"\n",
    "        )\n",
    "    # Return the path of the output\n",
    "    return list(BoundaryDataPath.parent.glob(f\"*{BoundaryDataPath.stem}*{VolumeFilePostFix}*.h5\"))\n",
    "\n",
    "\n",
    "def make_config_file_inverse_cube(BoundaryDataPath: Path,\n",
    "                                  InputSavePath: Path = None,\n",
    "                                  VolumeFilePostFix: str = None) -> Path:\n",
    "\n",
    "    if InputSavePath is None:\n",
    "        InputSavePath = BoundaryDataPath.parent / \\\n",
    "            (str(BoundaryDataPath.stem)+\".yaml\")\n",
    "    assert(InputSavePath.parent.exists())\n",
    "\n",
    "    if VolumeFilePostFix is None:\n",
    "        VolumeFilePostFix = \"_VolumeData\"\n",
    "\n",
    "    config_file =\\\n",
    "        f\"\"\"\n",
    "# Distributed under the MIT License.\n",
    "# See LICENSE.txt for details.\n",
    "\n",
    "# Executable: CharacteristicExtract\n",
    "# Check: parse\n",
    "\n",
    "Evolution:\n",
    "  InitialTimeStep: 0.25\n",
    "  InitialSlabSize: 10.0\n",
    "\n",
    "ResourceInfo:\n",
    "  AvoidGlobalProc0: false\n",
    "  Singletons:\n",
    "    CharacteristicEvolution:\n",
    "      Proc: Auto\n",
    "      Exclusive: False\n",
    "    H5WorldtubeBoundary:\n",
    "      Proc: Auto\n",
    "      Exclusive: False\n",
    "\n",
    "Observers:\n",
    "  VolumeFileName: {str(InputSavePath.stem)+VolumeFilePostFix}\n",
    "  ReductionFileName: \"CharacteristicExtractUnusedReduction\"\n",
    "\n",
    "Cce:\n",
    "  Evolution:\n",
    "    TimeStepper:\n",
    "      AdamsBashforth:\n",
    "        Order: 3\n",
    "    StepChoosers:\n",
    "      - Constant: 0.5\n",
    "      - Increase:\n",
    "          Factor: 2\n",
    "      - ErrorControl(SwshVars):\n",
    "          AbsoluteTolerance: 1e-8\n",
    "          RelativeTolerance: 1e-6\n",
    "          MaxFactor: 2\n",
    "          MinFactor: 0.25\n",
    "          SafetyFactor: 0.9\n",
    "      - ErrorControl(CoordVars):\n",
    "          AbsoluteTolerance: 1e-8\n",
    "          RelativeTolerance: 1e-7\n",
    "          MaxFactor: 2\n",
    "          MinFactor: 0.25\n",
    "          SafetyFactor: 0.9\n",
    "\n",
    "  LMax: 20\n",
    "  NumberOfRadialPoints: 12\n",
    "  ObservationLMax: 8\n",
    "\n",
    "  InitializeJ:\n",
    "    InverseCubic\n",
    "\n",
    "  StartTime: Auto\n",
    "  EndTime: Auto\n",
    "  BoundaryDataFilename: {BoundaryDataPath.name}\n",
    "  H5IsBondiData: False\n",
    "  H5Interpolator:\n",
    "    BarycentricRationalSpanInterpolator:\n",
    "      MinOrder: 10\n",
    "      MaxOrder: 10\n",
    "  ExtractionRadius: Auto\n",
    "  FixSpecNormalization: False\n",
    "\n",
    "  H5LookaheadTimes: 10000\n",
    "\n",
    "  Filtering:\n",
    "    RadialFilterHalfPower: 24\n",
    "    RadialFilterAlpha: 35.0\n",
    "    FilterLMax: 18\n",
    "\n",
    "  ScriInterpOrder: 5\n",
    "  ScriOutputDensity: 5\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    with InputSavePath.open('w') as f:\n",
    "        f.writelines(config_file)\n",
    "\n",
    "    return InputSavePath\n",
    "\n",
    "\n",
    "def make_config_file(BoundaryDataPath: Path,\n",
    "                     InputSavePath: Path = None,\n",
    "                     VolumeFilePostFix: str = None) -> Path:\n",
    "\n",
    "    if InputSavePath is None:\n",
    "        InputSavePath = BoundaryDataPath.parent / \\\n",
    "            (str(BoundaryDataPath.stem)+\".yaml\")\n",
    "    assert(InputSavePath.parent.exists())\n",
    "\n",
    "    if VolumeFilePostFix is None:\n",
    "        VolumeFilePostFix = \"_VolumeData\"\n",
    "\n",
    "    config_file =\\\n",
    "        f\"\"\"\n",
    "# Distributed under the MIT License.\n",
    "# See LICENSE.txt for details.\n",
    "\n",
    "# Executable: CharacteristicExtract\n",
    "# Check: parse\n",
    "\n",
    "Evolution:\n",
    "  InitialTimeStep: 0.25\n",
    "  InitialSlabSize: 10.0\n",
    "\n",
    "ResourceInfo:\n",
    "  AvoidGlobalProc0: false\n",
    "  Singletons:\n",
    "    CharacteristicEvolution:\n",
    "      Proc: Auto\n",
    "      Exclusive: False\n",
    "    H5WorldtubeBoundary:\n",
    "      Proc: Auto\n",
    "      Exclusive: False\n",
    "\n",
    "Observers:\n",
    "  VolumeFileName: vol_{str(InputSavePath.stem)+VolumeFilePostFix}\n",
    "  ReductionFileName: redu_{str(InputSavePath.stem)+VolumeFilePostFix}\n",
    "\n",
    "Cce:\n",
    "  Evolution:\n",
    "    TimeStepper:\n",
    "      AdamsBashforth:\n",
    "        Order: 3\n",
    "    StepChoosers:\n",
    "      - Constant: 0.5\n",
    "      - Increase:\n",
    "          Factor: 2\n",
    "      - ErrorControl(SwshVars):\n",
    "          AbsoluteTolerance: 1e-8\n",
    "          RelativeTolerance: 1e-6\n",
    "          MaxFactor: 2\n",
    "          MinFactor: 0.25\n",
    "          SafetyFactor: 0.9\n",
    "      - ErrorControl(CoordVars):\n",
    "          AbsoluteTolerance: 1e-8\n",
    "          RelativeTolerance: 1e-7\n",
    "          MaxFactor: 2\n",
    "          MinFactor: 0.25\n",
    "          SafetyFactor: 0.9\n",
    "\n",
    "  LMax: 20\n",
    "  NumberOfRadialPoints: 12\n",
    "  ObservationLMax: 8\n",
    "\n",
    "  InitializeJ:\n",
    "    ConformalFactor:\n",
    "      AngularCoordTolerance: 1e-13\n",
    "      MaxIterations: 1000\n",
    "      RequireConvergence: False\n",
    "      OptimizeL0Mode: True\n",
    "      UseBetaIntegralEstimate: False\n",
    "      ConformalFactorIterationHeuristic: SpinWeight1CoordPerturbation\n",
    "      UseInputModes: False\n",
    "      InputModes: []\n",
    "\n",
    "  StartTime: Auto\n",
    "  EndTime: Auto\n",
    "  BoundaryDataFilename: {BoundaryDataPath.name}\n",
    "  H5IsBondiData: False\n",
    "  H5Interpolator:\n",
    "    BarycentricRationalSpanInterpolator:\n",
    "      MinOrder: 10\n",
    "      MaxOrder: 10\n",
    "  ExtractionRadius: Auto\n",
    "  FixSpecNormalization: False\n",
    "\n",
    "  H5LookaheadTimes: 10000\n",
    "\n",
    "  Filtering:\n",
    "    RadialFilterHalfPower: 24\n",
    "    RadialFilterAlpha: 35.0\n",
    "    FilterLMax: 18\n",
    "\n",
    "  ScriInterpOrder: 5\n",
    "  ScriOutputDensity: 5\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    with InputSavePath.open('w') as f:\n",
    "        f.writelines(config_file)\n",
    "\n",
    "    return InputSavePath\n",
    "\n",
    "\n",
    "def create_metadata_with_masses(path_dict: dict):\n",
    "    file_path = path_dict['base_path'].parent/\"ID/SpEC.out\"\n",
    "\n",
    "    # Read the masses from the ID SpEC.out file\n",
    "    with file_path.open('r') as f:\n",
    "        file_contents = f.read()\n",
    "        MA = float((re.findall(\"MA=0.\\d*\", file_contents))[-1].split('=')[-1])\n",
    "        MB = float((re.findall(\"MB=0.\\d*\", file_contents))[-1].split('=')[-1])\n",
    "\n",
    "    metadata_file_contents = f\"\"\"{{ \"reference_mass1\": {MA},  \"reference_mass2\": {MB}}}\"\"\"\n",
    "\n",
    "    for bd_path in path_dict['boundary_data_paths']:\n",
    "        metadata_file = bd_path.parent/\"metadata.json\"\n",
    "\n",
    "        with metadata_file.open('w') as f:\n",
    "            f.write(metadata_file_contents)\n",
    "\n",
    "\n",
    "def make_submit_file(path_dict: dict):\n",
    "    path_dict['submit_script_paths'] = []\n",
    "    base_path = path_dict['base_path']\n",
    "    for cce_folder_name in path_dict['cce_paths_keys']:\n",
    "        run_name = f\"{cce_folder_name}_{base_path.stem}\"\n",
    "        run_path = base_path/\"cce\"/cce_folder_name\n",
    "        input_file_name = cce_folder_name+\".yaml\"\n",
    "\n",
    "        submit_script =\\\n",
    "            f\"\"\"#!/bin/bash -\n",
    "#SBATCH -J {run_name}              # Job Name\n",
    "#SBATCH -o SpEC.stdout                # Output file name\n",
    "#SBATCH -e SpEC.stderr                # Error file name\n",
    "#SBATCH -n 4                          # Number of cores\n",
    "#SBATCH --ntasks-per-node 4        # number of MPI ranks per node\n",
    "#SBATCH -t 24:0:00   # Run time\n",
    "#SBATCH -A sxs                # Account name\n",
    "#SBATCH --no-requeue\n",
    "\n",
    "# Go to the correct folder with the boundary data\n",
    "cd {run_path}\n",
    "\n",
    "export SPECTRE_HOME=/panfs/ds09/sxs/himanshu/spectre\n",
    "export SPECTRE_DEPS=/panfs/ds09/sxs/himanshu/SPECTRE_DEPS\n",
    "\n",
    "# Setup spectre environment\n",
    ". $SPECTRE_HOME/support/Environments/wheeler_gcc.sh && spectre_setup_modules $SPECTRE_DEPS && echo \\\"Modules build\\\" && spectre_load_modules && echo \\\"Modules loaded\\\"\n",
    "\n",
    "# run CCE\n",
    "/panfs/ds09/sxs/himanshu/spectre/build/bin/CharacteristicExtract +p4 --input-file ./{input_file_name}\n",
    "\"\"\"\n",
    "        submit_script_path = base_path/\"cce\"/cce_folder_name/\"submit.sh\"\n",
    "        path_dict['submit_script_paths'].append(submit_script_path)\n",
    "        with submit_script_path.open('w') as f:\n",
    "            f.writelines(submit_script)\n",
    "\n",
    "\n",
    "def submit_all_jobs(path_dict: dict):\n",
    "    for submit_script_path in path_dict['submit_script_paths']:\n",
    "        command = f\"cd {submit_script_path.parent} && qsub {submit_script_path}\"\n",
    "        status = subprocess.run(\n",
    "            command, capture_output=True, shell=True, text=True)\n",
    "        if status.returncode == 0:\n",
    "            print(\n",
    "                f\"Succesfully submitted {submit_script_path}\\n{status.stdout}\")\n",
    "        else:\n",
    "            sys.exit(\n",
    "                f\"Job submission failed for {submit_script_path} with error: \\n{status.stdout} \\n{status.stderr}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to deal with combining the cce data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Lev_list': ['1','3']\n",
    "def add_levs(path_dict):\n",
    "    base_path = path_dict[\"base_path\"]\n",
    "    path_dict[\"Lev_list\"] = []\n",
    "    for lev in range(10):\n",
    "        # Deal with runs started from a lev\n",
    "        for LevSec in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\":\n",
    "            if (base_path/f\"Lev{lev}_A{LevSec}\").exists():\n",
    "                path_dict[\"Lev_list\"].append(f\"{lev}\")\n",
    "                break\n",
    "\n",
    "# 'cce_radius': ['0112', '0540', '0397', '0255']\n",
    "\n",
    "\n",
    "def add_cce_radius(path_dict):\n",
    "    some_lev = path_dict[\"Lev_list\"][0]\n",
    "    # Deal with runs started from a lev\n",
    "    for LevSec in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\":\n",
    "        cce_list = list(path_dict[\"base_path\"].glob(\n",
    "            f\"Lev{some_lev}_A{LevSec}/Run/GW2/CceR????.h5\"))\n",
    "        if len(cce_list) > 0:\n",
    "            path_dict[\"cce_radius\"] = [file.name[4:-3] for file in cce_list]\n",
    "            break\n",
    "\n",
    "\n",
    "# 'cce_paths_keys': ['Lev_1_radius_0112', 'Lev_1_radius_0255']\n",
    "# 'Lev1_R0112': [path_list],\n",
    "# 'Lev1_R0255': [path_list],\n",
    "def add_cce_data_paths(path_dict):\n",
    "    path_dict[\"cce_paths_keys\"] = []\n",
    "    for lev in path_dict[\"Lev_list\"]:\n",
    "        for radius in path_dict[\"cce_radius\"]:\n",
    "            key_name = f\"Lev{lev}_R{radius}\"\n",
    "            path_dict[\"cce_paths_keys\"].append(key_name)\n",
    "            path_dict[key_name] = list(path_dict[\"base_path\"].glob(\n",
    "                f\"Lev{lev}_??/Run/GW2/CceR{radius}.h5\"))\n",
    "\n",
    "\n",
    "# create directories to save cce waveforms\n",
    "def create_folders_to_save_cce_data(path_dict):\n",
    "    for cce_lev_radius in path_dict[\"cce_paths_keys\"]:\n",
    "        folder_to_create = path_dict[\"base_path\"]/f\"cce/{cce_lev_radius}\"\n",
    "        folder_to_create.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def run_JoinH5(save_folder, h5_file_list, output_file_name):\n",
    "    file_list_str = \"\"\n",
    "    for file_path in h5_file_list:\n",
    "        file_list_str += f\" {file_path}\"\n",
    "\n",
    "    command = f\"cd {save_folder} && {spec_home}/Support/bin/JoinH5 -o {output_file_name} {file_list_str}\"\n",
    "    status = subprocess.run(\n",
    "        command, capture_output=True, shell=True, text=True)\n",
    "    if status.returncode == 0:\n",
    "        print(\n",
    "            f\"Succesfully saved joined h5 file {output_file_name} in {save_folder}\")\n",
    "    else:\n",
    "        sys.exit(\n",
    "            f\"JoinH5 failed in {save_folder} with error: \\n {status.stderr}\")\n",
    "\n",
    "# Combines h5 files for different levs and radius\n",
    "\n",
    "\n",
    "def save_joined_cce_h5_files(path_dict, cce_paths_keys_list=None):\n",
    "    if cce_paths_keys_list is None:\n",
    "        cce_paths_keys_list = path_dict[\"cce_paths_keys\"]\n",
    "\n",
    "    for cce_lev_radius in cce_paths_keys_list:\n",
    "        save_folder = str(path_dict[\"base_path\"]/f\"cce/{cce_lev_radius}\")\n",
    "        h5_file_list = path_dict[cce_lev_radius]\n",
    "        output_file_name = cce_lev_radius+\".h5\"\n",
    "\n",
    "        # check that the outputfile is not already present\n",
    "        output_file_path = path_dict[\"base_path\"] / \\\n",
    "            f\"cce/{cce_lev_radius}/{output_file_name}\"\n",
    "        if output_file_path.exists():\n",
    "            print(f\"File {output_file_path} already exisits. Doing nothing!!!\")\n",
    "        else:\n",
    "            run_JoinH5(save_folder, h5_file_list, output_file_name)\n",
    "\n",
    "# Makes input files for CCE for each radius of CCE\n",
    "\n",
    "\n",
    "def make_config_files_in_all_folders(path_dict: dict):\n",
    "    path_dict['config_file_paths'] = []\n",
    "    for bd_path in path_dict['boundary_data_paths']:\n",
    "        path_dict['config_file_paths'].append(make_config_file(bd_path))\n",
    "\n",
    "# Saves the path of the combined boundary data files into path_dict\n",
    "\n",
    "\n",
    "def save_boundary_data_paths(path_dict):\n",
    "    path_dict['boundary_data_paths'] = list(\n",
    "        path_dict['base_path'].glob(\"cce/*/*.h5\"))\n",
    "\n",
    "\n",
    "def pickle_path_dict(path_dict):\n",
    "    with open(path_dict['base_path']/\"cce/path_dict.pkl\", 'wb') as f:\n",
    "        pickle.dump(path_dict, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to do it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_CCE(run_path_list: list, CCE_executable: Path = None, submit_jobs=True):\n",
    "    if CCE_executable is None:\n",
    "        CCE_executable = Path(\n",
    "            \"/panfs/ds09/sxs/himanshu/spectre/build/bin/CharacteristicExtract\")\n",
    "\n",
    "    for base_path in run_path_list:\n",
    "        path_dict = {\"base_path\": Path(base_path)}\n",
    "        path_dict['CCE_Executable'] = CCE_executable\n",
    "\n",
    "        add_levs(path_dict)\n",
    "        add_cce_radius(path_dict)\n",
    "        add_cce_data_paths(path_dict)\n",
    "        create_folders_to_save_cce_data(path_dict)\n",
    "        save_joined_cce_h5_files(path_dict)\n",
    "        save_boundary_data_paths(path_dict)\n",
    "        pickle_path_dict(path_dict)\n",
    "\n",
    "        # make cce input files and submit the job\n",
    "        make_config_files_in_all_folders(path_dict)\n",
    "        create_metadata_with_masses(path_dict)\n",
    "        make_submit_file(path_dict)\n",
    "        if submit_jobs:\n",
    "            submit_all_jobs(path_dict)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_paths = [\n",
    "    Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/AccTest_q1ns_Lev3/Ev/\"),\n",
    "    Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/AccTest_q1ns_Lev7/Ev/\"),\n",
    "    Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/AccTest_q1ns_Lev9/Ev/\")\n",
    "    ]\n",
    "\n",
    "CCE_executable = Path(\n",
    "    \"/panfs/ds09/sxs/himanshu/spectre/build/bin/CharacteristicExtract\")\n",
    "\n",
    "do_CCE(runs_paths, CCE_executable, submit_jobs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "runs_paths = [\n",
    "    \"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/70_ngd_master_harmonic_mr1_50_400/\",\n",
    "    \"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/70_ngd_master_harmonic_mr1_200_400/\",\n",
    "    \"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/70_ngd_master_mr1_50_400/\",\n",
    "    \"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/70_ngd_master_mr1_200_400/\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/AccTest_q1ns_Lev5/Ev_Lev6/\")\n",
    "path_dict = {\"base_path\": base_path}\n",
    "add_levs(path_dict)\n",
    "add_cce_radius(path_dict)\n",
    "add_cce_data_paths(path_dict)\n",
    "create_folders_to_save_cce_data(path_dict)\n",
    "save_joined_cce_h5_files(path_dict)\n",
    "save_boundary_data_paths(path_dict)\n",
    "pickle_path_dict(path_dict)\n",
    "path_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dict['CCE_Executable'] = Path(\n",
    "    \"/panfs/ds09/sxs/himanshu/spectre/build/bin/CharacteristicExtract\")\n",
    "make_config_files_in_all_folders(path_dict)\n",
    "make_submit_file(path_dict)\n",
    "# submit_all_jobs(path_dict)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_paths = [\n",
    "    Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/67_master_mr1/\"),\n",
    "    Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/66_master_harmonic_mr1/\"),\n",
    "    Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/67_master_mr3/\"),\n",
    "    Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/66_master_harmonic_mr3/\"),\n",
    "    Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/70_ngd_master_harmonic_mr1_50_400/\"),\n",
    "    Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/70_ngd_master_harmonic_mr1_200_400/\"),\n",
    "    Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/70_ngd_master_mr1_50_400/\"),\n",
    "    Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/70_ngd_master_mr1_200_400/\"),\n",
    "    Path('/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/71_ngd_master_mr1_50_400_no_roll_on'),\n",
    "    Path('/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/71_ngd_master_mr1_200_400_no_roll_on'),\n",
    "    Path('/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/72_ngd_master_mr1_50_400_no_roll_on_pow2'),\n",
    "    Path('/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/72_ngd_master_mr1_200_400_no_roll_on_pow2')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def submit_bianchi_violation_job(cce_folder_path: Path, binachi_violation_script: Path, conda_env='sxs',submit_job=True):\n",
    "    \n",
    "    if not binachi_violation_script.exists():\n",
    "        raise Exception(f\"{binachi_violation_script} does not exists.\")\n",
    "\n",
    "    if not cce_folder_path.exists():\n",
    "        raise Exception(f\"{cce_folder_path} does not exists.\")\n",
    "\n",
    "    run_name = f\"bianchi_{cce_folder_path.parent}\"\n",
    "    run_path = cce_folder_path\n",
    "\n",
    "    shutil.copy(binachi_violation_script,run_path)\n",
    "        \n",
    "\n",
    "    submit_script=\\\n",
    "f\"\"\"#!/bin/bash -\n",
    "#SBATCH -J {run_name}              # Job Name\n",
    "#SBATCH -o SpEC.stdout                # Output file name\n",
    "#SBATCH -e SpEC.stderr                # Error file name\n",
    "#SBATCH -n 1                          # Number of cores\n",
    "#SBATCH --ntasks-per-node 1        # number of MPI ranks per node\n",
    "#SBATCH -t 24:0:00   # Run time\n",
    "#SBATCH -A sxs                # Account name\n",
    "#SBATCH --no-requeue\n",
    "\n",
    "# Go to the correct cce folder \n",
    "cd {run_path}\n",
    "\n",
    "export SPECTRE_HOME=/panfs/ds09/sxs/himanshu/spectre\n",
    "export SPECTRE_DEPS=/panfs/ds09/sxs/himanshu/SPECTRE_DEPS\n",
    "\n",
    "# Setup spectre environment\n",
    ". $SPECTRE_HOME/support/Environments/wheeler_gcc.sh && spectre_setup_modules $SPECTRE_DEPS && echo \\\"Modules build\\\" && spectre_load_modules && echo \\\"Modules loaded\\\"\n",
    "\n",
    "# activate the environment and run python script\n",
    "conda activate {conda_env}\n",
    "python {binachi_violation_script.name}\n",
    "\n",
    "\"\"\"\n",
    "    submit_script_path = cce_folder_path/\"bianchi_submit.sh\"\n",
    "    with submit_script_path.open('w') as f:\n",
    "        f.writelines(submit_script)\n",
    "    if submit_job:\n",
    "        command = f\"cd {submit_script_path.parent} && qsub {submit_script_path}\"\n",
    "        status = subprocess.run(command, capture_output=True, shell=True, text=True)\n",
    "        if status.returncode == 0:\n",
    "          print(f\"Succesfully submitted {submit_script_path}\\n{status.stdout}\")\n",
    "        else:\n",
    "          sys.exit(\n",
    "              f\"Job submission failed for {submit_script_path} with error: \\n{status.stdout} \\n{status.stderr}\")\n",
    "\n",
    "def are_runs_going_on(re_text=r\"Lev\\d_R\\d\\d\\d\\d\",user='himanshu'):\n",
    "    command = f\"qstat -u {user}\"\n",
    "    status = subprocess.run(command, capture_output=True, shell=True, text=True)\n",
    "\n",
    "    qstat_output = status.stdout.split(\"\\n\")\n",
    "\n",
    "    JOBS_STILL_PENDING = False\n",
    "\n",
    "    for line in qstat_output:\n",
    "        matches = list(re.finditer(re_text, line, re.MULTILINE))\n",
    "        if len(matches)>0:\n",
    "            JOBS_STILL_PENDING = True\n",
    "\n",
    "    return JOBS_STILL_PENDING\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_paths = [\n",
    "    Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/67_master_mr1/\"),\n",
    "    Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/66_master_harmonic_mr1/\"),\n",
    "    Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/67_master_mr3/\"),\n",
    "    Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/66_master_harmonic_mr3/\"),\n",
    "    Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/70_ngd_master_harmonic_mr1_50_400/\"),\n",
    "    Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/70_ngd_master_harmonic_mr1_200_400/\"),\n",
    "    Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/70_ngd_master_mr1_50_400/\"),\n",
    "    Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/70_ngd_master_mr1_200_400/\"),\n",
    "    Path('/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/71_ngd_master_mr1_50_400_no_roll_on'),\n",
    "    Path('/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/71_ngd_master_mr1_200_400_no_roll_on'),\n",
    "    Path('/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/72_ngd_master_mr1_50_400_no_roll_on_pow2'),\n",
    "    Path('/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/72_ngd_master_mr1_200_400_no_roll_on_pow2')\n",
    "]\n",
    "\n",
    "while True:\n",
    "    if are_runs_going_on(re_text=r\"Lev\\d_R\\d\\d\\d\\d\",user='himanshu'):\n",
    "        time.sleep(10)\n",
    "    else:\n",
    "        for path in runs_paths:\n",
    "            cce_folder_path = path/\"cce\"\n",
    "            binachi_violation_script = Path(\"/panfs/ds09/sxs/himanshu/scripts/run_cce_on_a_spec_run/bianchi_violation.py\")\n",
    "            submit_bianchi_violation_job(cce_folder_path=cce_folder_path,binachi_violation_script=binachi_violation_script)\n",
    "        break\n",
    "            \n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binachi_violation_script = Path(\n",
    "    \"/panfs/ds09/sxs/himanshu/scripts/run_cce_on_a_spec_run/bianchi_violation.py\")\n",
    "submit_bianchi_violation_job(\n",
    "    submit_job=False, cce_folder_path=runs_paths[-1]/\"cce\", binachi_violation_script=binachi_violation_script)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot bianchi violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(pkl_path: Path):\n",
    "    with pkl_path.open('rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cce_path = Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/78_ngd_master_mr1/cce\")\n",
    "# cce_path = Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/76_ngd_master_mr1_200_3000/cce\")\n",
    "# cce_path = Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/76_ngd_master_mr1_50_3000/cce\")\n",
    "bondi_violation_list = list(cce_path.glob(\"**/bondi*.pkl\"))\n",
    "radius_dict = {}\n",
    "for path in bondi_violation_list:\n",
    "   radius_dict[path.parts[-2][-4:]] = path\n",
    "radius_dict = dict(sorted(radius_dict.items()))\n",
    "radius_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for radius in radius_dict.keys():\n",
    "    data[radius] = load_pickle(radius_dict[radius])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = '2'\n",
    "outer_boundary = 1189\n",
    "for radius in data.keys():\n",
    "    p = plt.semilogy(data[radius]['t'],data[radius][norm],label = radius)\n",
    "    color_used = p[0].get_color()\n",
    "    plt.axvline((outer_boundary - int(radius))*2,color = color_used)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel(f'violation norm {norm}')\n",
    "plt.title(norm)\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare psi values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cce_path1 = Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/78_ngd_master_mr1/cce\")\n",
    "cce_path2 = Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/76_ngd_master_mr1_200_3000/cce\")\n",
    "# cce_path = Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/76_ngd_master_mr1_50_3000/cce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Levs1 = sorted([i.stem for i in list(cce_path1.glob(\"Lev*\"))])\n",
    "Levs2 = sorted([i.stem for i in list(cce_path2.glob(\"Lev*\"))])\n",
    "assert Levs1 == Levs2\n",
    "current_lev = Levs1[0]\n",
    "current_lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abd1 = scri.SpEC.file_io.create_abd_from_h5(h=str(cce_path1/current_lev/f'extracted_data/rhOverM_BondiCce_R{current_lev[-4:]}.h5'),\n",
    "                                          Psi4=str(cce_path1/current_lev/f'extracted_data/rMPsi4_BondiCce_R{current_lev[-4:]}.h5'),\n",
    "                                          Psi3=str(cce_path1/current_lev/f'extracted_data/r2Psi3_BondiCce_R{current_lev[-4:]}.h5'),\n",
    "                                          Psi2=str(cce_path1/current_lev/f'extracted_data/r3Psi2OverM_BondiCce_R{current_lev[-4:]}.h5'),\n",
    "                                          Psi1=str(cce_path1/current_lev/f'extracted_data/r4Psi1OverM2_BondiCce_R{current_lev[-4:]}.h5'),\n",
    "                                          Psi0=str(cce_path1/current_lev/f'extracted_data/r5Psi0OverM3_BondiCce_R{current_lev[-4:]}.h5'),\n",
    "                                          file_format='SXS')\n",
    "abd2 = scri.SpEC.file_io.create_abd_from_h5(h=str(cce_path2/current_lev/f'extracted_data/rhOverM_BondiCce_R{current_lev[-4:]}.h5'),\n",
    "                                          Psi4=str(cce_path2/current_lev/f'extracted_data/rMPsi4_BondiCce_R{current_lev[-4:]}.h5'),\n",
    "                                          Psi3=str(cce_path2/current_lev/f'extracted_data/r2Psi3_BondiCce_R{current_lev[-4:]}.h5'),\n",
    "                                          Psi2=str(cce_path2/current_lev/f'extracted_data/r3Psi2OverM_BondiCce_R{current_lev[-4:]}.h5'),\n",
    "                                          Psi1=str(cce_path2/current_lev/f'extracted_data/r4Psi1OverM2_BondiCce_R{current_lev[-4:]}.h5'),\n",
    "                                          Psi0=str(cce_path2/current_lev/f'extracted_data/r5Psi0OverM3_BondiCce_R{current_lev[-4:]}.h5'),\n",
    "                                          file_format='SXS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l,m = 2,2\n",
    "plt.plot(abd1.t[:],abd1.psi4[:, lm(l,m,0)])\n",
    "plt.plot(abd2.t[:],abd2.psi4[:, lm(l,m,0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abd2.psi4[:, lm(l,m,0)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtract using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mode(data_path:Path,l:int,m:int):\n",
    "    file_name = f\"Y_l{l}_m{m}.dat\"\n",
    "    with h5py.File(data_path,'r') as hf:\n",
    "        data = hf[file_name][:]\n",
    "    return data\n",
    "\n",
    "def load_into_pandas(data_path:Path,l_max:int = 8):\n",
    "    pd_data = pd.DataFrame()\n",
    "    i = 0\n",
    "    with h5py.File(data_path,'r') as hf:\n",
    "        file_name = \"Y_l0_m0.dat\"\n",
    "        data = hf[file_name][:]\n",
    "        pd_data.insert(i,'t',data[:,0])\n",
    "        i = i + 1\n",
    "        for l in range(l_max+1):    \n",
    "            for m in range(-l,l+1):\n",
    "                file_name = f\"Y_l{l}_m{m}.dat\"\n",
    "                data = hf[file_name][:]\n",
    "                pd_data.insert(i,f\"{l},{m}\",data[:,1]+data[:,2]*1j)\n",
    "                i = i + 1\n",
    "\n",
    "    return pd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path1 = Path('/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/78_ngd_master_mr1/cce/Lev3_R0300/extracted_data/rMPsi4_BondiCce_R0300.h5')\n",
    "data_path2 = Path('/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/76_ngd_master_mr1_50_3000/cce/Lev3_R0300/extracted_data/rMPsi4_BondiCce_R0300.h5')\n",
    "data_path3 = Path('/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/76_ngd_master_mr1_200_3000/cce/Lev3_R0300/extracted_data/rMPsi4_BondiCce_R0300.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data1 = load_into_pandas(data_path1)\n",
    "pd_data2 = load_into_pandas(data_path2)\n",
    "pd_data3 = load_into_pandas(data_path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 20000\n",
    "mode = '2,0'\n",
    "plt.plot(pd_data1['t'][start:],pd_data1[mode][start:],label=\"master\")\n",
    "plt.plot(pd_data2['t'][start:],pd_data2[mode][start:],label=\"50\")\n",
    "plt.plot(pd_data3['t'][start:],pd_data3[mode][start:],label=\"200\")\n",
    "plt.axvline(3000,color='r')\n",
    "plt.axvline(2*1189,color='r')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel(mode)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l,m,rc = 2,2,1\n",
    "data1 = load_mode(data_path1,l,m)\n",
    "data2 = load_mode(data_path2,l,m)\n",
    "data3 = load_mode(data_path3,l,m)\n",
    "print(data1.shape,data2.shape,data3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = interpolate.interp1d(data3[:,0],data3[:,rc],kind='cubic')\n",
    "start = 12000\n",
    "factor = 50\n",
    "plt.plot(data2[start:,0],(data2[start:,rc]-f1(data2[start:,0]))*factor,label=f\"diff*{factor}\")\n",
    "plt.plot(data2[start:,0],data2[start:,rc],label=\"50-200\")\n",
    "plt.xlabel('t')\n",
    "plt.ylabel(f'{l},{m},{rc}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = interpolate.interp1d(data1[:,0],data1[:,rc],kind='cubic')\n",
    "plt.plot(data3[:,0],data3[:,rc]-f1(data3[:,0]),label=\"master-200\")\n",
    "plt.xlabel('t')\n",
    "plt.ylabel(f'{l},{m},{rc}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = interpolate.interp1d(data3[:,0],data3[:,rc],kind='cubic')\n",
    "plt.plot(data2[:,0],data2[:,rc]-f1(data2[:,0]),label=\"200-50\")\n",
    "plt.xlabel('t')\n",
    "plt.ylabel(f'{l},{m},{rc}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = interpolate.interp1d(data1[:,0],data1[:,rc],kind='cubic')\n",
    "plt.plot(data1[:,0],data1[:,rc]-f1(data1[:,0]),label=\"master-master\")\n",
    "plt.xlabel('t')\n",
    "plt.ylabel(f'{l},{m},{rc}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = 1\n",
    "plt.plot(data1[:,0],data1[:,rc],label=\"data1\")\n",
    "plt.plot(data2[:,0],data2[:,rc], '--',label=\"data2\")\n",
    "plt.xlabel('t')\n",
    "plt.ylabel(f'{l},{m},{rc}')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Psi4 in catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/data/rMPsi4_Asymptotic_GeometricUnits_CoM.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l,m,rc = 2,0,1\n",
    "file_name = f\"/Extrapolated_N2.dir/Y_l{l}_m{m}.dat\"\n",
    "with h5py.File(data_path,'r') as hf:\n",
    "    data = hf[file_name][:]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 2000\n",
    "end = 12000\n",
    "factor = 50\n",
    "plt.plot(data[start:end,0],data[start:end,rc],label=\"50-200\")\n",
    "plt.xlabel('t')\n",
    "plt.ylabel(f'{l},{m},{rc}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waveform = sxs.load(\"SXS:BBH:0123/Lev/rhOverM\", extrapolation_order=2)\n",
    "# waveform = sxs.load(\"SXS:BBH:1106/Lev/rhOverM\", extrapolation_order=2)\n",
    "waveform = sxs.load(\"SXS:BBH:0623/Lev/rMPsi4_Asymptotic_GeometricUnits_CoM\", extrapolation_order=2)\n",
    "# plt.plot(waveform.t, waveform.data.real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l,m = 2,0\n",
    "data = waveform[f\"Y_l{l}_m{m}.dat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_t = 200\n",
    "end_t = 6000\n",
    "rc = 1\n",
    "start_indx = len(data[data[:,0] < start_t,0])\n",
    "end_indx = len(data[data[:,0] < end_t,0])\n",
    "plt.plot(data[start_indx:end_indx,0],data[start_indx:end_indx,rc],label=\"50-200\")\n",
    "plt.xlabel('t')\n",
    "plt.ylabel(f'{l},{m},{rc}')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sxs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "009adc1c8ee1f76b2251d0bb13ed6e10d4fef5bd0a6f7d195d9f2892e5880fe6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
