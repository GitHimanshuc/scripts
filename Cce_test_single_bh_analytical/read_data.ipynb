{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scri\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import curve_fit\n",
    "from spherical_functions import LM_index as lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCE ABD code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average_valid(array,avg_len):\n",
    "    return np.convolve(array,np.ones(avg_len),'valid')/avg_len\n",
    "\n",
    "def load_and_pickle(data_path:Path, reload_data:bool=False, data_type:str='abd', options:dict={}):\n",
    "  if not data_path.exists():\n",
    "    raise Exception(f\"{data_path} does not exist!\")\n",
    "\n",
    "  saved_data_path = data_path.parent/\"saved.pkl\"\n",
    "  \n",
    "  if saved_data_path.exists() and reload_data == False:\n",
    "    with open(saved_data_path, 'rb') as f:\n",
    "      saved_data = pickle.load(f)\n",
    "      print(f\"Saved data loaded: {saved_data_path}\")\n",
    "  else:\n",
    "    saved_data = {}\n",
    "    if data_type == 'abd':\n",
    "      saved_data['abd']= scri.create_abd_from_h5(\n",
    "          file_name=str(data_path),\n",
    "          file_format=\"spectrecce_v1\",\n",
    "          **options\n",
    "        )\n",
    "      with open(saved_data_path, 'wb') as f:\n",
    "        pickle.dump(saved_data,f)\n",
    "      print(f\"Data loaded and saved at : {saved_data_path}\")\n",
    "\n",
    "  return saved_data\n",
    "\n",
    "def load_bondi_constraints(data_path:Path):\n",
    "  if not data_path.exists():\n",
    "    raise Exception(f\"{data_path} does not exist!\")\n",
    "  saved_data_path = data_path.parent/\"saved.pkl\"\n",
    "  if not saved_data_path.exists():\n",
    "    raise Exception(f\"{saved_data_path} does not exist\")\n",
    "  else:\n",
    "    with open(saved_data_path, 'rb') as f:\n",
    "      saved_data = pickle.load(f)\n",
    "      if 'bondi_violation_norms' in saved_data:\n",
    "        print(f\"bondi_violation_norms loaded for {data_path}\")\n",
    "      else:\n",
    "        print(f\"Computing bondi_violation_norms for: {data_path}\")\n",
    "        saved_data['bondi_violation_norms'] = saved_data['abd'].bondi_violation_norms\n",
    "        with open(saved_data_path, 'wb') as f:\n",
    "          pickle.dump(saved_data,f)\n",
    "\n",
    "        print(f\"Saved bondi_violation_norms for: {data_path}\")\n",
    "    return saved_data\n",
    "\n",
    "def add_bondi_constraints(abd_data:dict):\n",
    "  for key in abd_data:\n",
    "    abd_data[key]['bondi_violation_norms'] = abd_data[key][\"abd\"].bondi_violation_norms\n",
    "    print(f\"bondi_violation_norms computed for {key}\")\n",
    "\n",
    "def create_diff_dict_cce(WT_data_dict:dict, l:int, m:int, base_key:str, t_interpolate:np.ndarray):\n",
    "  h = WT_data_dict[base_key]['abd'].h.interpolate(t_interpolate)\n",
    "  diff_dict = {\"t\": h.t}\n",
    "  y_base = h.data[:,  lm(l,m,h.ell_min)]\n",
    "  y_norm = np.linalg.norm(y_base)\n",
    "  for key in WT_data_dict:\n",
    "    if key == base_key:\n",
    "      continue\n",
    "    h = WT_data_dict[key]['abd'].h.interpolate(t_interpolate)\n",
    "    y_inter = h.data[:,  lm(l,m,h.ell_min)]\n",
    "    diff_dict[key+\"_diff\"] = y_inter-y_base\n",
    "    diff_dict[key+\"_absdiff\"] = np.abs(y_inter-y_base)\n",
    "    diff_dict[key+\"_rel_diff\"] = (y_inter-y_base)/y_norm\n",
    "    diff_dict[key+\"_rel_absdiff\"] = np.abs(y_inter-y_base)/y_norm\n",
    "  return diff_dict\n",
    "\n",
    "def extract_radii(h5_file_path:Path):\n",
    "  radii = set()\n",
    "  with h5py.File(h5_file_path,'r') as f:\n",
    "    names = []\n",
    "    f.visit(names.append)\n",
    "  for name in names:\n",
    "    if \"Version\" in name:\n",
    "      continue\n",
    "    radii.add(name[1:5])\n",
    "  radii = list(radii)\n",
    "  radii.sort()\n",
    "  return radii\n",
    "\n",
    "\n",
    "def generate_columns(num_cols:int,beta_type=False):\n",
    "  if beta_type:\n",
    "    num_cols = num_cols*2\n",
    "  L_max = int(np.sqrt((num_cols-1)/2))-1\n",
    "  # print(L_max,np.sqrt((num_cols-1)/2)-1)\n",
    "  col_names = ['t(M)']\n",
    "  for l in range(0,L_max+1):\n",
    "    for m in range(-l,l+1):\n",
    "      if beta_type:\n",
    "        if m==0:\n",
    "          col_names.append(f\"Re({l},{m})\")\n",
    "        elif m < 0:\n",
    "          continue\n",
    "        else:\n",
    "          col_names.append(f\"Re({l},{m})\")\n",
    "          col_names.append(f\"Im({l},{m})\")\n",
    "      else:\n",
    "        col_names.append(f\"Re({l},{m})\")\n",
    "        col_names.append(f\"Im({l},{m})\")\n",
    "  return col_names\n",
    "\n",
    "\n",
    "def WT_to_pandas(horizon_path:Path):\n",
    "    assert(horizon_path.exists())\n",
    "    df_dict = {}\n",
    "    beta_type_list = ['Beta.dat', 'DuR.dat', 'R.dat', 'W.dat']\n",
    "    with h5py.File(horizon_path,'r') as hf:\n",
    "        # Not all horizon files may have AhC\n",
    "        for key in hf.keys():\n",
    "            if key == \"VersionHist.ver\":\n",
    "              continue \n",
    "            if key in beta_type_list:\n",
    "              df_dict[key] = pd.DataFrame(hf[key], columns=generate_columns(hf[key].shape[1],beta_type=True))\n",
    "            else:\n",
    "              df_dict[key] = pd.DataFrame(hf[key], columns=generate_columns(hf[key].shape[1]))\n",
    "\n",
    "\n",
    "    return df_dict\n",
    "\n",
    "\n",
    "def create_diff_dict(WT_data_dict:dict, mode:str, variable:str, base_key:str):\n",
    "  diff_dict = {\"t(M)\":WT_data_dict[base_key][variable]['t(M)']}\n",
    "  y_base = WT_data_dict[base_key][variable][mode]\n",
    "  y_norm = np.linalg.norm(y_base)\n",
    "  for key in WT_data_dict:\n",
    "    if key == base_key:\n",
    "      continue\n",
    "    y = WT_data_dict[key][variable][mode]\n",
    "    t = WT_data_dict[key][variable]['t(M)']\n",
    "    y_interpolator = interp1d(t, y, kind='cubic',fill_value='extrapolate')\n",
    "    y_inter = y_interpolator(diff_dict['t(M)'])\n",
    "    diff_dict[key+\"_diff\"] = y_inter-y_base\n",
    "    diff_dict[key+\"_absdiff\"] = np.abs(y_inter-y_base)\n",
    "    diff_dict[key+\"_rel_diff\"] = (y_inter-y_base)/y_norm\n",
    "    diff_dict[key+\"_rel_absdiff\"] = np.abs(y_inter-y_base)/y_norm\n",
    "  return diff_dict\n",
    "\n",
    "def filter_by_regex(regex,col_list,exclude=False):\n",
    "  filtered_set = set()\n",
    "  if type(regex) is list:\n",
    "    for reg in regex:\n",
    "      for i in col_list:\n",
    "        if re.search(reg,i):\n",
    "          filtered_set.add(i)\n",
    "  else:\n",
    "    for i in col_list:\n",
    "      if re.search(regex,i):\n",
    "        filtered_set.add(i)\n",
    "\n",
    "  filtered_list = list(filtered_set)\n",
    "  if exclude:\n",
    "    col_list_copy = list(col_list.copy())\n",
    "    for i in filtered_list:\n",
    "      if i in col_list_copy:\n",
    "        col_list_copy.remove(i)\n",
    "    filtered_list = col_list_copy\n",
    "\n",
    "  # Restore the original order\n",
    "  filtered_original_ordered_list = []\n",
    "  for i in list(col_list):\n",
    "    if i in filtered_list:\n",
    "      filtered_original_ordered_list.append(i)\n",
    "  return filtered_original_ordered_list\n",
    "\n",
    "def limit_by_col_val(min_val,max_val,col_name,df):\n",
    "  filter = (df[col_name]>=min_val) &(df[col_name] <=max_val)\n",
    "  return df[filter]\n",
    "\n",
    "def abs_mean_value_upto_l(pd_series,L_max:int):\n",
    "  idx = pd_series.index\n",
    "  abs_cum_sum = 0\n",
    "  num = 0\n",
    "  for i in idx:\n",
    "    L = int(i.split(\",\")[0][3:])\n",
    "    if L > L_max:\n",
    "      continue\n",
    "    else:\n",
    "      abs_cum_sum = abs_cum_sum+abs(pd_series[i])\n",
    "      num = num +1\n",
    "  return abs_cum_sum/num\n",
    "\n",
    "def get_mode(name):\n",
    "  return int(name.split(\"(\")[-1].split(\")\")[0])\n",
    "def get_radii(name):\n",
    "  if name[-5]=='R':\n",
    "    # R0257 -> 0257load_and_pickle\n",
    "    return int(name.split('_')[-1][1:])\n",
    "  else:\n",
    "    return int(name.split('_')[-1])\n",
    "def sort_by_power_modes(col_names):\n",
    "  col_name_copy = list(col_names).copy()\n",
    "  return sorted(col_name_copy, key=lambda x: int(get_mode(x)))\n",
    "\n",
    "def add_L_mode_power(df:pd.DataFrame,L:int, ReOrIm:str):\n",
    "  column_names = df.columns\n",
    "  n = 0\n",
    "  power = 0\n",
    "  for m in range(-L,L+1):\n",
    "    col_name = f'{ReOrIm}({L},{m})'\n",
    "    # print(col_name)\n",
    "    if col_name in column_names:\n",
    "      power = power + df[col_name]*df[col_name]\n",
    "      n = n + 1\n",
    "  if n != 0:\n",
    "    power = power/n\n",
    "    df[f'pow_{ReOrIm}({L})'] = power\n",
    "  return power\n",
    "\n",
    "def add_all_L_mode_power(df:pd.DataFrame,L_max:int):\n",
    "  local_df = df.copy()\n",
    "  total_power_Re = 0\n",
    "  total_power_Im = 0\n",
    "  for l in range(0,L_max+1):\n",
    "    total_power_Re = total_power_Re + add_L_mode_power(local_df,l,\"Re\")\n",
    "    total_power_Im = total_power_Im + add_L_mode_power(local_df,l,\"Im\")\n",
    "    local_df[f\"pow_cum_Re({l})\"] = total_power_Re\n",
    "    local_df[f\"pow_cum_Im({l})\"] = total_power_Im\n",
    "  return local_df\n",
    "\n",
    "\n",
    "def create_power_diff_dict(power_dict:dict, pow_mode:str, variable:str, base_key:str):\n",
    "  diff_dict = {\"t(M)\":power_dict[base_key]['t(M)']}\n",
    "  y_base = power_dict[base_key][variable][pow_mode]\n",
    "  y_norm = np.linalg.norm(y_base)\n",
    "  for key in power_dict:\n",
    "    if key == base_key:\n",
    "      continue\n",
    "    y = power_dict[key][variable][pow_mode]\n",
    "    t = power_dict[key]['t(M)']\n",
    "    y_interpolator = interp1d(t, y, kind='cubic',fill_value='extrapolate')\n",
    "    y_inter = y_interpolator(diff_dict['t(M)'])\n",
    "    diff_dict[key+\"_diff\"] = y_inter-y_base\n",
    "    diff_dict[key+\"_absdiff\"] = np.abs(y_inter-y_base)\n",
    "    diff_dict[key+\"_rel_diff\"] = (y_inter-y_base)/y_norm\n",
    "    diff_dict[key+\"_rel_absdiff\"] = np.abs(y_inter-y_base)/y_norm\n",
    "  return diff_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_and_modify_bondi_data(input_file, output_file, eps=1e-12, rand_amp=0):\n",
    "    shutil.copy(input_file, output_file)\n",
    "    with h5py.File(output_file, \"r+\") as outfile:\n",
    "        for key1, item1 in outfile.items():\n",
    "            if \"Version\" in key1:\n",
    "                continue\n",
    "            if isinstance(item1, h5py.Dataset):\n",
    "                print(f\"----Modifying {key1}\")\n",
    "                data = outfile[key1][()] + 0\n",
    "                mask = np.abs(data) < eps\n",
    "                mask[:, 0] = False  # Time should not be filtered\n",
    "                data[mask] = rand_amp * (2 * np.random.rand(*data[mask].shape) - 1)\n",
    "                outfile[key1][()] = data\n",
    "\n",
    "\n",
    "def create_WT_data_compressed(\n",
    "    input_file,\n",
    "    output_file,\n",
    "    trange=np.arange(0, 10000, 0.1),\n",
    "    rand_amp=None,\n",
    "    extraction_radii=100,\n",
    "):\n",
    "    shutil.copy(input_file, output_file)\n",
    "    with h5py.File(output_file, \"r+\") as outfile:\n",
    "        for key1, item1 in outfile.items():\n",
    "            if \"Version\" in key1:\n",
    "                continue\n",
    "            if isinstance(item1, h5py.Dataset):\n",
    "                print(f\"----Modifying {key1}\")\n",
    "                data = np.zeros((len(trange), outfile[key1].shape[1]))\n",
    "                print(data.shape)\n",
    "                data[:, 0] = trange\n",
    "                R_val = 3.544907701811031 * extraction_radii\n",
    "                if \"W.dat\" == key1:\n",
    "                    data[:, 1] = -89.09324794930309 / R_val**2\n",
    "                if \"R.dat\" == key1:\n",
    "                    data[:, 1] = R_val\n",
    "                if rand_amp is not None:\n",
    "                    data[:, 1:] += rand_amp * (\n",
    "                        2 * np.random.rand(*data[:, 1:].shape) - 1\n",
    "                    )\n",
    "\n",
    "                # Save all attributes\n",
    "                attr_dict = {}\n",
    "                for attr_key, attr_value in outfile[key1].attrs.items():\n",
    "                    attr_dict[attr_key] = attr_value\n",
    "\n",
    "                del outfile[key1]\n",
    "                ds = outfile.create_dataset(\n",
    "                    key1,\n",
    "                    data=data,\n",
    "                    compression=\"gzip\",  # or 'lzf', 'szip'\n",
    "                    compression_opts=9 if \"gzip\" else None,  # level 0-9 for gzip\n",
    "                    chunks=True,  # chunking is required for compression\n",
    "                )\n",
    "\n",
    "                # Restore all attributes\n",
    "                for attr_key, attr_value in attr_dict.items():\n",
    "                    ds.attrs[attr_key] = attr_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_config_file(\n",
    "    BoundaryDataPath: Path,\n",
    "    InputSavePath: Path = None,\n",
    "    which_ID: str = \"ConformalFactor\",\n",
    "    options_dict_user={},\n",
    ") -> Path:\n",
    "    options_dict = {\n",
    "        \"Cce.Evolution.TimeStepper.AdamsBashforth.Order\": 3,\n",
    "        \"Cce.Evolution.StepChoosers.Constant\": 0.1,\n",
    "        \"Cce.Evolution.StepChoosers.ErrorControl(SwshVars).AbsoluteTolerance\": 1e-9,\n",
    "        \"Cce.Evolution.StepChoosers.ErrorControl(SwshVars).RelativeTolerance\": 1e-7,\n",
    "        \"Cce.Evolution.StepChoosers.ErrorControl(CoordVars).AbsoluteTolerance\": 1e-9,\n",
    "        \"Cce.Evolution.StepChoosers.ErrorControl(CoordVars).RelativeTolerance\": 1e-8,\n",
    "        \"Cce.LMax\": 25,\n",
    "        \"Cce.NumberOfRadialPoints\": 20,\n",
    "        \"Cce.ObservationLMax\": 8,\n",
    "        \"Cce.H5Interpolator.BarycentricRationalSpanInterpolator.MinOrder\": 10,\n",
    "        \"Cce.H5Interpolator.BarycentricRationalSpanInterpolator.MaxOrder\": 10,\n",
    "        \"Cce.H5LookaheadTimes\": 10000,\n",
    "        \"Cce.Filtering.RadialFilterHalfPower\": 64,\n",
    "        \"Cce.Filtering.RadialFilterAlpha\": 35.0,\n",
    "        \"Cce.Filtering.FilterLMax\": 18,\n",
    "        \"Cce.ScriInterpOrder\": 5,\n",
    "        \"Cce.ScriOutputDensity\": 1,\n",
    "    }\n",
    "\n",
    "    for key in options_dict_user.keys():\n",
    "        if key not in options_dict:\n",
    "            raise ValueError(f\"Key {key} is not a valid option.\")\n",
    "        else:\n",
    "            options_dict[key] = options_dict_user[key]\n",
    "\n",
    "    CCE_ID_data = \"\"\n",
    "    match which_ID:\n",
    "        case \"ConformalFactor\":\n",
    "            CCE_ID_data = \"\"\"\n",
    "    ConformalFactor:\n",
    "      AngularCoordTolerance: 1e-13\n",
    "      MaxIterations: 1000 # Do extra iterations in case we improve.\n",
    "      RequireConvergence: False # Often don't converge to 1e-13, but that's fine\n",
    "      OptimizeL0Mode: True\n",
    "      UseBetaIntegralEstimate: False\n",
    "      ConformalFactorIterationHeuristic: SpinWeight1CoordPerturbation\n",
    "      UseInputModes: False\n",
    "      InputModes: []\n",
    "\"\"\"\n",
    "        case \"InverseCubic\":\n",
    "            CCE_ID_data = \"\"\"\n",
    "    InverseCubic:\n",
    "\"\"\"\n",
    "        case \"ZeroNonSmooth\":\n",
    "            CCE_ID_data = \"\"\"\n",
    "    ZeroNonSmooth:\n",
    "      AngularCoordTolerance: 1e-13\n",
    "      MaxIterations: 1000\n",
    "      RequireConvergence: False\n",
    "\"\"\"\n",
    "        case \"NoIncomingRadiation\":\n",
    "            CCE_ID_data = \"\"\"\n",
    "    NoIncomingRadiation:\n",
    "      AngularCoordTolerance: 1e-13\n",
    "      MaxIterations: 1000\n",
    "      RequireConvergence: False\n",
    "\"\"\"\n",
    "    if InputSavePath is None:\n",
    "        InputSavePath = BoundaryDataPath.parent / \"cce.yaml\"\n",
    "    assert InputSavePath.parent.exists()\n",
    "\n",
    "    config_file = f\"\"\"\n",
    "# Distributed under the MIT License.\n",
    "# See LICENSE.txt for details.\n",
    "\n",
    "# This block is used by testing and the SpECTRE command line interface.\n",
    "Executable: CharacteristicExtract\n",
    "Testing:\n",
    "  Check: parse\n",
    "  Priority: High\n",
    "\n",
    "---\n",
    "Evolution:\n",
    "  InitialTimeStep: 0.25\n",
    "  InitialSlabSize: 10.0\n",
    "\n",
    "ResourceInfo:\n",
    "  AvoidGlobalProc0: false\n",
    "  Singletons: Auto\n",
    "\n",
    "Observers:\n",
    "  VolumeFileName: \"vol_{str(InputSavePath.stem)}\"\n",
    "  ReductionFileName: \"red_{str(InputSavePath.stem)}\"\n",
    "\n",
    "EventsAndTriggers:\n",
    "  # Write the CCE time step every Slab. A Slab is a fixed length of simulation\n",
    "  # time and is not influenced by the dynamically adjusted step size.\n",
    "  - Trigger:\n",
    "      Slabs:\n",
    "        EvenlySpaced:\n",
    "          Offset: 0\n",
    "          Interval: 1\n",
    "    Events:\n",
    "      - ObserveTimeStep:\n",
    "          # The output is written into the \"ReductionFileName\" HDF5 file under\n",
    "          # \"/SubfileName.dat\"\n",
    "          SubfileName: CceTimeStep\n",
    "          PrintTimeToTerminal: true\n",
    "\n",
    "Cce:\n",
    "  Evolution:\n",
    "    TimeStepper:\n",
    "      AdamsBashforth:\n",
    "        Order: {options_dict['Cce.Evolution.TimeStepper.AdamsBashforth.Order']} # Going to higher order doesn't seem necessary for CCE\n",
    "    StepChoosers:\n",
    "      - Constant: {options_dict['Cce.Evolution.StepChoosers.Constant']} # Don't take steps bigger than 0.1M\n",
    "      - Increase:\n",
    "          Factor: 2\n",
    "      - ErrorControl(SwshVars):\n",
    "          AbsoluteTolerance: {options_dict['Cce.Evolution.StepChoosers.ErrorControl(SwshVars).AbsoluteTolerance']}\n",
    "          RelativeTolerance: {options_dict['Cce.Evolution.StepChoosers.ErrorControl(SwshVars).RelativeTolerance']}\n",
    "          # These factors control how much the time step is changed at once.\n",
    "          MaxFactor: 2\n",
    "          MinFactor: 0.25\n",
    "          # How close to the \"perfect\" time step we take. Since the \"perfect\"\n",
    "          # value assumes a linear system, we need some safety factor since our\n",
    "          # system is nonlinear, and also so that we reduce how often we retake\n",
    "          # time steps.\n",
    "          SafetyFactor: 0.9\n",
    "      - ErrorControl(CoordVars):\n",
    "          AbsoluteTolerance: {options_dict['Cce.Evolution.StepChoosers.ErrorControl(CoordVars).AbsoluteTolerance']}\n",
    "          RelativeTolerance: {options_dict['Cce.Evolution.StepChoosers.ErrorControl(CoordVars).RelativeTolerance']}\n",
    "          # These factors control how much the time step is changed at once.\n",
    "          MaxFactor: 2\n",
    "          MinFactor: 0.25\n",
    "          # How close to the \"perfect\" time step we take. Since the \"perfect\"\n",
    "          # value assumes a linear system, we need some safety factor since our\n",
    "          # system is nonlinear, and also so that we reduce how often we retake\n",
    "          # time steps.\n",
    "          SafetyFactor: 0.9\n",
    "\n",
    "  # The number of angular modes used by the CCE evolution. This must be larger\n",
    "  # than ObservationLMax. We always use all of the m modes for the LMax since\n",
    "  # using fewer m modes causes aliasing-driven instabilities.\n",
    "  LMax: {options_dict['Cce.LMax']}\n",
    "  # Probably don't need more than 15 radial grid points, but could increase\n",
    "  # up to ~20\n",
    "  NumberOfRadialPoints: {options_dict['Cce.NumberOfRadialPoints']}\n",
    "  # The maximum ell we use for writing waveform output. While CCE can dump\n",
    "  # more, you should be cautious with higher modes since mode mixing, truncation\n",
    "  # error, and systematic numerical effects can have significant contamination\n",
    "  # in these modes.\n",
    "  ObservationLMax: {options_dict['Cce.ObservationLMax']}\n",
    "\n",
    "  InitializeJ:\n",
    "    # To see what other J-initialization procedures are available, comment\n",
    "    # out this group of options and do, e.g. \"Blah:\" The code will print\n",
    "    # an error message with the available options and a help string.\n",
    "    # More details can be found at spectre-code.org.\n",
    "{CCE_ID_data}\n",
    "\n",
    "  StartTime: Auto\n",
    "  EndTime: Auto\n",
    "  ExtractionRadius: Auto\n",
    "\n",
    "  BoundaryDataFilename: {BoundaryDataPath.name}\n",
    "  H5IsBondiData: True\n",
    "  H5Interpolator:\n",
    "    BarycentricRationalSpanInterpolator:\n",
    "      MinOrder: {options_dict['Cce.H5Interpolator.BarycentricRationalSpanInterpolator.MinOrder']}\n",
    "      MaxOrder: {options_dict['Cce.H5Interpolator.BarycentricRationalSpanInterpolator.MaxOrder']}\n",
    "  FixSpecNormalization: False\n",
    "\n",
    "  H5LookaheadTimes: {options_dict['Cce.H5LookaheadTimes']}\n",
    "\n",
    "  Filtering:\n",
    "    RadialFilterHalfPower: {options_dict['Cce.Filtering.RadialFilterHalfPower']}\n",
    "    RadialFilterAlpha: {options_dict['Cce.Filtering.RadialFilterAlpha']}\n",
    "    FilterLMax: {options_dict['Cce.Filtering.FilterLMax']}\n",
    "\n",
    "  ScriInterpOrder: {options_dict['Cce.ScriInterpOrder']}\n",
    "  ScriOutputDensity: {options_dict['Cce.ScriOutputDensity']}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    with InputSavePath.open(\"w\") as f:\n",
    "        f.writelines(config_file)\n",
    "\n",
    "    return InputSavePath\n",
    "\n",
    "\n",
    "def make_submit_file(\n",
    "    save_folder_path: Path,\n",
    "    cce_input_file_path: Path,\n",
    "    CCE_Executable_path: Path,\n",
    "    write_scripts_only=False,\n",
    "):\n",
    "    submit_script = f\"\"\"#!/bin/bash -\n",
    "#SBATCH -J CCE_{save_folder_path.stem}             # Job Name\n",
    "#SBATCH -o CCE.stdout                 # Output file name\n",
    "#SBATCH -e CCE.stderr                 # Error file name\n",
    "#SBATCH -n 2                          # Number of cores\n",
    "#SBATCH -p expansion                  # Queue name\n",
    "#SBATCH --ntasks-per-node 2           # number of MPI ranks per node\n",
    "#SBATCH -t 24:0:00   # Run time\n",
    "#SBATCH -A sxs                # Account name\n",
    "#SBATCH --no-requeue\n",
    "#SBATCH --reservation=sxs_standing\n",
    "\n",
    "# Go to the correct folder with the boundary data\n",
    "cd {save_folder_path}\n",
    "\n",
    "# run CCE\n",
    "{CCE_Executable_path} --input-file ./{cce_input_file_path.name}\n",
    "\"\"\"\n",
    "    submit_script_path = save_folder_path / \"submit.sh\"\n",
    "    submit_script_path.write_text(submit_script)\n",
    "\n",
    "    if not write_scripts_only:\n",
    "        command = f\"cd {save_folder_path} && qsub {submit_script_path}\"\n",
    "        status = subprocess.run(command, capture_output=True, shell=True, text=True)\n",
    "        if status.returncode == 0:\n",
    "            print(f\"Succesfully submitted {submit_script_path}\\n{status.stdout}\")\n",
    "        else:\n",
    "            sys.exit(\n",
    "                f\"Job submission failed for {submit_script_path} with error: \\n{status.stdout} \\n{status.stderr}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_CCE_single_bh(\n",
    "    NewCCEPath: Path,\n",
    "    BondiBaseH5File: Path,\n",
    "    CCE_Executable_path: Path,\n",
    "    ExtractionRadii_str: str,\n",
    "    CCE_ID: str = \"ConformalFactor\",\n",
    "    options_dict_user={},\n",
    "    write_scripts_only=False,\n",
    "    trange=np.arange(0, 10000, 0.1),\n",
    "    rand_amp=None,\n",
    "):\n",
    "    NewCCEPath = NewCCEPath.resolve()\n",
    "    BondiBaseH5File = BondiBaseH5File.resolve()\n",
    "    CCE_Executable_path = CCE_Executable_path.resolve()\n",
    "\n",
    "    if not BondiBaseH5File.exists():\n",
    "        raise Exception(f\"{BondiBaseH5File} does not exist!\")\n",
    "    if not CCE_Executable_path.exists():\n",
    "        raise Exception(f\"{CCE_Executable_path} does not exist!\")\n",
    "\n",
    "    NewCCEPath.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "    create_WT_data_compressed(\n",
    "        BondiBaseH5File,\n",
    "        NewCCEPath / f\"BondiDataR{ExtractionRadii_str}.h5\",\n",
    "        trange=trange,\n",
    "        rand_amp=rand_amp,\n",
    "        extraction_radii=int(ExtractionRadii_str),\n",
    "    )\n",
    "\n",
    "    make_config_file(\n",
    "        BoundaryDataPath=NewCCEPath / f\"BondiDataR{ExtractionRadii_str}.h5\",\n",
    "        InputSavePath=NewCCEPath / f\"cce.yaml\",\n",
    "        which_ID=CCE_ID,\n",
    "        options_dict_user=options_dict_user,\n",
    "    )\n",
    "\n",
    "    make_submit_file(\n",
    "        save_folder_path=NewCCEPath,\n",
    "        cce_input_file_path=NewCCEPath / f\"cce.yaml\",\n",
    "        CCE_Executable_path=CCE_Executable_path,\n",
    "        write_scripts_only=write_scripts_only,\n",
    "    )\n",
    "\n",
    "    print(\"DONE!\\n\\n\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_CCE_single_bh(\n",
    "#     NewCCEPath = Path(\"./test_CCE\"),\n",
    "#     BondiBaseH5File = Path(\"/media/himanshu/T7_500g/CCE_data/all_bondi_data/BondiCceR0100.h5\"),\n",
    "#     CCE_Executable_path = Path(\"/media/himanshu/T7_500g/CCE_data/CCE_executable/CharacteristicExtract\"),\n",
    "#     ExtractionRadii_str='0100',\n",
    "#     CCE_ID=\"ConformalFactor\",\n",
    "#     write_scripts_only=True,\n",
    "#     trange=np.arange(0, 1000, 0.1),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_h5_file_dump_tensors(file_path: str) -> Dict[str, Any]:\n",
    "    def read_group(group) -> Dict[str, Any]:\n",
    "        result = {}\n",
    "\n",
    "        # Check if group is actually a group before iterating\n",
    "        if isinstance(group, h5py.Group):\n",
    "            # Read all datasets in current group\n",
    "            for name, item in group.items():\n",
    "                if isinstance(item, h5py.Dataset):\n",
    "                    # Convert dataset to numpy array\n",
    "                    result[name] = item[()]\n",
    "                elif isinstance(item, h5py.Group):\n",
    "                    # Recursively read nested group\n",
    "                    result[name] = read_group(item)\n",
    "                else:\n",
    "                    print(name, item)\n",
    "\n",
    "            # Read attributes\n",
    "            for name, item in group.attrs.items():\n",
    "                result[name] = item\n",
    "        elif isinstance(group, h5py.Dataset):\n",
    "            # If it's a dataset, just return its value\n",
    "            return group[()]\n",
    "\n",
    "        return result\n",
    "\n",
    "    # Check if file exists\n",
    "    if not Path(file_path).exists():\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist\")\n",
    "    try:\n",
    "        with h5py.File(file_path, \"r\") as f:\n",
    "            # Read all contents\n",
    "            data = {}\n",
    "            # Read main groups\n",
    "            for name, item in f.items():\n",
    "                data[name] = read_group(f[name])\n",
    "            # # Read main groups\n",
    "            # for group_name in [\"InitGridHi\", \"InitHhatt\", \"kappa\", \"psi\"]:\n",
    "            #     if group_name in f:\n",
    "            #         data[group_name] = read_group(f[group_name])\n",
    "        return data\n",
    "\n",
    "    except OSError as e:\n",
    "        raise OSError(f\"Error reading HDF5 file: {str(e)}\")\n",
    "\n",
    "\n",
    "def create_modification_dict(\n",
    "    filter_modes_output_path: Path, filtered_vars_folders_path: Path\n",
    "):\n",
    "\n",
    "    filtered_sub_domains = set()\n",
    "    with filter_modes_output_path.open() as f:\n",
    "        for l in f.readlines():\n",
    "            if re.match(r\"Filtered=\", l):\n",
    "                domain_name = l.split(\"=\")[-1].strip()\n",
    "                filtered_sub_domains.add(domain_name)\n",
    "\n",
    "    filtered_dict = {}\n",
    "    vars_h5_files = list(filtered_vars_folders_path.glob(\"Vars_*.h5\"))\n",
    "    # load data for subdomains that were filtered\n",
    "    for fp in vars_h5_files:\n",
    "        domain_name = fp.stem.split(\"_\")[-1]\n",
    "        if domain_name in filtered_sub_domains:\n",
    "            filtered_dict[domain_name] = read_h5_file_dump_tensors(fp)\n",
    "\n",
    "    return filtered_dict\n",
    "\n",
    "\n",
    "def copy_and_modify_h5file(input_file, output_file, modification_data_dict):\n",
    "    shutil.copy(input_file, output_file)\n",
    "    with h5py.File(output_file, \"r+\") as outfile:\n",
    "        # Level 1: Root level items\n",
    "        for key1, item1 in outfile.items():\n",
    "            if isinstance(item1, h5py.Group):\n",
    "                # Level 2: First nested level\n",
    "                for key2, item2 in item1.items():\n",
    "                    if isinstance(item2, h5py.Group):\n",
    "                        # Level 3: Second nested level\n",
    "                        for key3, item3 in item2.items():\n",
    "                            if isinstance(item3, h5py.Dataset):\n",
    "                                # print(f\"{key1}/{key2}/{key3}\")\n",
    "                                if key1 in modification_data_dict:\n",
    "                                    print(f\"----Modifying {key1}/{key3}\")\n",
    "                                    outfile[key1][key2][key3][()] = (\n",
    "                                        modification_data_dict[key1][key2][key3]\n",
    "                                    )\n",
    "                            else:\n",
    "                                raise ValueError(f\"Unexpected item type: {type(item3)}\")\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unexpected item type: {type(item2)}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected item type: {type(item1)}\")\n",
    "\n",
    "\n",
    "def make_filtered_checkpoints(\n",
    "    checkpoint_folder: Path, new_checkpoint_folder: Path, data_dict: dict\n",
    "):\n",
    "    # Copy the original checkpoint file\n",
    "    for fp in checkpoint_folder.glob(\"*.txt\"):\n",
    "        shutil.copy(fp, new_checkpoint_folder)\n",
    "\n",
    "    for fp in checkpoint_folder.glob(\"*.h5\"):\n",
    "        file_name = fp.stem\n",
    "        # get domain name for the h5 files\n",
    "        if file_name == \"SerialCheckpoint\":\n",
    "            # Copy this without change\n",
    "            shutil.copy(fp, new_checkpoint_folder)\n",
    "            continue\n",
    "\n",
    "        domain_name = file_name.split(\"_\")[-1]\n",
    "\n",
    "        if domain_name in data_dict:\n",
    "            print(f\"Modifying the domain {domain_name}\")\n",
    "            modified_data = data_dict[domain_name]\n",
    "            modified_fp = new_checkpoint_folder / fp.name\n",
    "            copy_and_modify_h5file(fp, modified_fp, modified_data)\n",
    "        else:\n",
    "            print(f\"Copying the domain {domain_name}\")\n",
    "            shutil.copy(fp, new_checkpoint_folder)\n",
    "\n",
    "\n",
    "def make_input_file_content(half_power_R, half_power_L):\n",
    "    input_file_text = f\"\"\"DataBoxItems =\n",
    "        ReadFromFile(File=./SpatialCoordMap.input),\n",
    "        ReadFromFile(File=./GaugeItems.input),\n",
    "        Domain(Items=\n",
    "            AddGeneralizedHarmonicInfo(MatterSourceName=;)\n",
    "            ),\n",
    "        Subdomain(Items =\n",
    "                Add3Plus1ItemsFromGhPsiKappa(psi=psi;kappa=kappa;OutputPrefix=),\n",
    "                AddSpacetimeJacobianAndHessianItems(MapPrefix=GridToInertial;),\n",
    "                GlobalDifferentiator\n",
    "                (GlobalDifferentiator=\n",
    "                    MatrixMultiply(MultiDim_by_BasisFunction=yes;\n",
    "                    TopologicalDifferentiator\n",
    "                    =Spectral(SetBasisFunctionsFromTimingInfo=yes;\n",
    "                            # BasisFunctions= (ChebyshevGaussLobatto=ChebyshevGaussLobattoMatrix);\n",
    "                            )\n",
    "                    );\n",
    "                ),\"\"\"\n",
    "    for R, L, i in zip(half_power_R, half_power_L, np.arange(len(half_power_R))):\n",
    "        input_file_text = (\n",
    "            input_file_text\n",
    "            + f\"\"\"\n",
    "    # ==============================================================================\n",
    "                FilterVar(\n",
    "                    Input = psi;\n",
    "                    Output = fil_psi{i};\n",
    "                    WhichFilter = Exp;\n",
    "                    RadialHalfPower = {R};\n",
    "                    AngularHalfPower = {L};\n",
    "                    DomainRegex = SphereC{i};\n",
    "                ),\n",
    "                FilterVar(\n",
    "                    Input = kappa;\n",
    "                    Output = fil_kappa{i};\n",
    "                    WhichFilter = Exp;\n",
    "                    RadialHalfPower = {R};\n",
    "                    AngularHalfPower = {L};\n",
    "                    DomainRegex = SphereC{i};\n",
    "                ),\n",
    "                FirstDeriv(\n",
    "                    Input = fil_psi{i};\n",
    "                    Output = fil_dpsi_TT{i};\n",
    "                    # SetDerivDimFromTensorDim=True;\n",
    "                    MapPrefix = GridToInertial;\n",
    "                ),\n",
    "                FlattenDeriv(\n",
    "                    Input = fil_dpsi_TT{i};\n",
    "                    Output = fil_dpsi{i};\n",
    "                    DerivPosition = First;\n",
    "                    ZeroFillOffset=1;\n",
    "                ),\n",
    "                FakeKappa(\n",
    "                    FildPsi = fil_dpsi{i};\n",
    "                    FilKappa = fil_kappa{i};\n",
    "                    Output = fake_kappa{i};\n",
    "                ),\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "    input_file_text = (\n",
    "        input_file_text\n",
    "        + \"\"\"\n",
    "\n",
    "\n",
    "    ;);#Subdomains\n",
    "    Observers =\"\"\"\n",
    "    )\n",
    "\n",
    "    for R, L, i in zip(half_power_R, half_power_L, np.arange(len(half_power_R))):\n",
    "        input_file_text = (\n",
    "            input_file_text\n",
    "            + f\"\"\"        DumpTensors(\n",
    "        Input = fil_psi{i},fake_kappa{i};\n",
    "        FileNames = psi,kappa;\n",
    "        OnlyTheseSubdomains = SphereC{i};\n",
    "        ),        \n",
    "        ObserveInSubdir\n",
    "        (Subdir=SpecCoeffs;\n",
    "          Observers =\n",
    "          DumpSpectralCoefficients(\n",
    "            Input = fake_kappa{i}, fil_psi{i}, fil_kappa{i}, fil_dpsi{i};\n",
    "            TakeLog=no;\n",
    "            Subdomains =SphereC{i};\n",
    "            # Eps = 1e-16;\n",
    "          ),\n",
    "        ),\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "    input_file_text = (\n",
    "        input_file_text\n",
    "        + \"\"\"\n",
    "        ObserveInSubdir\n",
    "        (Subdir=SpecCoeffs;\n",
    "          Observers =\n",
    "          DumpSpectralCoefficients(\n",
    "            Input = psi, kappa;\n",
    "            TakeLog=no;\n",
    "            Subdomains = *;\n",
    "            # Eps = 1e-16;\n",
    "          ),\n",
    "        ),\n",
    "        ;\n",
    "    \"\"\"\n",
    "    )\n",
    "    return input_file_text\n",
    "\n",
    "\n",
    "def make_filtered_checkpoint_from_another(\n",
    "    ApplyObserversPath: Path,\n",
    "    InputAndHistFolderPath: Path,\n",
    "    CheckpointFolderPath: Path,\n",
    "    work_dir: Path,\n",
    "    HalfPowerArr: np.ndarray,\n",
    "):\n",
    "\n",
    "    if not CheckpointFolderPath.exists():\n",
    "        raise Exception(f\"{CheckpointFolderPath} does not exist!!\")\n",
    "\n",
    "    filtered_checkpoint_path = work_dir / \"filtered_checkpoint\"\n",
    "    filtered_text_files_path = work_dir / \"data\"\n",
    "\n",
    "    work_dir.mkdir(parents=True, exist_ok=True)\n",
    "    filtered_checkpoint_path.mkdir(parents=False, exist_ok=True)\n",
    "    filtered_text_files_path.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "    for f in CheckpointFolderPath.glob(\"*\"):\n",
    "        shutil.copy(f, work_dir)\n",
    "    for f in InputAndHistFolderPath.glob(\"Hist*.txt\"):\n",
    "        shutil.copy(f, work_dir)\n",
    "    for f in InputAndHistFolderPath.glob(\"*.input\"):\n",
    "        shutil.copy(f, work_dir)\n",
    "\n",
    "    apply_observer = f\"\"\"#!/bin/bash\n",
    ". /home/hchaudha/spec/MakefileRules/this_machine.env\n",
    "cd {work_dir}\n",
    "\n",
    "{ApplyObserversPath} -t psi,kappa,InitHhatt,InitGridHi -r 11,122,,1 -d 4,4,1,3 -domaininput ./GrDomain.input -h5prefix Cp-VarsGr ./input_file.input\n",
    "    \"\"\"\n",
    "    with open(work_dir / \"input_file.input\", \"w\") as f:\n",
    "        f.write(make_input_file_content(HalfPowerArr, HalfPowerArr))\n",
    "\n",
    "    with open(work_dir / \"apply_observer.sh\", \"w\") as f:\n",
    "        f.write(apply_observer)\n",
    "\n",
    "    command = f\"cd {work_dir} && bash ./apply_observer.sh > ./apply_observer.out\"\n",
    "    apply_observer_output_path = work_dir / \"apply_observer.out\"\n",
    "\n",
    "    status = subprocess.run(command, capture_output=True, shell=True, text=True)\n",
    "    if status.returncode == 0:\n",
    "        print(f\"Ran FilterModes observer in {work_dir}.\\n {status.stdout}\")\n",
    "    else:\n",
    "        sys.exit(\n",
    "            f\"Failed to run FilterModes observer in {work_dir}. \\n {status.stderr}\"\n",
    "        )\n",
    "\n",
    "    # load the filtered data in txt format into a dictionary\n",
    "    filtered_data_dict = create_modification_dict(\n",
    "        filter_modes_output_path=apply_observer_output_path,\n",
    "        filtered_vars_folders_path=work_dir,\n",
    "    )\n",
    "    print(\n",
    "        f\"Loaded filtered data for the following domains: {filtered_data_dict.keys()}\"\n",
    "    )\n",
    "\n",
    "    # create new checkpoint files with the filtered data\n",
    "    make_filtered_checkpoints(\n",
    "        CheckpointFolderPath, filtered_checkpoint_path, filtered_data_dict\n",
    "    )\n",
    "\n",
    "    # Rename the original checkpoint folder\n",
    "    new_original_checkpoint_folder_name = (\n",
    "        CheckpointFolderPath.parent / f\"{CheckpointFolderPath.stem}_original\"\n",
    "    )\n",
    "    shutil.move(CheckpointFolderPath, new_original_checkpoint_folder_name)\n",
    "\n",
    "    # Copy the filtered checkpoint data into the place of the original checkpoint data\n",
    "    shutil.copytree(work_dir / \"filtered_checkpoint\", CheckpointFolderPath)\n",
    "\n",
    "    # Copy the main file for reproducibility\n",
    "    shutil.copy(Path(__file__).absolute(), work_dir, follow_symlinks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {}\n",
    "# for file in Path(\"./all_bondi_data\").glob(\"BondiCceR*\"):\n",
    "#     new_data = read_h5_file_dump_tensors(Path(file))\n",
    "#     W_max = np.max(np.abs(new_data['W.dat'][:,1:]))\n",
    "#     R_max = np.max(np.abs(new_data['R.dat'][:,1:]))\n",
    "#     key = file.stem\n",
    "#     data[key] = {'W': W_max, 'R': R_max}\n",
    "#     print(key, W_max, R_max)\n",
    "\n",
    "# for key in data.keys():\n",
    "#     radii = float(key.split(\"R\")[-1])\n",
    "#     print(key, data[key]['R']/radii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## abd stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cce_data = {}\n",
    "# cce_data[f\"ode_tol_10\"] = Path(f\"/central/groups/sxs/hchaudha/spec_runs/single_bh/20_zero_spin_AMR_L5_10000M/GW_data/r100_data/ode_10/red_cce.h5\")\n",
    "# cce_data[f\"ode_tol_100\"] = Path(f\"/central/groups/sxs/hchaudha/spec_runs/single_bh/20_zero_spin_AMR_L5_10000M/GW_data/r100_data/ode_100/red_cce.h5\")\n",
    "# cce_data[f\"ode_tol_1000\"] = Path(f\"/central/groups/sxs/hchaudha/spec_runs/single_bh/20_zero_spin_AMR_L5_10000M/GW_data/r100_data/ode_1000/red_cce.h5\")\n",
    "# cce_data[f\"ode_tol_10000\"] = Path(f\"/central/groups/sxs/hchaudha/spec_runs/single_bh/20_zero_spin_AMR_L5_10000M/GW_data/r100_data/ode_10000/red_cce.h5\")\n",
    "cce_data[f\"test\"] = Path(f\"/groups/sxs/hchaudha/spec_runs/single_bh_CCE/runs/test1/LMax10_RadPts10/red_cce.h5\")\n",
    "\n",
    "\n",
    "fail_flag=False\n",
    "for key in cce_data:\n",
    "  if not cce_data[key].exists():\n",
    "    fail_flag = True\n",
    "    print(f\"{cce_data[key]} does not exist!\")\n",
    "  if fail_flag:\n",
    "    raise Exception(\"Some paths do not exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_interpolate = np.linspace(-1000,12000,num=2000)\n",
    "abd_data = {}\n",
    "failed_keys=  {}\n",
    "for key in cce_data:\n",
    "  try:\n",
    "    abd_data[key] = load_and_pickle(cce_data[key], options={'t_interpolate': t_interpolate})\n",
    "    abd_data[key] = load_bondi_constraints(cce_data[key])\n",
    "  except Exception as e:\n",
    "    failed_keys[key] = str(e)\n",
    "    print(f\"Failed to load and pickle data for key {key}: {e}\")\n",
    "    continue\n",
    "\n",
    "print(abd_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bondi_norms_to_plot = [0,1,2,3,4,5]\n",
    "bondi_norms_to_plot = [2]\n",
    "\n",
    "t_min = -10000\n",
    "# t_min = 2000\n",
    "# t_min = 2500\n",
    "# t_min = 7500\n",
    "# t_min = 4000 - 250\n",
    "t_max = 10000\n",
    "# t_max = 7751.5\n",
    "labels = None\n",
    "\n",
    "def get_radius(run_name):\n",
    "  radius_part = run_name.split(\"_\")[-1]\n",
    "  a = \"\".join([i for i in radius_part if i.isdigit()])\n",
    "  return int(a)\n",
    "\n",
    "min_radius = 0\n",
    "min_radius = 260\n",
    "max_radius = 1000\n",
    "max_radius = 360\n",
    "\n",
    "y_list = []\n",
    "for key in abd_data:\n",
    "  violation_dict = abd_data[key]['bondi_violation_norms']\n",
    "#   radius = get_radius(key)\n",
    "#   if radius > max_radius or radius < min_radius:\n",
    "#     continue\n",
    "  # if \"Factor\" in key:\n",
    "  #   continue\n",
    "\n",
    "  t_arr = abd_data[key][\"abd\"].t\n",
    "  trimmed_indices = (t_arr>t_min) & (t_arr<t_max)\n",
    "  t_arr = t_arr[trimmed_indices]\n",
    "\n",
    "  for i in bondi_norms_to_plot:\n",
    "    if labels is None:\n",
    "      plt.semilogy(t_arr,violation_dict[i][trimmed_indices],label=f\"{key}_{i}\")\n",
    "      # plt.semilogy(t_arr,violation_dict[i][trimmed_indices])\n",
    "    else:\n",
    "      plt.semilogy(t_arr,violation_dict[i][trimmed_indices],label=f\"{labels[key]}_{i}\")\n",
    "\n",
    "  plt.xlabel('t(M)')\n",
    "  plt.ylabel(f\"bondi violations {bondi_norms_to_plot}\")\n",
    "  plt.legend()\n",
    "  plt.tight_layout()\n",
    "#   plt.show()\n",
    "  # plt.savefig(Path(\"/home/hchaudha/notes/spec_accuracy/CCE/new_set_L3\")/f\"violations_alllll_{bondi_norms_to_plot}_t={t_min}-{t_max}.png\")\n",
    "  # plt.savefig(Path(\"/home/hchaudha/notes/spec_accuracy/CCE/\")/f\"{bondi_norms_to_plot}_t={t_min}-{t_max}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bondi_norms_to_plot = [0,1,2,3,4,5]\n",
    "bondi_norms_to_plot = [2]\n",
    "\n",
    "t_min = -10000\n",
    "# t_min = 1200\n",
    "# t_min = 2500\n",
    "# t_min = 7500\n",
    "# t_min = 4000 - 250\n",
    "t_max = 10000\n",
    "# t_max = 7751.5\n",
    "# t_max = 1200\n",
    "# t_max = 3600\n",
    "# t_max = 4000\n",
    "# t_max = 7000\n",
    "\n",
    "labels = None\n",
    "# labels={\n",
    "# #  \"high_accuracy_Lev5_R0258\" : \"Lev5_ode_tol_R0258\",\n",
    "#  \"6_set1_L3s3_200\" : \"Lev3_variant_R0200\",\n",
    "#  \"6_set1_L3s3_250\" : \"Lev3_variant_R0250\",\n",
    "#  \"6_set1_L3s3_300\" : \"Lev3_variant_R0300\",\n",
    "#  \"6_set1_L3s2_200\" : \"Lev2_variant_R0200\",\n",
    "#  \"6_set1_L3s2_250\" : \"Lev2_variant_R0250\",\n",
    "#  \"6_set1_L3s2_300\" : \"Lev2_variant_R0300\",\n",
    "# }\n",
    "\n",
    "def get_radius(run_name):\n",
    "  radius_part = run_name.split(\"_\")[-1]\n",
    "  a = \"\".join([i for i in radius_part if i.isdigit()])\n",
    "  return int(a)\n",
    "\n",
    "min_radius = 0\n",
    "# min_radius = 260\n",
    "max_radius = 1000\n",
    "# max_radius = 360\n",
    "\n",
    "y_list = []\n",
    "for key in abd_data:\n",
    "#   violation_dict = abd_data[key]['bondi_violation_norms']\n",
    "#   radius = get_radius(key)\n",
    "#   if radius > max_radius or radius < min_radius:\n",
    "#     continue\n",
    "  t_arr = abd_data[key][\"abd\"].t\n",
    "  trimmed_indices = (t_arr>t_min) & (t_arr<t_max)\n",
    "  t_arr = t_arr[trimmed_indices]\n",
    "\n",
    "  print(key)\n",
    "  violation_dict = abd_data[key]['abd'].bondi_rest_mass() - 1.0\n",
    "  plt.plot(t_arr,violation_dict[trimmed_indices],label=f\"{key}\")\n",
    "\n",
    "\n",
    "#   violation_dict = abd_data[key]['abd'].bondi_angular_momentum()\n",
    "#   plt.plot(t_arr,violation_dict[:,0][trimmed_indices],label=f\"{key}_0\")\n",
    "#   plt.plot(t_arr,violation_dict[:,1][trimmed_indices],label=f\"{key}_1\")\n",
    "#   plt.plot(t_arr,violation_dict[:,2][trimmed_indices],label=f\"{key}_2\")\n",
    "\n",
    "  plt.xlabel('t(M)')\n",
    "#   plt.ylabel(f\"bondi violations \")\n",
    "  plt.legend()\n",
    "  plt.tight_layout()\n",
    "#   plt.yscale('log')\n",
    "  # plt.show()\n",
    "  # plt.savefig(Path(\"/home/hchaudha/notes/spec_accuracy/CCE/new_set_L3\")/f\"violations_alllll_{bondi_norms_to_plot}_t={t_min}-{t_max}.png\")\n",
    "  # plt.savefig(Path(\"/home/hchaudha/notes/spec_accuracy/CCE/\")/f\"{bondi_norms_to_plot}_t={t_min}-{t_max}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WT_data = Path(\"./BondiCceR0100/BondiCceR0100.h5\")\n",
    "if not WT_data.exists():\n",
    "    raise Exception(f\"{WT_data} does not exist!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_h5_file_dump_tensors(WT_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data.keys():\n",
    "    if \"Version\" in key:\n",
    "        continue\n",
    "    print(key, np.max(np.abs(data[key][:,1:])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data.keys():\n",
    "    if \"Version\" in key:\n",
    "        continue\n",
    "    print(key, np.max(data[key][:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_and_modify_bondi_data(WT_data, WT_data.parent/\"new.h5\", eps=1e-12, rand_amp=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = read_h5_file_dump_tensors(WT_data.parent/\"new.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data = read_h5_file_dump_tensors(Path(\"/media/himanshu/T7_500g/CCE_data/BondiCceR0100/red_cce.h5\"))\n",
    "new_data = read_h5_file_dump_tensors(Path(\"/media/himanshu/T7_500g/CCE_data/test/red_cce.h5\"))\n",
    "cce_key = list(new_data.keys())[1]\n",
    "print(cce_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = new_data[cce_key]['Strain'][:,1:]\n",
    "# y = new_data[cce_key]['Psi0'][:,1:]\n",
    "# y = new_data[cce_key]['Psi1'][:,1:]\n",
    "# y = new_data[cce_key]['Psi2'][:,1:]\n",
    "# y = new_data[cce_key]['Psi3'][:,1:]\n",
    "# y = new_data[cce_key]['Psi4'][:,1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data = read_h5_file_dump_tensors(Path(\"/media/himanshu/T7_500g/CCE_data/Set1L6/BondiCceR0300.h5\"))\n",
    "new_data = read_h5_file_dump_tensors(Path(\"all_bondi_data/BondiCceR0020.h5\"))\n",
    "# new_data = read_h5_file_dump_tensors(Path(\"/media/himanshu/T7_500g/CCE_data/test/red_cce.h5\"))\n",
    "print(new_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = new_data['Beta.dat'][:,1:]\n",
    "y = new_data['DrJ.dat'][:,1:]\n",
    "y = new_data['DuR.dat'][:,1:]\n",
    "y = new_data['H.dat'][:,1:]\n",
    "# y = new_data['J.dat'][:,1:]\n",
    "# y = new_data['Q.dat'][:,1:]\n",
    "y = new_data['R.dat'][:,1:]\n",
    "# y = new_data['U.dat'][:,1:]\n",
    "# y = new_data['W.dat'][:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.max(np.abs(y), axis=1)[:10000])\n",
    "# plt.plot(y[:,8])\n",
    "# plt.plot(np.median(np.abs(y), axis=1)[:])\n",
    "plt.plot(np.median(np.abs(y), axis=1)[:]+1e-17)\n",
    "plt.yscale('log')\n",
    "# plt.ylim(1e-16, 1e-13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(new_data['Cce']['CceTimeStep.dat'][:,1:], label='CceTimeStep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(WT_data, \"r\") as outfile:\n",
    "    for key1, item1 in outfile.items():\n",
    "        print(key1, item1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_radii = '9999'\n",
    "create_WT_data_compressed(WT_data, Path(\"/media/himanshu/T7_500g/CCE_data/test\")/f\"new_temp_R{extraction_radii}.h5\", trange=np.arange(0,2500,0.1), rand_amp=None, extraction_radii = int(extraction_radii))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_test1 = Path(\"/groups/sxs/hchaudha/spec_runs/single_bh_CCE/runs/test1\")\n",
    "for i in folder_test1.glob(\"*\"):\n",
    "    print(i.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_rad_dep = Path(\"/groups/sxs/hchaudha/spec_runs/single_bh_CCE/runs/radius_dependence\")\n",
    "for i in folder_rad_dep.glob(\"*\"):\n",
    "    if i.stem in ['0001','0002']:\n",
    "        continue\n",
    "    print(i.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sxs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
