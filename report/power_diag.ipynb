{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from random import choice as rc\n",
    "import re\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_by_col_val(min_val,max_val,col_name,df):\n",
    "  filter = (df[col_name]>=min_val) &(df[col_name] <=max_val)\n",
    "  return df[filter]\n",
    "\n",
    "def read_dat_file(file_name):\n",
    "  cols_names = []\n",
    "  # Read column names\n",
    "  with open(file_name,'r') as f:\n",
    "      lines = f.readlines()\n",
    "      for line in lines:\n",
    "        if \"#\" not in line:\n",
    "          # From now onwards it will be all data\n",
    "          break\n",
    "        elif \"=\" in line:\n",
    "          if (\"[\" not in line) and (\"]\" not in line):\n",
    "             continue\n",
    "          cols_names.append(line.split('=')[-1][1:-1].strip())\n",
    "        else:\n",
    "          continue\n",
    "\n",
    "  return pd.read_csv(file_name,sep=\"\\s+\",comment=\"#\",names=cols_names)\n",
    "\n",
    "def find_subdomains(path:Path):\n",
    "  subdomain_set = set()\n",
    "  for i in path.iterdir():\n",
    "    if i.is_dir():\n",
    "      subdomain_set.add(i.stem)\n",
    "\n",
    "  return list(subdomain_set)\n",
    "\n",
    "def find_topologies(path:Path):\n",
    "  topologies_set = set()\n",
    "  for i in path.iterdir():\n",
    "    if i.is_file():\n",
    "      topologies_set.add(i.stem.split(\"_\")[0])\n",
    "\n",
    "  return list(topologies_set)\n",
    "\n",
    "def find_dat_file_names(path:Path):\n",
    "  file_name_set = set()\n",
    "  for i in path.iterdir():\n",
    "    if i.is_file():\n",
    "      file_name_set.add(i.stem.split(\"_\")[1])\n",
    "\n",
    "  return list(file_name_set)\n",
    "\n",
    "def get_top_name_and_mode(name):\n",
    "  # Bf0I1(12 modes).dat -> Bf0I1, 12\n",
    "  top_name = name.split(\"(\")[0]\n",
    "  mode = int(name.split(\"(\")[-1].split(\" \")[0])\n",
    "  return top_name,mode\n",
    "\n",
    "def find_highest_modes_for_topologies(path:Path):\n",
    "  highest_mode_dict = {}\n",
    "  for i in path.iterdir():\n",
    "    if i.is_file():\n",
    "      top_name, mode = get_top_name_and_mode(i.stem)\n",
    "      if top_name in highest_mode_dict:\n",
    "        if highest_mode_dict[top_name] < mode:\n",
    "          highest_mode_dict[top_name] = mode\n",
    "      else:\n",
    "        highest_mode_dict[top_name] = mode\n",
    "\n",
    "  return highest_mode_dict\n",
    "\n",
    "def make_mode_dataframe(path:Path):\n",
    "  highest_mode_dict = find_highest_modes_for_topologies(path)\n",
    "  top_dataframe_list = {i:[] for i in highest_mode_dict}\n",
    "\n",
    "  for i in path.iterdir():\n",
    "    for top_name in highest_mode_dict:\n",
    "      if (top_name+\"(\") in i.stem:\n",
    "        top_dataframe_list[top_name].append(read_dat_file(i))\n",
    "\n",
    "  top_mode_df_dict = {}\n",
    "  for i,df_list in top_dataframe_list.items():\n",
    "    result = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Remove duplicates based on 't' column (keep first occurrence)\n",
    "    # result = result.drop_duplicates(subset='t', keep='first')\n",
    "\n",
    "    # Sort by 't' and reset index\n",
    "    top_mode_df_dict[i] = result.sort_values('t').reset_index(drop=True)\n",
    "  return top_mode_df_dict\n",
    "\n",
    "def filter_columns(cols: List[str], include_patterns: List[str] = None, \n",
    "                  exclude_patterns: List[str] = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Filter a list of column names using include and exclude regex patterns.\n",
    "    \n",
    "    Args:\n",
    "        cols: List of column names to filter\n",
    "        include_patterns: List of regex patterns to include (if None, includes all)\n",
    "        exclude_patterns: List of regex patterns to exclude (if None, excludes none)\n",
    "    \n",
    "    Returns:\n",
    "        List of filtered column names\n",
    "    \n",
    "    Examples:\n",
    "        >>> cols = ['age_2020', 'age_2021', 'height_2020', 'weight_2021']\n",
    "        >>> filter_columns(cols, ['age_.*'], ['.*2021'])\n",
    "        ['age_2020']\n",
    "    \"\"\"\n",
    "    # Handle None inputs\n",
    "    include_patterns = include_patterns or ['.*']\n",
    "    exclude_patterns = exclude_patterns or []\n",
    "    \n",
    "    # First, get columns that match any include pattern\n",
    "    included_cols = set()\n",
    "    for pattern in include_patterns:\n",
    "        included_cols.update(\n",
    "            col for col in cols \n",
    "            if re.search(pattern, col)\n",
    "        )\n",
    "    \n",
    "    # Then remove any columns that match exclude patterns\n",
    "    for pattern in exclude_patterns:\n",
    "        included_cols = {\n",
    "            col for col in included_cols \n",
    "            if not re.search(pattern, col)\n",
    "        }\n",
    "    \n",
    "    return sorted(list(included_cols))\n",
    "\n",
    "def chain_filter_columns(cols: List[str], include_patterns: List[str] = None, \n",
    "                        exclude_patterns: List[str] = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Filter columns sequentially using chained include and exclude regex patterns.\n",
    "    Each pattern filters from the result of the previous pattern.\n",
    "    \n",
    "    Args:\n",
    "        cols: List of column names to filter\n",
    "        include_patterns: List of regex patterns to include (if None, includes all)\n",
    "        exclude_patterns: List of regex patterns to exclude (if None, excludes none)\n",
    "    \n",
    "    Returns:\n",
    "        List of filtered column names\n",
    "    \n",
    "    Examples:\n",
    "        >>> cols = ['age_2020_q1', 'age_2020_q2', 'age_2021_q1', 'height_2020_q1']\n",
    "        >>> chain_filter_columns(cols, ['age_.*', '.*q1'], ['.*2021.*'])\n",
    "        ['age_2020_q1']\n",
    "    \"\"\"\n",
    "    # Handle None inputs\n",
    "    include_patterns = include_patterns or ['.*']\n",
    "    exclude_patterns = exclude_patterns or []\n",
    "    \n",
    "    # Start with all columns\n",
    "    filtered_cols = set(cols)\n",
    "    \n",
    "    # Apply include patterns sequentially\n",
    "    for pattern in include_patterns:\n",
    "        filtered_cols = {\n",
    "            col for col in filtered_cols \n",
    "            if re.search(pattern, col)\n",
    "        }\n",
    "    \n",
    "    # Apply exclude patterns sequentially\n",
    "    for pattern in exclude_patterns:\n",
    "        filtered_cols = {\n",
    "            col for col in filtered_cols \n",
    "            if not re.search(pattern, col)\n",
    "        }\n",
    "    \n",
    "    return sorted(list(filtered_cols))\n",
    "\n",
    "def sort_by_coefs_numbers(col_list:List[str]):\n",
    "  with_coef_list = []\n",
    "  without_coef_list = []\n",
    "  for col in col_list:\n",
    "    if 'coef' not in col:\n",
    "      without_coef_list.append(col)\n",
    "    else:\n",
    "      with_coef_list.append(col)\n",
    "  return without_coef_list+sorted(with_coef_list, key=lambda x: int(x.split(\"_\")[-1][4:]))\n",
    "\n",
    "\n",
    "def load_power_diagonistics(PowDiag_path:Path):\n",
    "  pow_diag_dict = {}\n",
    "  for sd in find_subdomains(PowDiag_path):\n",
    "    pow_diag_dict[sd] = {}\n",
    "    sd_path = PowDiag_path/f\"{sd}.dir\"\n",
    "\n",
    "    psi_pd = make_mode_dataframe(sd_path/f\"Powerpsi.dir\")\n",
    "    kappa_pd = make_mode_dataframe(sd_path/f\"Powerkappa.dir\")\n",
    "    # For each subdomain save things by topology\n",
    "    for top in find_topologies(sd_path):\n",
    "      pow_diag_dict[sd][top]={}\n",
    "      psi_pd_sorted_cols = sort_by_coefs_numbers(psi_pd[top].columns.to_list())\n",
    "      pow_diag_dict[sd][top][f'psi_ps'] = psi_pd[top][psi_pd_sorted_cols]\n",
    "\n",
    "      kappa_pd_sorted_cols = sort_by_coefs_numbers(kappa_pd[top].columns.to_list())\n",
    "      pow_diag_dict[sd][top][f'kappa_ps'] = kappa_pd[top][kappa_pd_sorted_cols]\n",
    "\n",
    "      for dat_file in find_dat_file_names(sd_path):\n",
    "        pow_diag_dict[sd][top][f'{dat_file}'] = read_dat_file(sd_path/f\"{top}_{dat_file}.dat\")\n",
    "  \n",
    "  return pow_diag_dict\n",
    "\n",
    "\n",
    "def load_power_diagonistics_flat(PowDiag_path:Path, return_df:bool=True):\n",
    "  # Same as load_power_diagonistics but no nested dicts. This makes it easy to filter\n",
    "  pow_diag_dict = {}\n",
    "  for sd in find_subdomains(PowDiag_path):\n",
    "    sd_path = PowDiag_path/f\"{sd}.dir\"\n",
    "\n",
    "    psi_pd = make_mode_dataframe(sd_path/f\"Powerpsi.dir\")\n",
    "    kappa_pd = make_mode_dataframe(sd_path/f\"Powerkappa.dir\")\n",
    "    # For each subdomain save things by topology\n",
    "    for top in find_topologies(sd_path):\n",
    "      psi_pd_sorted_cols = sort_by_coefs_numbers(psi_pd[top].columns.to_list())\n",
    "      pow_diag_dict[f'{sd}_{top}_psi_ps'] = psi_pd[top][psi_pd_sorted_cols]\n",
    "\n",
    "      kappa_pd_sorted_cols = sort_by_coefs_numbers(kappa_pd[top].columns.to_list())\n",
    "      pow_diag_dict[f'{sd}_{top}_kappa_ps'] = kappa_pd[top][kappa_pd_sorted_cols]\n",
    "\n",
    "      for dat_file in find_dat_file_names(sd_path):\n",
    "        pow_diag_dict[f'{sd}_{top}_{dat_file}'] = read_dat_file(sd_path/f\"{top}_{dat_file}.dat\")\n",
    "  \n",
    "  if return_df:\n",
    "    # This can be definitely merged with the stuff above but it's fast enough anyways\n",
    "    flat_dict = {}\n",
    "    flat_dict['t'] = pow_diag_dict[rc(list(pow_diag_dict.keys()))]['t']\n",
    "    for key,item in pow_diag_dict.items():\n",
    "      for col in item.columns:\n",
    "        if 't' == col:\n",
    "          continue \n",
    "        else:\n",
    "          flat_dict[f\"{key}_{col}\"] = item[col]\n",
    "\n",
    "    flat_df = pd.DataFrame(flat_dict)\n",
    "    return flat_df\n",
    "\n",
    "  return pow_diag_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = Path(\"/groups/sxs/hchaudha/spec_runs/del/filtering/6_set1_L3_Lev3/extracted-PowerDiagnostics/SphereC0.dir/Powerpsi.dir/Bf0I1(19 modes).dat\")\n",
    "file = Path(\"/groups/sxs/hchaudha/spec_runs/del/filtering/16_set1_L3_Lev3/extracted-PowerDiagnostics/SphereC0.dir/Powerpsi.dir/Bf0I1(20 modes).dat\")\n",
    "# file = Path(\"/groups/sxs/hchaudha/spec_runs/del/filtering/16_set1_L3_Lev3/extracted-PowerDiagnostics/SphereC0.dir/Bf0I1_HighestThirdConvergenceFactor.dat\")\n",
    "# file = Path(\"/groups/sxs/hchaudha/spec_runs/del/filtering/6_set1_L3_Lev3/extracted-PowerDiagnostics/SphereC0.dir/Bf0I1_TruncationError.dat\")\n",
    "# file = Path(\"/groups/sxs/hchaudha/spec_runs/del/filtering/13_set1_L4_1500_Lev4/extracted-PowerDiagnostics/SphereC10.dir/Powerpsi.dir/Bf1S2(15 modes).dat\")\n",
    "file = Path(\"/groups/sxs/hchaudha/spec_runs/del/filtering/16_set1_L3_HP32_AF_Lev3/extracted-PowerDiagnostics/SphereC0.dir/Powerkappa.dir/Bf0I1(15 modes).dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = read_dat_file(file)\n",
    "h5_path = Path('/groups/sxs/hchaudha/spec_runs/del/filtering/13_set1_L4_1500_Lev4')\n",
    "# h5_path = Path('/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L3/h5_files_Lev3')\n",
    "h5_path = Path('/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/h5_files_Lev6/')\n",
    "h5_path = Path('/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/h5_files_Lev5')\n",
    "h5_path = Path('/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/h5_files_Lev5')\n",
    "# h5_path = Path('/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/h5_files_Lev5/')\n",
    "# h5_path = Path('/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/h5_files_Lev4/')\n",
    "# h5_path = Path('/groups/sxs/hchaudha/spec_runs/16_set1_L3/h5_files_Lev3/')\n",
    "# h5_path = Path('/groups/sxs/hchaudha/spec_runs/16_set1_L3_HP28/h5_files_Lev3')\n",
    "# h5_path = Path('/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/h5_files_Lev6/')\n",
    "# h5_path = Path('/groups/sxs/hchaudha/spec_runs/17_set_main_q3_18_L3/h5_files_Lev3')\n",
    "# h5_path = Path('/groups/sxs/hchaudha/spec_runs/17_set1_q3_18_L3/h5_files_Lev3')\n",
    "# h5_path = Path('/groups/sxs/hchaudha/spec_runs/17_set3_q3_18_L3/h5_files_Lev3')\n",
    "\n",
    "domain = 'FilledCylinderCB0'\n",
    "domain = 'SphereC5'\n",
    "domain = 'SphereA0'\n",
    "\n",
    "psi_or_kappa = 'kappa'\n",
    "# psi_or_kappa = 'psi'\n",
    "\n",
    "folder_path = Path(f\"{h5_path}/extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir\")\n",
    "top_data = make_mode_dataframe(folder_path)\n",
    "print(top_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_name = 'Bf1S2'\n",
    "top_name = 'Bf0I1'\n",
    "# top_name = list(top_data.keys())[0]\n",
    "t_min = 0\n",
    "t_max = 4000\n",
    "data = top_data[top_name]\n",
    "data = limit_by_col_val(t_min,t_max,'t',data)\n",
    "data = data.dropna(axis=1, how='all')  # Some columns will have just nans remove those\n",
    "column_names = data.columns[1:]\n",
    "visual_data = data[column_names]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "imshow_plot = plt.imshow(\n",
    "    # visual_data, \n",
    "    np.log10(visual_data), \n",
    "    aspect='auto', \n",
    "    cmap='RdYlGn_r', \n",
    "    origin='lower',interpolation='none',\n",
    ")\n",
    "\n",
    "plt.xticks(\n",
    "    ticks=np.arange(len(visual_data.columns)), \n",
    "    labels=[i.split(\" \")[-1] for i in column_names], \n",
    "    rotation=90\n",
    ")\n",
    "\n",
    "ytick_step = 1\n",
    "ytick_step = len(visual_data) // 10  # Show about 10 ticks\n",
    "plt.yticks(\n",
    "    ticks=np.arange(0, len(visual_data), ytick_step), \n",
    "    labels=data['t'][::ytick_step].astype(int)\n",
    ")\n",
    "plt.colorbar(imshow_plot)\n",
    "plt.ylabel('t(M)')\n",
    "plt.title(f'{str(folder_path)[31:]}_{top_name}')\n",
    "plt.tight_layout() \n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_use = [i for i in data.columns if 't' not in i]\n",
    "df = np.log10(data[cols_to_use])\n",
    "df['row_min'] = df.min(axis=1)\n",
    "df['row_max'] = df.max(axis=1)\n",
    "df['row_mean'] = df.mean(axis=1)\n",
    "df['row_std'] = df.std(axis=1)\n",
    "\n",
    "# plt.plot(df['row_min'])\n",
    "# plt.plot(df['row_mean'])\n",
    "# plt.plot(df['row_max'])\n",
    "# plt.plot(df['row_std'])\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i in cols_to_use:\n",
    "  # plt.plot(data['t'], df[f'{i}'])\n",
    "  plt.plot(data['t'], df[f'{i}'], label = f'{i}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.title(f'{str(folder_path)[31:]}_{top_name}')\n",
    "plt.tight_layout() \n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "# x = data['t']\n",
    "# y = df['row_mean']\n",
    "# y_err = df['row_std']\n",
    "# plt.errorbar(x, y, yerr=y_err, fmt='-o', label='Data with Error Bars', ecolor='red', capsize=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load all of power diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PowDiag_path = Path(\"/groups/sxs/hchaudha/spec_runs/16_set1_L3/h5_files_Lev3/extracted-PowerDiagnostics\")\n",
    "flat_df = load_power_diagonistics_flat(PowDiag_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_list = [ r'SphereA0' ,r'Bf0I1', r'psi_ps', r'coef']\n",
    "exclude_list = [ ]\n",
    "# Example usage with a DataFrame\n",
    "filtered_cols = chain_filter_columns(\n",
    "    cols=flat_df.columns.tolist(),\n",
    "    include_patterns=include_list,\n",
    "    exclude_patterns=exclude_list\n",
    ")\n",
    "\n",
    "# You can then use these columns to filter your DataFrame\n",
    "filtered_cols = sort_by_coefs_numbers(filtered_cols)\n",
    "filtered_df = flat_df[filtered_cols]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_dat_file_names(Path(\"/groups/sxs/hchaudha/spec_runs/16_set1_L3/h5_files_Lev3/extracted-PowerDiagnostics/SphereC1.dir\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_list = [ r'SphereA0' ,r'Bf0I1', 'Number']\n",
    "exclude_list = [  r'coef']\n",
    "# Example usage with a DataFrame\n",
    "filtered_cols = chain_filter_columns(\n",
    "    cols=flat_df.columns.tolist(),\n",
    "    include_patterns=include_list,\n",
    "    exclude_patterns=exclude_list\n",
    ")\n",
    "\n",
    "# You can then use these columns to filter your DataFrame\n",
    "filtered_cols = ['t']+sort_by_coefs_numbers(filtered_cols)\n",
    "filtered_df = flat_df[filtered_cols]\n",
    "filtered_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for col in filtered_df.columns:\n",
    "  if 't' == col:\n",
    "    continue\n",
    "  plt.plot(filtered_df['t'], filtered_df[col],label=col)\n",
    "  # plt.plot(filtered_df['t'], np.log10(np.abs(filtered_df[col])),label=col)\n",
    "plt.legend()\n",
    "plt.xlabel('t')\n",
    "# plt.ylabel('Convergence Factor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PowDiag_path = Path(\"/groups/sxs/hchaudha/spec_runs/16_set1_L3/h5_files_Lev3/extracted-PowerDiagnostics\")\n",
    "dict_df = load_power_diagonistics(PowDiag_path)\n",
    "dict_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter subdomains\n",
    "# include_list = [ r'SphereA[2-9]']\n",
    "include_list = [ ]\n",
    "exclude_list = [  r'Cylinder',r'SphereC']\n",
    "# Example usage with a DataFrame\n",
    "filtered_cols = chain_filter_columns(\n",
    "    cols=dict_df.keys(),\n",
    "    include_patterns=include_list,\n",
    "    exclude_patterns=exclude_list\n",
    ")\n",
    "filtered_dict = {key:dict_df[key] for key in filtered_cols}\n",
    "top_set = set()\n",
    "for k,i in filtered_dict.items():\n",
    "  top_set.update(i.keys())\n",
    "print(filtered_dict.keys())\n",
    "print(top_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCOMPLETE!!!!!\n",
    "# filter subdomains\n",
    "include_list = [ 'Bf0I1']\n",
    "exclude_list = [ ]\n",
    "# Example usage with a DataFrame\n",
    "filtered_cols = chain_filter_columns(\n",
    "    cols=dict_df.keys(),\n",
    "    include_patterns=include_list,\n",
    "    exclude_patterns=exclude_list\n",
    ")\n",
    "filtered_dict = {key:dict_df[key] for key in filtered_cols}\n",
    "filtered_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp damping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_damping(p,a,N):\n",
    "  return np.exp(-a*(np.arange(N)/(N-1))**(2*p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-exp_damping(36,36,15), 1-exp_damping(30,36,15), 1-exp_damping(28,36,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(1-exp_damping(28,36,15),label='28,36', marker='o')\n",
    "plt.plot(1-exp_damping(30,36,15),label='30,36', marker='o')\n",
    "plt.plot(1-exp_damping(32,36,15),label='32,36', marker='o')\n",
    "plt.plot(1-exp_damping(36,36,15),label='36,36', marker='o')\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sxs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
