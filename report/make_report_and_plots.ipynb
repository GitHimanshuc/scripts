{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "import itertools\n",
    "\n",
    "import h5py\n",
    "import imageio.v3 as iio\n",
    "import matplotlib\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import cycler\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.patches import Circle, Polygon\n",
    "from numba import njit\n",
    "from scipy.interpolate import CubicSpline\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "import scipy as sp\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 10)\n",
    "spec_home = \"/home/himanshu/spec/my_spec\"\n",
    "matplotlib.matplotlib_fname()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Various functions to read across levs\n",
    "### Also functions to make reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### domain color "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_regex(regex,col_list,exclude=False):\n",
    "  filtered_set = set()\n",
    "  if type(regex) is list:\n",
    "    for reg in regex:\n",
    "      for i in col_list:\n",
    "        if re.search(reg,i):\n",
    "          filtered_set.add(i)\n",
    "  else:\n",
    "    for i in col_list:\n",
    "      if re.search(regex,i):\n",
    "        filtered_set.add(i)\n",
    "\n",
    "  filtered_list = list(filtered_set)\n",
    "  if exclude:\n",
    "    col_list_copy = list(col_list.copy())\n",
    "    for i in filtered_list:\n",
    "      if i in col_list_copy:\n",
    "        col_list_copy.remove(i)\n",
    "    filtered_list = col_list_copy\n",
    "\n",
    "  # Restore the original order\n",
    "  filtered_original_ordered_list = []\n",
    "  for i in list(col_list):\n",
    "    if i in filtered_list:\n",
    "      filtered_original_ordered_list.append(i)\n",
    "  return filtered_original_ordered_list\n",
    "\n",
    "def limit_by_col_val(min_val,max_val,col_name,df):\n",
    "  filter = (df[col_name]>=min_val) &(df[col_name] <=max_val)\n",
    "  return df[filter]\n",
    "\n",
    "def get_domain_name(col_name):\n",
    "  def AMR_domains_to_decimal(subdoamin_name):\n",
    "    # SphereC28.0.1\n",
    "    a = subdoamin_name.split(\".\")\n",
    "    # a = [SphereC28,0,1]\n",
    "    decimal_rep = a[0]+\".\"\n",
    "    # decimal_rep = SphereC28.\n",
    "    for i in a[1:]:\n",
    "      decimal_rep = decimal_rep + i\n",
    "    # decimal_rep = SphereC28.01\n",
    "    return decimal_rep\n",
    "\n",
    "  if \"on\" in col_name:\n",
    "    return AMR_domains_to_decimal(col_name.split(\" \")[-1])\n",
    "  elif \"_\" in col_name:\n",
    "    return col_name.split(\"_\")[0]\n",
    "  elif \"MinimumGridSpacing\" in col_name:\n",
    "    return col_name.split(\"[\")[-1][:-1]\n",
    "  else:\n",
    "    raise Exception(f\"{col_name} type not implemented in return_sorted_domain_names\")\n",
    "\n",
    "def return_sorted_domain_names(domain_names, repeated_symmetric=False, num_Excision=2):\n",
    "\n",
    "  # def filtered_domain_names(domain_names, filter):\n",
    "  #   return [i for i in domain_names if get_domain_name(i).startswith(filter)]\n",
    "\n",
    "  def filtered_domain_names(domain_names, filter):\n",
    "    return [i for i in domain_names if re.match(filter, get_domain_name(i))]\n",
    "\n",
    "  def sort_spheres(sphere_list,reverse=False):\n",
    "    if len(sphere_list) == 0:\n",
    "      return []\n",
    "    if \"SphereA\" in sphere_list[0]:\n",
    "      return sorted(sphere_list, key=lambda x: float(get_domain_name(x).lstrip('SphereA')), reverse=reverse)\n",
    "    elif \"SphereB\" in sphere_list[0]:\n",
    "      return sorted(sphere_list, key=lambda x: float(get_domain_name(x).lstrip('SphereB')), reverse=reverse)\n",
    "    elif \"SphereC\" in sphere_list[0]:\n",
    "      return sorted(sphere_list, key=lambda x: float(get_domain_name(x).lstrip('SphereC')), reverse=reverse)\n",
    "    elif \"SphereD\" in sphere_list[0]:\n",
    "      return sorted(sphere_list, key=lambda x: float(get_domain_name(x).lstrip('SphereD')), reverse=reverse)\n",
    "    elif \"SphereE\" in sphere_list[0]:\n",
    "      return sorted(sphere_list, key=lambda x: float(get_domain_name(x).lstrip('SphereE')), reverse=reverse)\n",
    "\n",
    "  FilledCylinderCA = filtered_domain_names(domain_names, r'FilledCylinder.{0,2}CA')\n",
    "  CylinderCA = filtered_domain_names(domain_names, r'Cylinder.{0,2}CA')\n",
    "  FilledCylinderEA = filtered_domain_names(domain_names, r'FilledCylinder.{0,2}EA')\n",
    "  CylinderEA = filtered_domain_names(domain_names, r'Cylinder.{0,2}EA')\n",
    "  SphereA = sort_spheres(filtered_domain_names(domain_names, 'SphereA'), reverse=True)\n",
    "  CylinderSMA = filtered_domain_names(domain_names, r'CylinderS.{0,2}MA')\n",
    "  FilledCylinderMA = filtered_domain_names(domain_names, r'FilledCylinder.{0,2}MA')\n",
    "\n",
    "  FilledCylinderMB = filtered_domain_names(domain_names, r'FilledCylinder.{0,2}MB')\n",
    "  CylinderSMB = filtered_domain_names(domain_names, r'CylinderS.{0,2}MB')\n",
    "  SphereB = sort_spheres(filtered_domain_names(domain_names, 'SphereB'), reverse=True)\n",
    "  CylinderEB = filtered_domain_names(domain_names, r'Cylinder.{0,2}EB')\n",
    "  FilledCylinderEB = filtered_domain_names(domain_names, r'FilledCylinder.{0,2}EB')\n",
    "  CylinderCB = filtered_domain_names(domain_names, r'Cylinder.{0,2}CB')\n",
    "  FilledCylinderCB = filtered_domain_names(domain_names, r'FilledCylinder.{0,2}CB')\n",
    "\n",
    "  SphereC = sort_spheres(filtered_domain_names(domain_names, 'SphereC'), reverse=False)\n",
    "  SphereD = sort_spheres(filtered_domain_names(domain_names, 'SphereD'), reverse=False)\n",
    "  SphereE = sort_spheres(filtered_domain_names(domain_names, 'SphereE'), reverse=False)\n",
    "  \n",
    "  NAN_cols = ['Excision']*num_Excision\n",
    "  combined_columns = [FilledCylinderCA, CylinderCA, FilledCylinderEA, CylinderEA, SphereA, CylinderSMA, FilledCylinderMA, FilledCylinderMB, CylinderSMB, SphereB, CylinderEB, FilledCylinderEB, CylinderCB, FilledCylinderCB, SphereC, SphereD, SphereE]\n",
    "  if repeated_symmetric:\n",
    "    combined_columns = [SphereE[::-1], SphereD[::-1], SphereC[::-1],FilledCylinderCA[::-1], CylinderCA[::-1], FilledCylinderEA[::-1], CylinderEA[::-1], SphereA, NAN_cols, SphereA[::-1], CylinderSMA[::-1], FilledCylinderMA[::-1], FilledCylinderMB, CylinderSMB, SphereB,NAN_cols, SphereB[::-1], CylinderEB, FilledCylinderEB, CylinderCB, FilledCylinderCB, SphereC, SphereD, SphereE]\n",
    "  combined_columns = [item for sublist in combined_columns for item in sublist]\n",
    "\n",
    "  # Just append the domains not following any patterns in the front. Mostly domains surrounding sphereA for high spin and mass ratios\n",
    "  combined_columns_set = set(combined_columns)\n",
    "  domain_names_set = set()\n",
    "  for i in domain_names:\n",
    "    domain_names_set.add(i)\n",
    "  subdomains_not_sorted = list(domain_names_set - combined_columns_set)\n",
    "  return subdomains_not_sorted+combined_columns\n",
    "\n",
    "class BBH_domain_sym_ploy:\n",
    "  def __init__(self, center_xA, rA,RA,rC,RC,nA,nC,color_dict:dict=None):\n",
    "    self.center_xA = center_xA\n",
    "    self.color_dict = color_dict\n",
    "    self.rA = rA # Largest SphereA radius\n",
    "    self.RA = RA # Radius of FilledCylinderE\n",
    "    self.rC = rC # Smallest SphereC radius\n",
    "    self.RC = RC # Radius of the largest SphereC\n",
    "\n",
    "    self.nA = nA # Number of SphereA\n",
    "    self.nC = nC # Number of SphereC\n",
    "\n",
    "    self.alpha_for_FilledCylinderE_from_Center_bh = np.radians(50)\n",
    "    self.outer_angle_for_CylinderSM_from_Center_bh = np.arccos(self.center_xA/self.RA)\n",
    "    self.inner_angle_for_CylinderSM_from_Center_bh = self.outer_angle_for_CylinderSM_from_Center_bh/3\n",
    "\n",
    "    self.patches = []\n",
    "\n",
    "    self.add_shpereCs()\n",
    "\n",
    "    self.add_CylinderC(which_bh='A')\n",
    "    self.add_FilledCylinderE(which_bh='A')\n",
    "    self.add_CylinderE(which_bh='A')\n",
    "    self.add_CylinderSM(which_bh='A')\n",
    "    self.add_FilledCylinderM(which_bh='A')\n",
    "    self.add_FilledCylinderC(which_bh='A')\n",
    "\n",
    "    self.add_CylinderC(which_bh='B')\n",
    "    self.add_FilledCylinderE(which_bh='B')\n",
    "    self.add_CylinderE(which_bh='B')\n",
    "    self.add_CylinderSM(which_bh='B')\n",
    "    self.add_FilledCylinderM(which_bh='B')\n",
    "    self.add_FilledCylinderC(which_bh='B')\n",
    "\n",
    "    self.add_inner_shperes(which_bh='A')\n",
    "    self.add_inner_shperes(which_bh='B')\n",
    "\n",
    "    # print the unmatched domains\n",
    "    print(self.color_dict)\n",
    "\n",
    "  def get_matching_color(self, domain_name:str):\n",
    "    if self.color_dict is None:\n",
    "      return np.random.rand(3,)\n",
    "    for key in self.color_dict.keys():\n",
    "      if domain_name in key:\n",
    "        # Remove the domain name from the key, this will allow us to see which domains were not matched\n",
    "        return self.color_dict.pop(key)\n",
    "    # No match found\n",
    "    return 'pink'\n",
    "\n",
    "  def add_inner_shperes(self,which_bh):\n",
    "    center = self.center_xA\n",
    "    if which_bh == 'B':\n",
    "      center = -self.center_xA\n",
    "  \n",
    "    spheres_outer_radii = np.linspace(self.rA, 0, self.nA+2)\n",
    "    i=nA-1\n",
    "    for r in spheres_outer_radii[:-2]:\n",
    "      domain_name = f'Sphere{which_bh}{i}'\n",
    "      i = i-1\n",
    "      color = self.get_matching_color(domain_name)\n",
    "      self.patches.append(Circle((center, 0), r, facecolor=color, edgecolor='black'))\n",
    "\n",
    "    domain_name = f'Sphere{which_bh}{i}'\n",
    "    i = i-1\n",
    "    color = self.get_matching_color(domain_name)\n",
    "    self.patches.append(Circle((center, 0), spheres_outer_radii[-2], facecolor='black', edgecolor='black'))\n",
    "\n",
    "  def add_shpereCs(self):\n",
    "    spheres_outer_radii = np.linspace(self.RC, self.rC, self.nC+1)[:-1]\n",
    "    i=nC-1\n",
    "    for r in spheres_outer_radii:\n",
    "      domain_name = f'SphereC{i}'\n",
    "      i = i-1\n",
    "      color = self.get_matching_color(domain_name)\n",
    "      self.patches.append(Circle((0, 0), r, facecolor=color, edgecolor='black'))\n",
    "    \n",
    "  def add_FilledCylinderE(self,which_bh):\n",
    "    alpha = self.alpha_for_FilledCylinderE_from_Center_bh\n",
    "    \n",
    "    x_inner = self.center_xA+self.rA*np.cos(alpha)\n",
    "    y_inner = self.rA*np.sin(alpha)\n",
    "    x_outer = self.center_xA+self.RA*np.cos(alpha)\n",
    "    y_outer = self.RA*np.sin(alpha)\n",
    "\n",
    "    if which_bh == 'B':\n",
    "      x_inner = -x_inner\n",
    "      x_outer = -x_outer\n",
    "    vertices=[\n",
    "      (x_inner,y_inner),\n",
    "      (x_outer,y_outer),\n",
    "      (x_outer,-y_outer),\n",
    "      (x_inner,-y_inner),\n",
    "    ]\n",
    "    color = self.get_matching_color(f'FilledCylinderE{which_bh}')\n",
    "    self.patches.append(Polygon(vertices, closed=True, facecolor=color, edgecolor='black'))\n",
    "\n",
    "  def add_CylinderE(self,which_bh):\n",
    "    alpha = self.alpha_for_FilledCylinderE_from_Center_bh\n",
    "    beta = self.outer_angle_for_CylinderSM_from_Center_bh\n",
    "\n",
    "    x_inner_away_from_center = self.center_xA+self.rA*np.cos(alpha)\n",
    "    y_inner_away_from_center = self.rA*np.sin(alpha)\n",
    "    x_outer_away_from_center = self.center_xA+self.RA*np.cos(alpha)\n",
    "    y_outer_away_from_center = self.RA*np.sin(alpha)\n",
    "\n",
    "    x_inner_closer_to_center = self.center_xA-self.rA*np.cos(beta)\n",
    "    y_inner_closer_to_center = self.rA*np.sin(beta)\n",
    "    x_outer_closer_to_center = 0\n",
    "    y_outer_closer_to_center = self.RA*np.sin(beta)\n",
    "\n",
    "    if which_bh == 'B':\n",
    "      x_inner_away_from_center = -x_inner_away_from_center\n",
    "      x_outer_away_from_center = -x_outer_away_from_center\n",
    "      x_inner_closer_to_center = -x_inner_closer_to_center\n",
    "      x_outer_closer_to_center = -x_outer_closer_to_center\n",
    "\n",
    "    vertices=[\n",
    "      (x_inner_away_from_center,y_inner_away_from_center),\n",
    "      (x_outer_away_from_center,y_outer_away_from_center),\n",
    "      (x_outer_closer_to_center,y_outer_closer_to_center),\n",
    "      (x_inner_closer_to_center,y_inner_closer_to_center),\n",
    "      (x_inner_closer_to_center,-y_inner_closer_to_center),\n",
    "      (x_outer_closer_to_center,-y_outer_closer_to_center),\n",
    "      (x_outer_away_from_center,-y_outer_away_from_center),\n",
    "      (x_inner_away_from_center,-y_inner_away_from_center),\n",
    "    ]\n",
    "    color = self.get_matching_color(f'CylinderE{which_bh}')\n",
    "    self.patches.append(Polygon(vertices, closed=True, facecolor=color, edgecolor='black'))\n",
    "\n",
    "  def add_CylinderC(self,which_bh):\n",
    "    alpha = self.alpha_for_FilledCylinderE_from_Center_bh\n",
    "    beta = self.outer_angle_for_CylinderSM_from_Center_bh\n",
    "\n",
    "    x_inner_away_from_center = self.center_xA+self.rA*np.cos(alpha)\n",
    "    y_inner_away_from_center = self.rA*np.sin(alpha)\n",
    "    x_outer_away_from_center = self.rC*np.cos(np.radians(30))\n",
    "    y_outer_away_from_center = self.rC*np.sin(np.radians(30))\n",
    "\n",
    "    x_inner_closer_to_center = 0\n",
    "    y_inner_closer_to_center = self.RA*np.sin(beta)\n",
    "    x_outer_closer_to_center = 0\n",
    "    y_outer_closer_to_center = self.rC\n",
    "\n",
    "    if which_bh == 'B':\n",
    "      x_inner_away_from_center = -x_inner_away_from_center\n",
    "      x_outer_away_from_center = -x_outer_away_from_center\n",
    "      x_inner_closer_to_center = -x_inner_closer_to_center\n",
    "      x_outer_closer_to_center = -x_outer_closer_to_center\n",
    "\n",
    "    vertices=[\n",
    "      (x_inner_closer_to_center,y_inner_closer_to_center),\n",
    "      (x_outer_closer_to_center,y_outer_closer_to_center),\n",
    "      (x_outer_away_from_center,y_outer_away_from_center),\n",
    "      (x_inner_away_from_center,y_inner_away_from_center),\n",
    "      (x_inner_away_from_center,-y_inner_away_from_center),\n",
    "      (x_outer_away_from_center,-y_outer_away_from_center),\n",
    "      (x_outer_closer_to_center,-y_outer_closer_to_center),\n",
    "      (x_inner_closer_to_center,-y_inner_closer_to_center),\n",
    "    ]\n",
    "    color = self.get_matching_color(f'CylinderC{which_bh}')\n",
    "    self.patches.append(Polygon(vertices, closed=True, facecolor=color, edgecolor='black'))\n",
    "\n",
    "  def add_CylinderSM(self,which_bh):\n",
    "    beta = self.outer_angle_for_CylinderSM_from_Center_bh\n",
    "    gamma = self.inner_angle_for_CylinderSM_from_Center_bh\n",
    "\n",
    "    x_inner_away_from_center = self.center_xA-self.rA*np.cos(beta)\n",
    "    y_inner_away_from_center = self.rA*np.sin(beta)\n",
    "    x_outer_away_from_center = 0\n",
    "    y_outer_away_from_center = self.RA*np.sin(beta)\n",
    "\n",
    "    x_inner_closer_to_center = self.center_xA-self.rA*np.cos(gamma)\n",
    "    y_inner_closer_to_center = self.rA*np.sin(gamma)\n",
    "    x_outer_closer_to_center = 0\n",
    "    y_outer_closer_to_center = self.RA*np.sin(gamma)\n",
    "\n",
    "    if which_bh == 'B':\n",
    "      x_inner_away_from_center = -x_inner_away_from_center\n",
    "      x_outer_away_from_center = -x_outer_away_from_center\n",
    "      x_inner_closer_to_center = -x_inner_closer_to_center\n",
    "      x_outer_closer_to_center = -x_outer_closer_to_center\n",
    "\n",
    "    vertices=[\n",
    "      (x_inner_away_from_center,y_inner_away_from_center),\n",
    "      (x_outer_away_from_center,y_outer_away_from_center),\n",
    "      (x_outer_closer_to_center,y_outer_closer_to_center),\n",
    "      (x_inner_closer_to_center,y_inner_closer_to_center),\n",
    "      (x_inner_closer_to_center,-y_inner_closer_to_center),\n",
    "      (x_outer_closer_to_center,-y_outer_closer_to_center),\n",
    "      (x_outer_away_from_center,-y_outer_away_from_center),\n",
    "      (x_inner_away_from_center,-y_inner_away_from_center),\n",
    "    ]\n",
    "    color = self.get_matching_color(f'CylinderSM{which_bh}')\n",
    "    self.patches.append(Polygon(vertices, closed=True, facecolor=color, edgecolor='black'))\n",
    "    \n",
    "  def add_FilledCylinderM(self,which_bh):\n",
    "    gamma = self.inner_angle_for_CylinderSM_from_Center_bh\n",
    "\n",
    "    x_inner = self.center_xA-self.rA*np.cos(gamma)\n",
    "    y_inner = self.rA*np.sin(gamma)\n",
    "    x_outer = 0\n",
    "    y_outer = self.RA*np.sin(gamma)\n",
    "\n",
    "    if which_bh == 'B':\n",
    "      x_inner = -x_inner\n",
    "      x_outer = -x_outer\n",
    "    vertices=[\n",
    "      (x_inner,y_inner),\n",
    "      (x_outer,y_outer),\n",
    "      (x_outer,-y_outer),\n",
    "      (x_inner,-y_inner),\n",
    "    ]\n",
    "    color = self.get_matching_color(f'FilledCylinderM{which_bh}')\n",
    "    self.patches.append(Polygon(vertices, closed=True, facecolor=color, edgecolor='black'))\n",
    "\n",
    "  def add_FilledCylinderC(self,which_bh):\n",
    "    alpha = self.alpha_for_FilledCylinderE_from_Center_bh\n",
    "\n",
    "    x_inner = self.center_xA+self.RA*np.cos(alpha)\n",
    "    y_inner = self.RA*np.sin(alpha)\n",
    "    x_outer = self.rC*np.cos(np.radians(30))\n",
    "    y_outer = self.rC*np.sin(np.radians(30))\n",
    "\n",
    "    if which_bh == 'B':\n",
    "      x_inner = -x_inner\n",
    "      x_outer = -x_outer\n",
    "    vertices=[\n",
    "      (x_inner,y_inner),\n",
    "      (x_outer,y_outer),\n",
    "      (x_outer,-y_outer),\n",
    "      (x_inner,-y_inner),\n",
    "    ]\n",
    "    color = self.get_matching_color(f'FilledCylinderC{which_bh}')\n",
    "    self.patches.append(Polygon(vertices, closed=True, facecolor=color, edgecolor='black'))\n",
    "\n",
    "def scalar_to_color(scalar_dict,min_max_tuple=None,color_map=\"viridis\"):\n",
    "  arr_keys,arr_vals = [], []\n",
    "  for key,val in scalar_dict.items():\n",
    "    if np.isnan(val):\n",
    "      continue\n",
    "    else:\n",
    "      arr_keys.append(key)\n",
    "      arr_vals.append(val)\n",
    "\n",
    "  scalar_array = np.array(arr_vals, dtype=np.float64) \n",
    "  scalar_array = np.log10(scalar_array)\n",
    "  min_val = np.min(scalar_array)\n",
    "  max_val = np.max(scalar_array)\n",
    "  print(min_val,max_val)\n",
    "  if min_max_tuple is not None:\n",
    "    min_val, max_val = min_max_tuple\n",
    "  scalar_normalized = (scalar_array - min_val) / (max_val - min_val)\n",
    "\n",
    "  colormap = plt.get_cmap(color_map)\n",
    "  colors = {}\n",
    "  for key,value in zip(arr_keys,scalar_normalized):\n",
    "    colors[key] = colormap(value)\n",
    "\n",
    "  # Get colorbar\n",
    "  norm = Normalize(vmin=min_val, vmax=max_val)\n",
    "\n",
    "  sm = plt.cm.ScalarMappable(cmap=colormap, norm=norm)\n",
    "  sm.set_array([])\n",
    "\n",
    "  return colors,sm\n",
    "\n",
    "# nA=4\n",
    "# rA=nA*1.5\n",
    "# center_xA=rA + 2\n",
    "# RA=rA+5\n",
    "# rC=RA*2\n",
    "# nC=30\n",
    "# RC=rC+nC\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# domain_color_local = domain_color.copy()\n",
    "# patches_class = BBH_domain_sym_ploy(center_xA=center_xA, rA=rA, RA=RA, rC=rC, RC=RC, nA=nA, nC=nC, color_dict=domain_color_local) \n",
    "# for patch in patches_class.patches:\n",
    "#   ax.add_patch(patch)\n",
    "\n",
    "# ax.set_xlim(-RC, RC)\n",
    "# ax.set_ylim(-RC, RC)\n",
    "# ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to read h5 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to read horizon files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Bh_pandas(h5_dir):\n",
    "    # Empty dataframe\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # List of all the vars in the h5 file\n",
    "    var_list = []\n",
    "    h5_dir.visit(var_list.append)\n",
    "    \n",
    "    \n",
    "    for var in var_list:\n",
    "        # This means there is no time column\n",
    "        # print(f\"{var} : {h5_dir[var].shape}\")\n",
    "        if df.shape == (0,0):\n",
    "            # data[:,0] is time and then we have the data\n",
    "            data = h5_dir[var]\n",
    "            \n",
    "            # vars[:-4] to remove the .dat at the end\n",
    "            col_names = make_col_names(var[:-4],data.shape[1]-1)\n",
    "            col_names.append('t')\n",
    "            # Reverse the list so that we get [\"t\",\"var_name\"]\n",
    "            col_names.reverse()            \n",
    "            append_to_df(data[:],col_names,df)\n",
    "            \n",
    "        else:\n",
    "            data = h5_dir[var]\n",
    "            col_names = make_col_names(var[:-4],data.shape[1]-1)         \n",
    "            append_to_df(data[:,1:],col_names,df)\n",
    "            \n",
    "    return df\n",
    "\n",
    "def append_to_df(data,col_names,df):\n",
    "    for i,col_name in enumerate(col_names):\n",
    "        df[col_name] = data[:,i]\n",
    "        \n",
    "def make_col_names(val_name:str,val_size:int):\n",
    "    col_names = []\n",
    "    if val_size == 1:\n",
    "        col_names.append(val_name)\n",
    "    else:\n",
    "        for i in range(val_size):\n",
    "            col_names.append(val_name+f\"_{i}\")\n",
    "    return col_names\n",
    "\n",
    "\n",
    "def horizon_to_pandas(horizon_path:Path):\n",
    "    assert(horizon_path.exists())\n",
    "    df_dict = {}\n",
    "    with h5py.File(horizon_path,'r') as hf:\n",
    "        # Not all horizon files may have AhC\n",
    "        for key in hf.keys():\n",
    "            if key == 'VersionHist.ver':\n",
    "                # Newer runs have this\n",
    "                continue\n",
    "            df_dict[key[:-4]] = make_Bh_pandas(hf[key])\n",
    "\n",
    "    return df_dict\n",
    "\n",
    "def read_horizon_across_Levs(path_list:List[Path]):\n",
    "    df_listAB = []\n",
    "    df_listC = []\n",
    "    final_dict = {}\n",
    "    for path in path_list:\n",
    "        df_lev = horizon_to_pandas(path)\n",
    "        # Either [AhA,AhB] or [AhA,AhB,AhC]\n",
    "        if len(df_lev.keys()) > 1:\n",
    "            df_listAB.append(df_lev)\n",
    "        # Either [AhC] or [AhA,AhB,AhC]\n",
    "        if (len(df_lev.keys()) == 1) or (len(df_lev.keys()) ==3):\n",
    "            df_listC.append(df_lev)\n",
    "    if len(df_listAB)==1:\n",
    "        # There was only one lev\n",
    "        final_dict = df_listAB[0]\n",
    "    else:\n",
    "        final_dict[\"AhA\"] = pd.concat([df[\"AhA\"] for df in df_listAB])\n",
    "        final_dict[\"AhB\"] = pd.concat([df[\"AhB\"] for df in df_listAB])\n",
    "        if len(df_listC) > 0:\n",
    "            final_dict[\"AhC\"] = pd.concat([df[\"AhC\"] for df in df_listC])\n",
    "    \n",
    "    return final_dict\n",
    "\n",
    "def load_horizon_data_from_levs(base_path:Path, runs_path:Dict[str,Path]):\n",
    "  data_dict = {}\n",
    "  for run_name in runs_path.keys():\n",
    "    path_list = list(base_path.glob(runs_path[run_name]))\n",
    "    print(path_list)\n",
    "    data_dict[run_name] = read_horizon_across_Levs(path_list)\n",
    "  return data_dict\n",
    "\n",
    "def flatten_dict(horizon_data_dict:Dict[str,pd.DataFrame]) -> Dict[str,pd.DataFrame] :\n",
    "  flattened_data = {}\n",
    "  for run_name in horizon_data_dict.keys():\n",
    "      for horizons in horizon_data_dict[run_name]:\n",
    "          flattened_data[run_name+\"_\"+horizons] = horizon_data_dict[run_name][horizons]\n",
    "          # print(run_name+\"_\"+horizons)\n",
    "  return flattened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_profiler(file_name):\n",
    "  with h5py.File(file_name,'r') as f:\n",
    "    steps = set()\n",
    "    procs = set()\n",
    "    names = []\n",
    "    f.visit(names.append)\n",
    "    for name in names:\n",
    "      step = name.split('.')[0][4:]\n",
    "      steps.add(step)\n",
    "      if 'Proc' in name:\n",
    "        procs.add(name.split('/')[-1][4:-4])\n",
    "\n",
    "    dict_list = []\n",
    "    for step in steps:\n",
    "      for proc in procs:\n",
    "        data = f[f'Step{step}.dir/Proc{proc}.txt'][0].decode()\n",
    "\n",
    "        lines = data.split(\"\\n\")\n",
    "        time = float((lines[0].split(\"=\")[-1])[:-1])\n",
    "\n",
    "        curr_dict = {\n",
    "            \"t(M)\": time,\n",
    "            \"step\": step,\n",
    "            \"proc\": proc\n",
    "        }\n",
    "        # Find where the columns end\n",
    "        a = lines[4]\n",
    "        event_end = a.find(\"Event\")+5\n",
    "        cum_end = a.find(\"cum(%)\")+6\n",
    "        exc_end = a.find(\"exc(%)\")+6\n",
    "        inc_end = a.find(\"inc(%)\")+6\n",
    "\n",
    "        for line in lines[6:-2]:\n",
    "          Event = line[:event_end].strip()\n",
    "          cum = float(line[event_end:cum_end].strip())\n",
    "          exc = float(line[cum_end:exc_end].strip())\n",
    "          inc = float(line[exc_end:inc_end].strip())\n",
    "          N = int(line[inc_end:].strip())\n",
    "          # print(a)\n",
    "          # a = line.split(\"  \")\n",
    "          # Event,cum,exc,inc,N = [i.strip() for i in a if i!= '']\n",
    "          curr_dict[f'{Event}_cum'] = cum\n",
    "          curr_dict[f'{Event}_exc'] = exc\n",
    "          curr_dict[f'{Event}_inc'] = inc\n",
    "          curr_dict[f'{Event}_N'] = N\n",
    "\n",
    "        dict_list.append(curr_dict)\n",
    "  return pd.DataFrame(dict_list)\n",
    "\n",
    "def read_profiler_multiindex(folder_path:Path):\n",
    "  dir_paths,dat_paths = list_all_dir_and_dat_files(folder_path)\n",
    "  steps = set()\n",
    "  # Get step names\n",
    "  for dir in dir_paths:\n",
    "    step = dir.name.split('.')[0][4:]\n",
    "    steps.add(step)\n",
    "\n",
    "  procs = set()\n",
    "  # Get the proc names\n",
    "  for txt in dir_paths[0].iterdir():\n",
    "    if \".txt\" in txt.name and \"Summary\" not in txt.name:\n",
    "      procs.add(txt.name[4:-4])\n",
    "\n",
    "  dict_list = []\n",
    "  col_names = set()\n",
    "  row_names = []\n",
    "  for step in steps:\n",
    "    for proc in procs:\n",
    "      txt_file_path = folder_path/f'Step{step}.dir/Proc{proc}.txt'\n",
    "\n",
    "      with txt_file_path.open(\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "      time = float((lines[0].split(\"=\")[-1])[:-2])\n",
    "\n",
    "      curr_dict = {\n",
    "          \"time\": time,\n",
    "          \"step\": step,\n",
    "          \"proc\": proc\n",
    "      }\n",
    "\n",
    "      # Find where the columns end\n",
    "      a = lines[4]\n",
    "      event_end = a.find(\"Event\")+5\n",
    "      cum_end = a.find(\"cum(%)\")+6\n",
    "      exc_end = a.find(\"exc(%)\")+6\n",
    "      inc_end = a.find(\"inc(%)\")+6\n",
    "\n",
    "      row_names.append((str(proc),str(time)))\n",
    "\n",
    "      for line in lines[6:-2]:\n",
    "        Event = line[:event_end].strip()\n",
    "        cum = float(line[event_end:cum_end].strip())\n",
    "        exc = float(line[cum_end:exc_end].strip())\n",
    "        inc = float(line[exc_end:inc_end].strip())\n",
    "        N = int(line[inc_end:].strip())\n",
    "        # print(a)\n",
    "        # a = line.split(\"  \")\n",
    "        # Event,cum,exc,inc,N = [i.strip() for i in a if i!= '']\n",
    "        col_names.add(Event)\n",
    "        curr_dict[(\"cum\",Event)] = cum\n",
    "        curr_dict[(\"exc\",Event)] = exc\n",
    "        curr_dict[(\"inc\",Event)] = inc\n",
    "        curr_dict[(\"N\",Event)] = N\n",
    "\n",
    "      dict_list.append(curr_dict)\n",
    "\n",
    "  # Multi index rows\n",
    "  index = pd.MultiIndex.from_tuples(row_names, names=[\"proc\",\"t(M)\"])\n",
    "  df = pd.DataFrame(dict_list,index=index)\n",
    "  \n",
    "  # Multi index cols\n",
    "  multi_index_columns = [(k if isinstance(k, tuple) else (k, '')) for k in df.columns]\n",
    "  df.columns = pd.MultiIndex.from_tuples(multi_index_columns)\n",
    "  df.columns.names = ['metric', 'process']\n",
    "\n",
    "  # data.xs('24', level=\"proc\")['N']\n",
    "  # data.xs('0.511442', level=\"t(M)\")['cum']\n",
    "  # data.xs(('0','0.511442'),level=('proc','t(M)'))\n",
    "  # data.xs('cum',level='metric',axis=1) = data['cum']\n",
    "  # data.xs('MPI::MPreduceAdd(MV<double>)',level='process',axis=1)\n",
    "  # data[data['time']<50]\n",
    "  # data[data['time']<50]['cum'].xs('0',level='proc')['MPI::MPreduceAdd(MV<double>)']\n",
    "  return df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to read dat and hist files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dat_file(file_name):\n",
    "  cols_names = []\n",
    "  # Read column names\n",
    "  with open(file_name,'r') as f:\n",
    "      lines = f.readlines()\n",
    "      for line in lines:\n",
    "        if \"#\" not in line:\n",
    "          # From now onwards it will be all data\n",
    "          break\n",
    "        elif \"=\" in line:\n",
    "          if (\"[\" not in line) and (\"]\" not in line):\n",
    "             continue\n",
    "          cols_names.append(line.split('=')[-1][1:-1].strip())\n",
    "        else:\n",
    "          continue\n",
    "\n",
    "  return pd.read_csv(file_name,sep=\"\\s+\",comment=\"#\",names=cols_names)\n",
    "\n",
    "def hist_files_to_dataframe(file_path):\n",
    "  # Function to parse a single line and return a dictionary of values\n",
    "  def parse_line(line):\n",
    "      data = {}\n",
    "      # Find all variable=value pairs\n",
    "      pairs = re.findall(r'([^;=\\s]+)=\\s*([^;]+)', line)\n",
    "      for var, val in pairs:\n",
    "          # Hist-GrDomain.txt should be parsed a little differently\n",
    "          if 'ResizeTheseSubdomains' in var:\n",
    "              items = val.split('),')\n",
    "              items[-1] = items[-1][:-1]\n",
    "              for item in items:\n",
    "                name,_,vals = item.split(\"(\")\n",
    "                r,l,m=vals[:-1].split(',')\n",
    "                data[f\"{name}_R\"] = int(r)\n",
    "                data[f\"{name}_L\"] = int(l)\n",
    "                data[f\"{name}_M\"] = int(m)\n",
    "          else:\n",
    "              data[var] = float(val) if re.match(r'^[\\d.e+-]+$', val) else val\n",
    "      return data\n",
    "  \n",
    "  with open(file_path, 'r') as file:\n",
    "    # Parse the lines\n",
    "    data = []\n",
    "    for line in file.readlines():\n",
    "        data.append(parse_line(line.strip()))\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "  return df\n",
    "\n",
    "# Files like AhACoefs.dat have unequal number of columns\n",
    "def read_dat_file_uneq_cols(file_name):\n",
    "  cols_names = []\n",
    "\n",
    "  temp_file = \"./temp.csv\"\n",
    "  col_length = 0\n",
    "  with open(file_name,'r') as f:\n",
    "    with open(temp_file,'w') as w:\n",
    "      lines = f.readlines()\n",
    "      for line in lines:\n",
    "        if(line[0] != '#'): # This is data\n",
    "          w.writelines(\" \".join(line.split()[:col_length])+\"\\n\")\n",
    "        if(line[0:3] == '# [' or line[0:4] == '#  ['): # Some dat files have comments on the top\n",
    "          cols_names.append(line.split('=')[-1][1:-1].strip())\n",
    "          col_length = col_length+1\n",
    "\n",
    "\n",
    "  return pd.read_csv(temp_file,delim_whitespace=True,names=cols_names)\n",
    "\n",
    "def read_dat_file_across_AA(file_pattern):\n",
    "\n",
    "  # ApparentHorizons/Horizons.h5@AhA\n",
    "  if 'Horizons.h5@' in file_pattern:\n",
    "    file_pattern,h5_key = file_pattern.split('@')\n",
    "\n",
    "  path_pattern = file_pattern\n",
    "  path_collection = []\n",
    "\n",
    "\n",
    "  for folder_name in glob.iglob(path_pattern, recursive=True):\n",
    "      if os.path.isdir(folder_name) or os.path.isfile(folder_name):\n",
    "          path_collection.append(folder_name)\n",
    "  path_collection.sort()\n",
    "\n",
    "\n",
    "  read_data_collection = []\n",
    "  for path in path_collection:\n",
    "    print(path)\n",
    "    # AhACoefs.dat has uneq cols\n",
    "    if \"Coefs.dat\" in path:\n",
    "        read_data_collection.append(read_dat_file_uneq_cols(path))\n",
    "    elif \"Hist-\" in path:\n",
    "        read_data_collection.append(hist_files_to_dataframe(path))\n",
    "    elif \"Profiler\" in path:\n",
    "        read_data_collection.append(read_profiler(path))\n",
    "    elif \"Horizons.h5\" in path:\n",
    "        returned_data = read_horizonh5(path,h5_key)\n",
    "        if returned_data is not None:\n",
    "            read_data_collection.append(returned_data)\n",
    "    else:\n",
    "        read_data_collection.append(read_dat_file(path))\n",
    "\n",
    "  data = pd.concat(read_data_collection)\n",
    "  rename_dict = {\n",
    "     't':'t(M)',\n",
    "     'time':'t(M)',\n",
    "     'Time':'t(M)',\n",
    "     'time after step':'t(M)',\n",
    "  }\n",
    "  data.rename(columns=rename_dict, inplace=True)\n",
    "  # print(data.columns)\n",
    "  return data\n",
    "\n",
    "def read_horizonh5(horizonh5_path,h5_key):\n",
    "  with h5py.File(horizonh5_path,'r') as hf:\n",
    "    # h5_key = ['AhA','AhB','AhC']\n",
    "    # Horizons.h5 has keys 'AhA.dir'\n",
    "    key = h5_key+\".dir\"\n",
    "    # 'AhC' will not be all the horizons.h5\n",
    "    if key in hf.keys():\n",
    "      return make_Bh_pandas(hf[key])\n",
    "    else:\n",
    "      return None\n",
    "\n",
    "\n",
    "def read_AH_files(Ev_path):\n",
    "  fileA = Ev_path + \"Run/ApparentHorizons/AhA.dat\"\n",
    "  fileB = Ev_path + \"Run/ApparentHorizons/AhB.dat\"\n",
    "\n",
    "  dataA = read_dat_file_across_AA(fileA)\n",
    "  dataB = read_dat_file_across_AA(fileB)\n",
    "\n",
    "  return dataA,dataB  \n",
    "\n",
    "  \n",
    "# Combines all the pvd files into a single file and save it in the base folder\n",
    "def combine_pvd_files(base_folder:Path, file_pattern:str, output_path=None):\n",
    "  pvd_start =\"\"\"<?xml version=\"1.0\"?>\\n<VTKFile type=\"Collection\" version=\"0.1\" byte_order=\"LittleEndian\">\\n  <Collection>\\n\"\"\"\n",
    "  pvd_end =\"  </Collection>\\n</VTKFile>\"\n",
    "\n",
    "  vis_folder_name = file_pattern.split(\"/\")[-1][:-4]\n",
    "  Lev = file_pattern[0:4]\n",
    "\n",
    "  if output_path is None:\n",
    "    output_path = f\"{base_folder}/{vis_folder_name}_{Lev}.pvd\"\n",
    "\n",
    "  pvd_files = list(base_folder.glob(file_pattern))\n",
    "  pvd_folders = list(base_folder.glob(file_pattern[:-4]))\n",
    "\n",
    "\n",
    "  with open(output_path,'w') as write_file:\n",
    "    write_file.writelines(pvd_start)\n",
    "    for files in pvd_files:\n",
    "      print(files)\n",
    "      with files.open(\"r\") as f:\n",
    "        for line in f.readlines():\n",
    "          line = line.replace(vis_folder_name,str(files)[:-4])\n",
    "          if \"DataSet\" in line:\n",
    "            write_file.writelines(line)\n",
    "    write_file.writelines(pvd_end)\n",
    "  \n",
    "  print(output_path)\n",
    "\n",
    "def moving_average(array,avg_len):\n",
    "    return np.convolve(array,np.ones(avg_len))/avg_len\n",
    "    \n",
    "def moving_average_valid(array,avg_len):\n",
    "    return np.convolve(array,np.ones(avg_len),'valid')/avg_len\n",
    "\n",
    "\n",
    "def path_to_folder_name(folder_name):\n",
    "  return folder_name.replace(\"/\",\"_\")\n",
    "\n",
    "# Give a dict of {\"run_name\" = runs_path} and data_file_path to get {\"run_name\" = dat_file_data}\n",
    "def load_data_from_levs(runs_path, data_file_path):\n",
    "  data_dict = {}\n",
    "  column_list = \"\"\n",
    "  for run_name in runs_path.keys():\n",
    "    data_dict[run_name] = read_dat_file_across_AA(runs_path[run_name]+data_file_path)\n",
    "    column_list = data_dict[run_name].columns\n",
    "  return column_list, data_dict\n",
    "\n",
    "def add_diff_columns(runs_data_dict, x_axis, y_axis, diff_base):\n",
    "  if diff_base not in runs_data_dict.keys():\n",
    "    raise Exception(f\"{diff_base} not in {runs_data_dict.keys()}\")\n",
    "\n",
    "  unique_x_data, unique_indices = np.unique(runs_data_dict[diff_base][x_axis], return_index=True)\n",
    "  # sorted_indices = np.sort(unique_indices)\n",
    "  unique_y_data = runs_data_dict[diff_base][y_axis].iloc[unique_indices]\n",
    "  interpolated_data = CubicSpline(unique_x_data,unique_y_data,extrapolate=False)\n",
    "\n",
    "  for key in runs_data_dict:\n",
    "    if key == diff_base:\n",
    "      continue\n",
    "    df = runs_data_dict[key]\n",
    "    df['diff_abs_'+y_axis] = np.abs(df[y_axis] - interpolated_data(df[x_axis]))\n",
    "    df['diff_'+y_axis] = df[y_axis] - interpolated_data(df[x_axis])\n",
    "\n",
    "def plot_graph_for_runs_wrapper(runs_data_dict, x_axis, y_axis_list, minT, maxT, legend_dict=None, save_path=None, moving_avg_len=0, plot_fun = lambda x,y,label : plt.plot(x,y,label=label),sort_by=None, diff_base=None, title=None,append_to_title=\"\",plot_abs_diff=False,constant_shift_val_time=None):\n",
    "\n",
    "  # Do this better using columns of a pandas dataframe\n",
    "  for y_axis in y_axis_list[:-1]:\n",
    "    legend_dict = {}\n",
    "    for key in runs_data_dict:\n",
    "      legend_dict[key] = key+\"_\"+str(y_axis)\n",
    "    plot_graph_for_runs(runs_data_dict, x_axis, y_axis, minT, maxT, legend_dict=legend_dict, save_path=None, moving_avg_len=moving_avg_len, plot_fun = plot_fun,sort_by=sort_by, diff_base=diff_base, title=title,append_to_title=append_to_title,plot_abs_diff=plot_abs_diff,constant_shift_val_time=constant_shift_val_time)\n",
    "\n",
    "  # Save when plotting the last y_axis.\n",
    "  y_axis = y_axis_list[-1]\n",
    "  legend_dict = {}\n",
    "  for key in runs_data_dict:\n",
    "    legend_dict[key] = key+\"_\"+str(y_axis)\n",
    "  plot_graph_for_runs(runs_data_dict, x_axis, y_axis, minT, maxT, legend_dict=legend_dict, save_path=save_path, moving_avg_len=moving_avg_len, plot_fun = plot_fun,sort_by=sort_by, diff_base=diff_base, title=title,append_to_title=append_to_title,plot_abs_diff=plot_abs_diff,constant_shift_val_time=constant_shift_val_time)\n",
    "\n",
    "  plt.ylabel(\"\")\n",
    "  plt.title(\"\"+append_to_title)\n",
    "\n",
    "  if save_path is not None:\n",
    "    fig_x_label = \"\"\n",
    "    fig_y_label = \"\"\n",
    "\n",
    "    for y_axis in y_axis_list:\n",
    "      fig_x_label = fig_x_label + x_axis.replace(\"/\",\"_\").replace(\".\",\"_\")\n",
    "      fig_y_label = fig_y_label + y_axis.replace(\"/\",\"_\").replace(\".\",\"_\")\n",
    "    save_file_name = f\"{fig_y_label}_vs_{fig_x_label}_minT={minT}_maxT={maxT}\".replace(\".\",\"_\")\n",
    "    if moving_avg_len > 0:\n",
    "      save_file_name = save_file_name + f\"_moving_avg_len={moving_avg_len}\"\n",
    "    if diff_base is not None:\n",
    "      save_file_name = save_file_name + f\"_diff_base={diff_base}\"\n",
    "\n",
    "    if len(save_file_name) >= 251: # <save_file_name>.png >=255\n",
    "      save_file_name = save_file_name[:245]+str(random.randint(10000,99999))\n",
    "      print(f\"The filename was too long!! New filename is {save_file_name}\")\n",
    "\n",
    "    plt.savefig(save_path+save_file_name)\n",
    "\n",
    "def plot_graph_for_runs(runs_data_dict, x_axis, y_axis, minT, maxT, legend_dict=None, save_path=None, moving_avg_len=0, plot_fun = lambda x,y,label : plt.plot(x,y,label=label),sort_by=None, diff_base=None, title=None,append_to_title=\"\",plot_abs_diff=False,constant_shift_val_time=None):\n",
    "  sort_run_data_dict(runs_data_dict,sort_by=sort_by)\n",
    "  current_runs_data_dict_keys = list(runs_data_dict.keys())\n",
    "\n",
    "  if diff_base is not None:\n",
    "    add_diff_columns(runs_data_dict, x_axis, y_axis, diff_base)\n",
    "    current_runs_data_dict_keys = []\n",
    "    for key in runs_data_dict:\n",
    "      if key == diff_base:\n",
    "        continue\n",
    "      else:\n",
    "        current_runs_data_dict_keys.append(key)\n",
    "    if plot_abs_diff:\n",
    "      y_axis = \"diff_abs_\" + y_axis\n",
    "    else:\n",
    "      y_axis = \"diff_\"+y_axis\n",
    " \n",
    "  # Find the indices corresponding to maxT and minT\n",
    "  minT_indx_list={}\n",
    "  maxT_indx_list={}\n",
    "\n",
    "  if legend_dict is None:\n",
    "    legend_dict = {}\n",
    "    for run_name in current_runs_data_dict_keys:\n",
    "      legend_dict[run_name] = None\n",
    "  else:\n",
    "    for run_name in current_runs_data_dict_keys:\n",
    "      if run_name not in legend_dict:\n",
    "        raise ValueError(f\"{run_name} not in {legend_dict=}\")\n",
    "\n",
    "  \n",
    "  for run_name in current_runs_data_dict_keys:\n",
    "    minT_indx_list[run_name] = len(runs_data_dict[run_name][x_axis][runs_data_dict[run_name][x_axis] < minT])\n",
    "    maxT_indx_list[run_name] = len(runs_data_dict[run_name][x_axis][runs_data_dict[run_name][x_axis] < maxT])\n",
    "\n",
    "  if moving_avg_len == 0:\n",
    "\n",
    "    for run_name in current_runs_data_dict_keys:\n",
    "      x_data = runs_data_dict[run_name][x_axis][minT_indx_list[run_name]:maxT_indx_list[run_name]]\n",
    "      y_data = runs_data_dict[run_name][y_axis][minT_indx_list[run_name]:maxT_indx_list[run_name]]\n",
    "\n",
    "      if constant_shift_val_time is not None:\n",
    "          shift_label_val = np.abs(x_data.iloc[-1] - x_data.iloc[0])/4\n",
    "          unique_x_data, unique_indices = np.unique(x_data, return_index=True)\n",
    "          # sorted_indices = np.sort(unique_indices)\n",
    "          unique_y_data = y_data.iloc[unique_indices]\n",
    "          try:\n",
    "            interpolated_data = CubicSpline(unique_x_data,unique_y_data,extrapolate=False)\n",
    "          except Exception as e:\n",
    "            print(run_name,unique_y_data)\n",
    "          y_data = y_data - interpolated_data(constant_shift_val_time)\n",
    "      \n",
    "\n",
    "    #   print(f\"{len(x_data)=},{len(y_data)=},{len(np.argsort(x_data))=},{type(x_data)=}\")\n",
    "\n",
    "    #   sorted_indices = x_data.argsort()\n",
    "    #   x_data = x_data.iloc[sorted_indices]\n",
    "    #   y_data = y_data.iloc[sorted_indices]\n",
    "      legend = legend_dict[run_name]\n",
    "      if legend is None:\n",
    "        legend = run_name\n",
    "      plot_fun(x_data, y_data,legend)\n",
    "\n",
    "      if constant_shift_val_time is not None:\n",
    "        plt.axhline(y=y_data.iloc[-1], linestyle=':')\n",
    "        plt.text(x=np.random.rand()*shift_label_val+x_data.iloc[0], y=y_data.iloc[-1], s=f'{y_data.iloc[-1]:.2e}', verticalalignment='bottom')\n",
    "\n",
    "    plt.xlabel(x_axis)\n",
    "    plt.ylabel(y_axis)\n",
    "    if constant_shift_val_time is not None:\n",
    "      plt.axvline(x=constant_shift_val_time, linestyle=':', color='red')\n",
    "    if title is None:\n",
    "      title = \"\\\"\" +  y_axis+\"\\\" vs \\\"\"+x_axis+\"\\\"\"\n",
    "      if constant_shift_val_time is not None:\n",
    "        title = title + f\" constant_shift_val_time={constant_shift_val_time}\"\n",
    "      if diff_base is not None:\n",
    "        title = title + f\" diff_base={diff_base}\"\n",
    "    plt.title(title+append_to_title)\n",
    "    plt.legend()\n",
    "\n",
    "  else:\n",
    "    for run_name in current_runs_data_dict_keys:\n",
    "      x_data = np.array(runs_data_dict[run_name][x_axis][minT_indx_list[run_name] + moving_avg_len-1:maxT_indx_list[run_name]])\n",
    "      y_data = np.array(moving_average_valid(runs_data_dict[run_name][y_axis][minT_indx_list[run_name]:maxT_indx_list[run_name]], moving_avg_len))\n",
    "\n",
    "      if constant_shift_val_time is not None:\n",
    "          shift_label_val = np.abs(x_data.iloc[-1] - x_data.iloc[0])/4\n",
    "          unique_x_data, unique_indices = np.unique(x_data, return_index=True)\n",
    "          # sorted_indices = np.sort(unique_indices)\n",
    "          unique_y_data = y_data.iloc[unique_indices]\n",
    "          \n",
    "          interpolated_data = CubicSpline(unique_x_data,unique_y_data,extrapolate=False)\n",
    "          y_data = y_data - interpolated_data(constant_shift_val_time)\n",
    "      \n",
    "\n",
    "    #   sorted_indices = np.argsort(x_data)\n",
    "    #   x_data = x_data[sorted_indices]\n",
    "    #   y_data = y_data[sorted_indices]\n",
    "      legend = legend_dict[run_name]\n",
    "      if legend is None:\n",
    "        legend = run_name\n",
    "      plot_fun(x_data, y_data,legend)\n",
    "\n",
    "      if constant_shift_val_time is not None:\n",
    "        plt.axhline(y=y_data.iloc[-1], linestyle=':')\n",
    "        plt.text(x=np.random.rand()*shift_label_val+x_data.iloc[0], y=y_data.iloc[-1], s=f'{y_data.iloc[-1]:.1f}', verticalalignment='bottom')\n",
    "\n",
    "    plt.xlabel(x_axis)\n",
    "    plt.ylabel(y_axis)\n",
    "    if constant_shift_val_time is not None:\n",
    "      plt.axvline(x=constant_shift_val_time, linestyle=':', color='red')\n",
    "    if title is None:\n",
    "      title = \"\\\"\" + y_axis+ \"\\\" vs \\\"\" + x_axis + \"\\\"  \" + f\"avg_window_len={moving_avg_len}\"\n",
    "      if constant_shift_val_time is not None:\n",
    "        title = title + f\" constant_shift_val_time={constant_shift_val_time}\"\n",
    "      if diff_base is not None:\n",
    "        title = title + f\" diff_base={diff_base}\"\n",
    "    plt.title(title+append_to_title)\n",
    "    plt.legend()\n",
    "\n",
    "  \n",
    "  if save_path is not None:\n",
    "    fig_x_label = x_axis.replace(\"/\",\"_\").replace(\".\",\"_\")\n",
    "    fig_y_label = y_axis.replace(\"/\",\"_\").replace(\".\",\"_\")\n",
    "    save_file_name = f\"{fig_y_label}_vs_{fig_x_label}_minT={minT}_maxT={maxT}\".replace(\".\",\"_\")\n",
    "    if moving_avg_len > 0:\n",
    "      save_file_name = save_file_name + f\"_moving_avg_len={moving_avg_len}\"\n",
    "    if diff_base is not None:\n",
    "      save_file_name = save_file_name + f\"_diff_base={diff_base}\"\n",
    "\n",
    "    for run_name in current_runs_data_dict_keys:\n",
    "      save_file_name = save_file_name + \"__\" + run_name.replace(\"/\",\"_\").replace(\".\",\"_\")\n",
    "\n",
    "    if len(save_file_name) >= 251: # <save_file_name>.png >=255\n",
    "      save_file_name = save_file_name[:245]+str(random.randint(10000,99999))\n",
    "      print(f\"The filename was too long!! New filename is {save_file_name}\")\n",
    "\n",
    "    plt.savefig(save_path+save_file_name)\n",
    "\n",
    "\n",
    "def find_file(pattern):\n",
    "  return glob.glob(pattern, recursive=True)[0]\n",
    "\n",
    "def plots_for_a_folder(things_to_plot,plot_folder_path,data_folder_path):\n",
    "  for plot_info in things_to_plot:\n",
    "    file_name = plot_info['file_name']\n",
    "    y_arr = plot_info['columns'][1:]\n",
    "    x_arr = [plot_info['columns'][0]]*len(y_arr)\n",
    "\n",
    "    data = read_dat_file_across_AA(data_folder_path+\"/**/\"+file_name)\n",
    "    plot_and_save(data,x_arr,y_arr,plot_folder_path,file_name)\n",
    "\n",
    "def is_the_current_run_going_on(run_folder):\n",
    "  if len(find_file(run_folder+\"/**/\"+\"TerminationReason.txt\")) > 0:\n",
    "    return False\n",
    "  else:\n",
    "    return True\n",
    "\n",
    "def plot_min_grid_spacing(runs_data_dict):\n",
    "    '''\n",
    "    runs_data_dict should have dataframes with MinimumGridSpacing.dat data.\n",
    "    The function will compute the min grid spacing along all domains and plot it.\n",
    "    '''\n",
    "    keys = runs_data_dict.keys()\n",
    "    if len(keys) == 0:\n",
    "        print(\"There are no dataframes in the dict\")\n",
    "\n",
    "    for key in keys:\n",
    "        t_step = runs_data_dict[key]['t']\n",
    "        min_val = runs_data_dict[key].drop(columns=['t']).min(axis='columns')\n",
    "        plt.plot(t_step,min_val,label=key)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"Min Grid Spacing\")\n",
    "    plt.title(\"Min grid spacing in all domains\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_GrAdjustSubChunksToDampingTimes(runs_data_dict):\n",
    "    keys = runs_data_dict.keys()\n",
    "    if len(keys) > 1:\n",
    "        print(\"To plot the Tdamp for various quantities only put one dataframe in the runs_data_dict\")\n",
    "\n",
    "    data:pd.DataFrame = runs_data_dict[list(keys)[0]]\n",
    "    tdamp_keys = []\n",
    "    for key in data.keys():\n",
    "        if 'Tdamp' in key:\n",
    "            tdamp_keys.append(key)\n",
    "\n",
    "    # Get a colormap\n",
    "    cmap = plt.get_cmap('tab10')\n",
    "    colors = cmap(np.linspace(0, 1, len(tdamp_keys)))\n",
    "\n",
    "    t_vals = data['time']\n",
    "    for i, color, key in zip(range(len(tdamp_keys)),colors, tdamp_keys):\n",
    "        if i%2==0:\n",
    "            plt.plot(t_vals,data[key],label=key,color=color)\n",
    "        else:\n",
    "            plt.plot(t_vals,data[key],label=key,color=color,linestyle=\"--\")\n",
    "\n",
    "\n",
    "    min_tdamp = data[tdamp_keys].min(axis='columns')\n",
    "    plt.plot(t_vals,min_tdamp,label=\"min_tdamp\",linewidth=3,linestyle=\"dotted\",color=\"red\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"time\")\n",
    "    plt.title(list(keys)[0])\n",
    "    plt.show()\n",
    "\n",
    "def add_max_and_min_val(runs_data_dict):\n",
    "    # If we load a file with 5 columns with first being time, then find max and min values for all the other columns, at all times and add it to the dataframe.\n",
    "    # Useful when you want to find like Linf across all domains at all times\n",
    "    for run_name in runs_data_dict.keys():\n",
    "        data_frame = runs_data_dict[run_name]\n",
    "        t = data_frame.iloc[:,0]\n",
    "        max_val = np.zeros_like(t)\n",
    "        min_val = np.zeros_like(t)\n",
    "        for i in range(len(t)):\n",
    "            max_val[i] = data_frame.iloc[i,1:].max()\n",
    "            min_val[i] = data_frame.iloc[i,1:].max()\n",
    "\n",
    "        # Add the values to the dataframe\n",
    "        data_frame['max_val'] = max_val\n",
    "        data_frame['min_val'] = min_val\n",
    "\n",
    "def sort_run_data_dict(runs_data_dict:dict,sort_by=None):\n",
    "    for run_name in runs_data_dict.keys():\n",
    "        run_df = runs_data_dict[run_name]\n",
    "        if sort_by is None:\n",
    "            sort_by = run_df.keys()[0]\n",
    "        runs_data_dict[run_name] = run_df.sort_values(by=sort_by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot dat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old runs\n",
    "runs_to_plot = {}\n",
    "# runs_to_plot[\"boost_ID_test_wrong\"] =  \"/groups/sxs/hchaudha/spec_runs/boost_ID_test_wrong/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"boost_ID_test_correct\"] =  \"/groups/sxs/hchaudha/spec_runs/boost_ID_test_correct/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"corrected_coord_spin1\"] =  \"/groups/sxs/hchaudha/spec_runs/corrected_coord_spin1/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"corrected_coord_spin2\"] =  \"/groups/sxs/hchaudha/spec_runs/corrected_coord_spin2/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"2_SpKS_q1_sA_0_0_0_sB_0_0_0_d15\"] =  \"/groups/sxs/hchaudha/spec_runs/2_SpKS_q1_sA_0_0_0_sB_0_0_0_d15/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"2_SpKS_q1_sA_0_0_0_sB_0_0_99_d15\"] =  \"/groups/sxs/hchaudha/spec_runs/2_SpKS_q1_sA_0_0_0_sB_0_0_99_d15/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"2_SpKS_q1_sA_0_0_0_sB_0_0_9_d15\"] =  \"/groups/sxs/hchaudha/spec_runs/2_SpKS_q1_sA_0_0_0_sB_0_0_9_d15/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"2_SpKS_q1_sA_0_0_9_sB_0_0_9_d15\"] =  \"/groups/sxs/hchaudha/spec_runs/2_SpKS_q1_sA_0_0_9_sB_0_0_9_d15/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_tol8_eq\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol8_eq/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_tol9\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol9/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_tol10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_tol10_hi\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol10_hi/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_tol11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_all_100_tol10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/L6_tol10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_all_1000_tol11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/L6_all_10_tol11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"local_100_tol5_11\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/local_100_tol5_11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"local_10_tol5_10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/local_10_tol5_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_local_10_tol5_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/local_10_tol5_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_local_100_tol5_11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/local_100_tol5_11/Lev3_A?/Run/\"\n",
    "\n",
    "\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3/Ev/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_tol8_eq_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol8_eq/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_tol9_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol9/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_tol10_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol10/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_tol10_hi_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol10_hi/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_tol11_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol11/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_all_100_tol10_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/L6_tol10/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_all_1000_tol11_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/L6_all_10_tol11/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3/Ev/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L6\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L6/Ev/Lev6_A?/Run/\"\n",
    "# runs_to_plot[\"L6_1.1\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L6_higher_acc/L6_1.1/Lev6_A?/Run/\"\n",
    "# runs_to_plot[\"L6_1.1_dp8_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L6_higher_acc/L6_1.1_dp8_tol_10/Lev6_A?/Run/\"\n",
    "# runs_to_plot[\"L6_1.1_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L6_higher_acc/L6_1.1_tol_10/Lev6_A?/Run/\"\n",
    "# runs_to_plot[\"L6_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L6_higher_acc/L6_tol_10/Lev6_A?/Run/\"\n",
    "# runs_to_plot[\"all_10\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"near_bhs_10\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/near_bhs_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"near_bhs_100\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/near_bhs_100/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"same_obs\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/same_obs/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_10_obs\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_10_obs/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_obs\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_obs/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_10_obs_tol_10\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_10_obs_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_obs_tol_10\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_obs_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_1.1_b0\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_1.1_b0/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_1.1_b0_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_1.1_b0_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_1.1\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_1.1/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_1.1_dp8_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_1.1_dp8_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_1.1_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_1.1_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_2\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_2/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_2\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_2/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_3\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_3/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dp8\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dp8/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dp8_tol10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dp8_tol10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dp8_tol11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dp8_tol11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dt\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"all_100_t2690_obs_grid_dt_0.02\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.02/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"all_100_t2690_obs_grid_dt_0.03\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.03/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"all_100_t2690_obs_grid_dt_0.04\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.04/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"all_100_t2690_obs_grid_dt_0.025\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.025/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"all_100_t2690_obs_grid_dt_0.021\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.021/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"all_100_t2690_obs_grid_dt_0.022\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.022/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"all_100_t2690_obs_grid_dt_0.023\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.023/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"all_100_t2690_obs_grid_dt_0.024\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.024/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"all_100_t2690_obs_grid_dt_0.0225\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.0225/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dt005\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt0.005/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_tol_11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_tol_11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_tol_9\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_tol_9/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_tol_1.128e-11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_tol_1.128e-11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_tol_1.692e-11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_tol_1.692e-11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_tol_3.383e-11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_tol_3.383e-11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"near_bhs_10_obs\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/near_bhs_10_obs/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"near_bhs_100_obs\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/near_bhs_100_obs/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L6_AA\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L6_AA/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"4_SphKS_q1_15_SSphKS_ID\"] =  \"/groups/sxs/hchaudha/spec_runs/4_SphKS_q1_15_SSphKS_ID/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"4_SphKS_q1_15_SKS_ID\"] =  \"/groups/sxs/hchaudha/spec_runs/4_SphKS_q1_15_SKS_ID/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"5_gd_SphKS_gauge_ID\"] =  \"/groups/sxs/hchaudha/spec_runs/5_gd_SphKS_gauge_ID/Ev/Lev2_A[A-S]/Run/\"\n",
    "# runs_to_plot[\"5_ngd_SphKS_ID\"] =  \"/groups/sxs/hchaudha/spec_runs/5_ngd_SphKS_ID/Ev/Lev2_A?/Run/\"\n",
    "# runs_to_plot[\"5_ngd_KS_ID\"] =  \"/groups/sxs/hchaudha/spec_runs/5_ngd_KS_ID/Ev/Lev2_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"all_100_t2690_eteq_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eteq_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eteq_tol_11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eteq_tol_11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eteq_tol_12\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eteq_tol_12/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eteq_tol_8\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eteq_tol_8/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eteq_tol_9\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eteq_tol_9/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eteq_tol_eq\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eteq_tol_eq/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eth_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eth_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eth_tol_11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eth_tol_11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eth_tol_12\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eth_tol_12/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eth_tol_8\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eth_tol_8/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eth_tol_9\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eth_tol_9/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eth_tol_eq\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eth_tol_eq/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_etl_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_etl_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_etl_tol_11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_etl_tol_11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_etl_tol_8\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_etl_tol_8/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_etl_tol_9\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_etl_tol_9/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_etl_tol_eq\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_etl_tol_eq/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_linf\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_linf/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_tol_10_linf\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_tol_10_linf/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_tol_11_linf\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_tol_11_linf/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_tol_9_linf\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_tol_9_linf/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"t6115_tol11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol9\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol9/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol8\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol8/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol7\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol7/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol11_AMR\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol11_AMR/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol10_AMR\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol10_AMR/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol9_AMR\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol9_AMR/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol8_AMR\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol8_AMR/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol7_AMR\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol7_AMR/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol8_linf\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol8_linf/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.02\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.02/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.03\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.03/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.041\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.041/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.042\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.042/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.043\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.043/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.044\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.044/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.045\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.045/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.046\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.046/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.047\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.047/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.048\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.048/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.049\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.049/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.050\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.050/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.052\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.052/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.054\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.054/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.056\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.056/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_2.368e-07\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_2.368e-07/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_1.692e-07\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_1.692e-07/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_1.015e-07\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_1.015e-07/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_6.767e-08\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_6.767e-08/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_5.075e-08\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_5.075e-08/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_3.383e-08\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_3.383e-08/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_2.256e-08\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_2.256e-08/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_1.692e-08\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_1.692e-08/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_1.128e-08\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_1.128e-08/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_6.767e-09\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_6.767e-09/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_4.833e-09\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_4.833e-09/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_3.383e-09\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_3.383e-09/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_1.692e-09\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_1.692e-09/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_1.128e-09\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_1.128e-09/Lev3_A?/Run/\"\n",
    "\n",
    "\n",
    "# runs_to_plot[\"all_100_t2710_0.021_0.021\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_0.21/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_0.022\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_0.22/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_0.0225\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_0.225/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_0.023\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_0.23/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_0.024\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_0.24/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_max_tol8\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_max_tol8/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_max_tol9\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_max_tol9/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_max_tol10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_max_tol10/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_max_tol10.5\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_max_tol10.5/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_max_tol11\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_max_tol11/Lev3_AE/Run/\"\n",
    "\n",
    "# runs_to_plot[\"eq_t4000_tol10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/eq_t4000_tol10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"eq_t4000_tol5_10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/eq_t4000_tol5_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"eq_t4000_tol5_11\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/eq_t4000_tol5_11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"eq_t4000_tol9\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/eq_t4000_tol9/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t4000_tol10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/t4000_tol10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t4000_tol5_10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/t4000_tol5_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t4000_tol5_11\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/t4000_tol5_11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t4000_tol8\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/t4000_tol8/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t4000_tol9\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/t4000_tol9/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"Lev3_AA_tol10_all_10\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol10_all_10/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol11_all_10\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol11_all_10/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol12_all_10\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol12_all_10/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol5_10_all_10\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol5_10_all_10/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol5_11_all_10\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol5_11_all_10/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol10\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol10/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol11\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol11/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol12\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol12/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol5_10\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol5_10/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol5_11\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol5_11/Lev3_A?_/Run/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_to_plot = {}\n",
    "\n",
    "# runs_to_plot[\"ode_change_Run1\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/0/Lev1/Run/\"\n",
    "# runs_to_plot[\"ode_change_Run2\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/0/Lev2/Run/\"\n",
    "# runs_to_plot[\"ode_change_Run3\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/0/Lev3/Run/\"\n",
    "# runs_to_plot[\"ode_change_Run4\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/0/Lev4/Run/\"\n",
    "# runs_to_plot[\"ode_change_Run5\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/0/Lev5/Run/\"\n",
    "# runs_to_plot[\"ode_change_Run6\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/0/Lev6/Run/\"\n",
    "# runs_to_plot[\"ode_change_Run7\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/0/Lev7/Run/\"\n",
    "# runs_to_plot[\"ode_change_Run8\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/0/Lev8/Run/\"\n",
    "\n",
    "# runs_to_plot[\"1000M_Run1\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/2_1000M/Lev1/Run/\"\n",
    "# runs_to_plot[\"1000M_Run2\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/2_1000M/Lev2/Run/\"\n",
    "# runs_to_plot[\"1000M_Run3\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/2_1000M/Lev3/Run/\"\n",
    "# runs_to_plot[\"1000M_Run4\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/2_1000M/Lev4/Run/\"\n",
    "# runs_to_plot[\"1000M_Run5\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/2_1000M/Lev5/Run/\"\n",
    "# runs_to_plot[\"1000M_Run6\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/2_1000M/Lev6/Run/\"\n",
    "# runs_to_plot[\"1000M_Run7\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/2_1000M/Lev7/Run/\"\n",
    "# runs_to_plot[\"1000M_Run7_a\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/2_1000M/Lev7_a/Run/\"\n",
    "\n",
    "# runs_to_plot[\"Lin_Run1\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/3_Lin/Lev1/Run/\"\n",
    "# runs_to_plot[\"Lin_Run2\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/3_Lin/Lev2/Run/\"\n",
    "# runs_to_plot[\"Lin_Run3\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/3_Lin/Lev3/Run/\"\n",
    "# runs_to_plot[\"Lin_Run4\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/3_Lin/Lev4/Run/\"\n",
    "# runs_to_plot[\"Lin_Run5\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/3_Lin/Lev5/Run/\"\n",
    "# runs_to_plot[\"Lin_Run6\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/3_Lin/Lev6/Run/\"\n",
    "# runs_to_plot[\"Lin_Run7\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/3_Lin/Lev7/Run/\"\n",
    "\n",
    "# runs_to_plot[\"400M_base1\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/4_400M_base/Lev1/Run/\"\n",
    "# runs_to_plot[\"400M_base2\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/4_400M_base/Lev2/Run/\"\n",
    "# runs_to_plot[\"400M_base3\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/4_400M_base/Lev3/Run/\"\n",
    "# runs_to_plot[\"400M_base4\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/4_400M_base/Lev4/Run/\"\n",
    "# runs_to_plot[\"400M_base5\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/4_400M_base/Lev5/Run/\"\n",
    "# runs_to_plot[\"Lin_Run7\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/3_Lin/Lev7/Run/\"\n",
    "\n",
    "# runs_to_plot[\"400M_phys_bc1\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/12_400M_phys_bc/Lev1/Run/\"\n",
    "# runs_to_plot[\"400M_phys_bc2\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/12_400M_phys_bc/Lev2/Run/\"\n",
    "# runs_to_plot[\"400M_phys_bc3\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/12_400M_phys_bc/Lev3/Run/\"\n",
    "# runs_to_plot[\"400M_phys_bc4\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/12_400M_phys_bc/Lev4/Run/\"\n",
    "# runs_to_plot[\"400M_phys_bc5\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/12_400M_phys_bc/Lev5/Run/\"\n",
    "\n",
    "# runs_to_plot[\"400M_phys_bc5\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/12_400M_phys_bc/Lev5/Run/\"\n",
    "\n",
    "# runs_to_plot[\"13_Lev4_250\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/13_error_falloff/Lev4_250/Run/\"\n",
    "# runs_to_plot[\"13_Lev4_300\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/13_error_falloff/Lev4_300/Run/\"\n",
    "# runs_to_plot[\"13_Lev4_350\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/13_error_falloff/Lev4_350/Run/\"\n",
    "# runs_to_plot[\"13_Lev4_400\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/13_error_falloff/Lev4_400/Run/\"\n",
    "# runs_to_plot[\"13_Lev4_450\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/13_error_falloff/Lev4_450/Run/\"\n",
    "# runs_to_plot[\"13_Lev4_500\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/13_error_falloff/Lev4_500/Run/\"\n",
    "# runs_to_plot[\"13_Lev4_550\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/13_error_falloff/Lev4_550/Run/\"\n",
    "# runs_to_plot[\"13_Lev4_600\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/13_error_falloff/Lev4_600/Run/\"\n",
    "# runs_to_plot[\"13_Lev4_650\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/13_error_falloff/Lev4_650/Run/\"\n",
    "# runs_to_plot[\"13_Lev4_700\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/13_error_falloff/Lev4_700/Run/\"\n",
    "# runs_to_plot[\"13_Lev4_750\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/13_error_falloff/Lev4_750/Run/\"\n",
    "\n",
    "# runs_to_plot[\"14_Lev4_250\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/14_error_fo_2000M/Lev4_250/Run/\"\n",
    "# runs_to_plot[\"14_Lev4_300\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/14_error_fo_2000M/Lev4_300/Run/\"\n",
    "# runs_to_plot[\"14_Lev4_350\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/14_error_fo_2000M/Lev4_350/Run/\"\n",
    "# runs_to_plot[\"14_Lev4_400\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/14_error_fo_2000M/Lev4_400/Run/\"\n",
    "# runs_to_plot[\"14_Lev4_450\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/14_error_fo_2000M/Lev4_450/Run/\"\n",
    "# runs_to_plot[\"14_Lev4_500\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/14_error_fo_2000M/Lev4_500/Run/\"\n",
    "# runs_to_plot[\"14_Lev4_550\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/14_error_fo_2000M/Lev4_550/Run/\"\n",
    "# runs_to_plot[\"14_Lev4_600\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/14_error_fo_2000M/Lev4_600/Run/\"\n",
    "# runs_to_plot[\"14_Lev4_650\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/14_error_fo_2000M/Lev4_650/Run/\"\n",
    "# runs_to_plot[\"14_Lev4_700\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/14_error_fo_2000M/Lev4_700/Run/\"\n",
    "# runs_to_plot[\"14_Lev4_750\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/14_error_fo_2000M/Lev4_750/Run/\"\n",
    "\n",
    "# runs_to_plot[\"15_AMR_Lev0_255\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/15_AMR_test/Lev0_255/Run/\"\n",
    "# runs_to_plot[\"15_AMR_Lev0_355\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/15_AMR_test/Lev0_355/Run/\"\n",
    "# runs_to_plot[\"15_AMR_Lev0_455\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/15_AMR_test/Lev0_455/Run/\"\n",
    "# runs_to_plot[\"15_AMR_Lev1_255\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/15_AMR_test/Lev1_255/Run/\"\n",
    "# runs_to_plot[\"15_AMR_Lev1_355\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/15_AMR_test/Lev1_355/Run/\"\n",
    "# runs_to_plot[\"15_AMR_Lev1_455\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/15_AMR_test/Lev1_455/Run/\"\n",
    "# runs_to_plot[\"15_AMR_Lev2_255\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/15_AMR_test/Lev2_255/Run/\"\n",
    "# runs_to_plot[\"15_AMR_Lev2_355\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/15_AMR_test/Lev2_355/Run/\"\n",
    "# runs_to_plot[\"15_AMR_Lev2_455\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/15_AMR_test/Lev2_455/Run/\"\n",
    "# runs_to_plot[\"15_AMR_Lev3_255\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/15_AMR_test/Lev3_255/Run/\"\n",
    "# runs_to_plot[\"15_AMR_Lev3_355\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/15_AMR_test/Lev3_355/Run/\"\n",
    "# runs_to_plot[\"15_AMR_Lev3_455\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/15_AMR_test/Lev3_455/Run/\"\n",
    "# runs_to_plot[\"15_AMR_Lev4_255\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/15_AMR_test/Lev4_255/Run/\"\n",
    "# runs_to_plot[\"15_AMR_Lev4_355\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/15_AMR_test/Lev4_355/Run/\"\n",
    "# runs_to_plot[\"15_AMR_Lev4_455\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/15_AMR_test/Lev4_455/Run/\"\n",
    "# runs_to_plot[\"15_AMR_Lev5_255\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/15_AMR_test/Lev5_255/Run/\"\n",
    "# runs_to_plot[\"15_AMR_Lev5_355\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/15_AMR_test/Lev5_355/Run/\"\n",
    "# runs_to_plot[\"15_AMR_Lev5_455\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/15_AMR_test/Lev5_455/Run/\"\n",
    "\n",
    "# runs_to_plot[\"15_Lev5_255\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/16_AMR_ode_tol_test/Lev5_255/Run/\"\n",
    "# runs_to_plot[\"15_Lev5_255_010\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/16_AMR_ode_tol_test/Lev5_255_010/Run/\"\n",
    "# runs_to_plot[\"15_Lev5_255_0100\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/16_AMR_ode_tol_test/Lev5_255_0100/Run/\"\n",
    "# runs_to_plot[\"15_Lev5_255_01000\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/16_AMR_ode_tol_test/Lev5_255_01000/Run/\"\n",
    "# runs_to_plot[\"15_Lev5_255_010000\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/16_AMR_ode_tol_test/Lev5_255_010000/Run/\"\n",
    "# runs_to_plot[\"15_Lev5_255_05\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/16_AMR_ode_tol_test/Lev5_255_05/Run/\"\n",
    "# runs_to_plot[\"15_Lev5_255_050\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/16_AMR_ode_tol_test/Lev5_255_050/Run/\"\n",
    "# runs_to_plot[\"15_Lev5_255_0500\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/16_AMR_ode_tol_test/Lev5_255_0500/Run/\"\n",
    "# runs_to_plot[\"15_Lev5_255_05000\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/16_AMR_ode_tol_test/Lev5_255_05000/Run/\"\n",
    "# runs_to_plot[\"15_Lev5_255_050000\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/16_AMR_ode_tol_test/Lev5_255_050000/Run/\"\n",
    "\n",
    "# runs_to_plot[\"400M_gamma1_2\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/5_400M_gamma1/Lev2/Run/\"\n",
    "# runs_to_plot[\"400M_gamma1_3\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/5_400M_gamma1/Lev3/Run/\"\n",
    "# runs_to_plot[\"400M_gamma1_4\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/5_400M_gamma1/Lev4/Run/\"\n",
    "\n",
    "# runs_to_plot[\"400M_gamma1_001_2\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/6_400M_gamma1_001/Lev2/Run/\"\n",
    "# runs_to_plot[\"400M_gamma1_001_3\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/6_400M_gamma1_001/Lev3/Run/\"\n",
    "# runs_to_plot[\"400M_gamma1_001_4\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/6_400M_gamma1_001/Lev4/Run/\"\n",
    "\n",
    "# runs_to_plot[\"400M_gamma1_01_2\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/7_400M_gamma1_01/Lev2/Run/\"\n",
    "# runs_to_plot[\"400M_gamma1_01_3\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/7_400M_gamma1_01/Lev3/Run/\"\n",
    "# runs_to_plot[\"400M_gamma1_01_4\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/7_400M_gamma1_01/Lev4/Run/\"\n",
    "\n",
    "# runs_to_plot[\"400M_BDres_2\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/8_400M_BDres/Lev2/Run/\"\n",
    "# runs_to_plot[\"400M_BDres_3\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/8_400M_BDres/Lev3/Run/\"\n",
    "# runs_to_plot[\"400M_BDres_4\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/8_400M_BDres/Lev4/Run/\"\n",
    "\n",
    "# runs_to_plot[\"9_400M_BDres_05_2\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/9_400M_BDres_05/Lev2/Run/\"\n",
    "# runs_to_plot[\"9_400M_BDres_05_3\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/9_400M_BDres_05/Lev3/Run/\"\n",
    "# runs_to_plot[\"9_400M_BDres_05_4\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/9_400M_BDres_05/Lev4/Run/\"\n",
    "\n",
    "# runs_to_plot[\"10_freezing\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/10_freezing/Lev4/Run/\"\n",
    "# runs_to_plot[\"11_physical_bc\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/11_physical_bc/Lev4/Run/\"\n",
    "\n",
    "# runs_to_plot[\"Lev0_265\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/17_zero_spin_AMR/Lev0_265/Run/\"\n",
    "# runs_to_plot[\"Lev1_265\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/17_zero_spin_AMR/Lev1_265/Run/\"\n",
    "# runs_to_plot[\"Lev2_265\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/17_zero_spin_AMR/Lev2_265/Run/\"\n",
    "# runs_to_plot[\"Lev3_265\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/17_zero_spin_AMR/Lev3_265/Run/\"\n",
    "# runs_to_plot[\"Lev4_265\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/17_zero_spin_AMR/Lev4_265/Run/\"\n",
    "# runs_to_plot[\"Lev5_265\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/17_zero_spin_AMR/Lev5_265/Run/\"\n",
    "\n",
    "runs_to_plot[\"Lev5_265\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/18_zero_spin_const_damp/Lev5_265/Run/\"\n",
    "# runs_to_plot[\"Lev5_265_003\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/18_zero_spin_const_damp/Lev5_265_003/Run/\"\n",
    "# runs_to_plot[\"Lev5_265_03\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/18_zero_spin_const_damp/Lev5_265_03/Run/\"\n",
    "# runs_to_plot[\"Lev5_265_10\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/18_zero_spin_const_damp/Lev5_265_10/Run/\"\n",
    "# runs_to_plot[\"Lev5_265_100\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/18_zero_spin_const_damp/Lev5_265_100/Run/\"\n",
    "# runs_to_plot[\"Lev5_265_1000\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/18_zero_spin_const_damp/Lev5_265_1000/Run/\"\n",
    "# runs_to_plot[\"Lev5_265_10000\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/18_zero_spin_const_damp/Lev5_265_10000/Run/\"\n",
    "# runs_to_plot[\"Lev5_265_30\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/18_zero_spin_const_damp/Lev5_265_30/Run/\"\n",
    "# runs_to_plot[\"Lev5_265_9\"] = \"/groups/sxs/hchaudha/spec_runs/single_bh/18_zero_spin_const_damp/Lev5_265_9/Run/\"\n",
    "# runs_to_plot[\"23_nobounds_AMR\"] = \"/groups/sxs/hchaudha/spec_runs/23_nobounds_AMR/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"23_allcd_gaussExc_400\"] = \"/groups/sxs/hchaudha/spec_runs/23_allcd_gaussExc_400/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"24_allcd_gaussEx_5_800\"] = \"/groups/sxs/hchaudha/spec_runs/24_allcd_gaussEx_5_800/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"24_allcd_gaussEx_10_800\"] = \"/groups/sxs/hchaudha/spec_runs/24_allcd_gaussEx_10_800/Ev/Lev3_??/Run/\"\n",
    "\n",
    "# runs_to_plot[\"L5_AD_L5_ps_10\"] = \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master_segs/L5_AD_L5_ps_10/Ev/Lev5_AD/Run/\"\n",
    "# runs_to_plot[\"L5_AD_L5_ps_5\"] = \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master_segs/L5_AD_L5_ps_5/Ev/Lev5_AD/Run/\"\n",
    "# runs_to_plot[\"L5_AD_L5_ps_2\"] = \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master_segs/L5_AD_L5_ps_2/Ev/Lev5_AD/Run/\"\n",
    "# runs_to_plot[\"L5_AD_L5_BCSC_8\"] = \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master_segs/L5_AD_L5_BCSC_8/Ev/Lev5_AD/Run/\"\n",
    "\n",
    "# runs_to_plot[\"Lev5_ode_controller_fixed_0.07\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev5_ode_controller_0.07/Lev5_AC/Run/\"\n",
    "# runs_to_plot[\"Lev5_ode_controller\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev5_ode_controller/Lev5_AC/Run/\"\n",
    "\n",
    "# runs_to_plot[\"eq_AMR_3_tier_const\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L3_contraints/eq_AMR_3_tier_const/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"eq_AMR_3_tier_const_gamma2\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L3_contraints/eq_AMR_3_tier_const_gamma2/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"three_tier_AMR_const_L1\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L3_contraints/three_tier_AMR_const/Ev/Lev1_A?/Run/\"\n",
    "# runs_to_plot[\"three_tier_AMR_const_L2\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L3_contraints/three_tier_AMR_const/Ev/Lev2_A?/Run/\"\n",
    "# runs_to_plot[\"three_tier_AMR_const_L3\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L3_contraints/three_tier_AMR_const/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"normal_constraints\"]=\"/groups/sxs/hchaudha/spec_runs/high_accuracy_L3_contraints/eq_AMR_3_tier_const_variations/normal_constraints/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"normal_constraints_12_AB\"]=\"/groups/sxs/hchaudha/spec_runs/high_accuracy_L3_contraints/eq_AMR_3_tier_const_variations/normal_constraints_12_AB/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"normal_constraints_const1\"]=\"/groups/sxs/hchaudha/spec_runs/high_accuracy_L3_contraints/eq_AMR_3_tier_const_variations/normal_constraints_const1/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"high_accuracy_L3_tol8\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev3_tol8_checkpoint/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L4_tol8\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev4_tol8/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"Lev4_AD_uniform_5\"]  = \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev4_uniform_constraints/Lev4_AD_uniform_5/Run/\"\n",
    "# runs_to_plot[\"Lev4_AD_uniform_1\"]  = \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev4_uniform_constraints/Lev4_AD_uniform_1/Run/\"\n",
    "# runs_to_plot[\"Lev4_AD_uniform_0.1\"]  = \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev4_uniform_constraints/Lev4_AD_uniform_0.1/Run/\"\n",
    "# runs_to_plot[\"Lev4_AD_big_gaussian\"]  = \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev4_uniform_constraints/Lev4_AD_big_gaussian/Run/\"\n",
    "# runs_to_plot[\"Lev4_AD_uniform_1_gamma2_0999\"]  = \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev4_uniform_constraints/Lev4_AD_uniform_1_gamma2_0999/Run/\"\n",
    "# runs_to_plot[\"Lev4_AD_uniform_0.1_gamma2_0999 \"]  = \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev4_uniform_constraints/Lev4_AD_uniform_0.1_gamma2_0999/Run/\"\n",
    "\n",
    "# runs_to_plot[\"3_100\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/changing_spectral_grid/3_100/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/changing_spectral_grid/3_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_1\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/changing_spectral_grid/3_1/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"2_100\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/changing_spectral_grid/2_100/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"2_10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/changing_spectral_grid/2_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"2_1\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/changing_spectral_grid/2_1/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"1_100\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/changing_spectral_grid/1_100/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"1_10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/changing_spectral_grid/1_10/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"1686_1.0e-07_043\"] = \"/groups/sxs/hchaudha/spec_runs/6_lev3_step_size_check/1686_1.0e-07_043/Run/\"\n",
    "# runs_to_plot[\"1686_1.0e-07_046\"] = \"/groups/sxs/hchaudha/spec_runs/6_lev3_step_size_check/1686_1.0e-07_046/Run/\"\n",
    "# runs_to_plot[\"1686_1.0e-07_046_10\"] = \"/groups/sxs/hchaudha/spec_runs/6_lev3_step_size_check/1686_1.0e-07_046_10/Run/\"\n",
    "# runs_to_plot[\"1686_1.0e-07_046_6\"] = \"/groups/sxs/hchaudha/spec_runs/6_lev3_step_size_check/1686_1.0e-07_046_6/Run/\"\n",
    "# runs_to_plot[\"1686_1.0e-07_046x0.5\"] = \"/groups/sxs/hchaudha/spec_runs/6_lev3_step_size_check/1686_1.0e-07_046x0.5/Run/\"\n",
    "# runs_to_plot[\"1686_1.0e-07_046x2\"] = \"/groups/sxs/hchaudha/spec_runs/6_lev3_step_size_check/1686_1.0e-07_046x2/Run/\"\n",
    "# runs_to_plot[\"1686_1.0e-07_048\"] = \"/groups/sxs/hchaudha/spec_runs/6_lev3_step_size_check/1686_1.0e-07_048/Run/\"\n",
    "# runs_to_plot[\"1686_1.0e-07_050\"] = \"/groups/sxs/hchaudha/spec_runs/6_lev3_step_size_check/1686_1.0e-07_050/Run/\"\n",
    "# runs_to_plot[\"1686_1.0e-07_055\"] = \"/groups/sxs/hchaudha/spec_runs/6_lev3_step_size_check/1686_1.0e-07_055/Run/\"\n",
    "# runs_to_plot[\"3555_1.0e-07_040\"] = \"/groups/sxs/hchaudha/spec_runs/6_lev3_step_size_check/3555_1.0e-07_040/Run/\"\n",
    "# runs_to_plot[\"3555_1.0e-07_045\"] = \"/groups/sxs/hchaudha/spec_runs/6_lev3_step_size_check/3555_1.0e-07_045/Run/\"\n",
    "# runs_to_plot[\"3555_1.0e-07_045_10\"] = \"/groups/sxs/hchaudha/spec_runs/6_lev3_step_size_check/3555_1.0e-07_045_10/Run/\"\n",
    "# runs_to_plot[\"3555_1.0e-07_045_6\"] = \"/groups/sxs/hchaudha/spec_runs/6_lev3_step_size_check/3555_1.0e-07_045_6/Run/\"\n",
    "# runs_to_plot[\"3555_1.0e-07_045x0.5\"] = \"/groups/sxs/hchaudha/spec_runs/6_lev3_step_size_check/3555_1.0e-07_045x0.5/Run/\"\n",
    "# runs_to_plot[\"3555_1.0e-07_045x2\"] = \"/groups/sxs/hchaudha/spec_runs/6_lev3_step_size_check/3555_1.0e-07_045x2/Run/\"\n",
    "# runs_to_plot[\"3555_1.0e-07_046\"] = \"/groups/sxs/hchaudha/spec_runs/6_lev3_step_size_check/3555_1.0e-07_046/Run/\"\n",
    "# runs_to_plot[\"3555_1.0e-07_047\"] = \"/groups/sxs/hchaudha/spec_runs/6_lev3_step_size_check/3555_1.0e-07_047/Run/\"\n",
    "# runs_to_plot[\"3555_1.0e-07_050\"] = \"/groups/sxs/hchaudha/spec_runs/6_lev3_step_size_check/3555_1.0e-07_050/Run/\"\n",
    "# runs_to_plot[\"3555_1.0e-07_055\"] = \"/groups/sxs/hchaudha/spec_runs/6_lev3_step_size_check/3555_1.0e-07_055/Run/\"\n",
    "# runs_to_plot[\"3555_1.0e-07_060\"] = \"/groups/sxs/hchaudha/spec_runs/6_lev3_step_size_check/3555_1.0e-07_060/Run/\"\n",
    "# runs_to_plot[\"3555_1.0e-07_065\"] = \"/groups/sxs/hchaudha/spec_runs/6_lev3_step_size_check/3555_1.0e-07_065/Run/\"\n",
    "# runs_to_plot[\"3555_1.0e-07_070\"] = \"/groups/sxs/hchaudha/spec_runs/6_lev3_step_size_check/3555_1.0e-07_070/Run/\"\n",
    "# runs_to_plot[\"3555_1.0e-07_100\"] = \"/groups/sxs/hchaudha/spec_runs/6_lev3_step_size_check/3555_1.0e-07_100/Run/\"\n",
    "\n",
    "# runs_to_plot[\"5_ngd_SphKS_ID\"] = \"/groups/sxs/hchaudha/spec_runs/5_ngd_SphKS_ID/Ev/Lev2_??/Run/\"\n",
    "# runs_to_plot[\"5_ngd_KS_ID\"] = \"/groups/sxs/hchaudha/spec_runs/5_ngd_KS_ID/Ev/Lev2_??/Run/\"\n",
    "# runs_to_plot[\"5_gd_SphKS_gauge_ID\"] = \"/groups/sxs/hchaudha/spec_runs/5_gd_SphKS_gauge_ID/Ev/Lev2_??/Run/\"\n",
    "\n",
    "# runs_to_plot[\"6_set1_L3_fil_buff0\"] = \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_fil_buff0/Ev/Lev3_AB/Run/\"\n",
    "# runs_to_plot[\"6_set1_L3_fil_buff3\"] = \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_del/6_set1_L3_template/Ev/Lev3_AB/Run/\"\n",
    "# runs_to_plot[\"6_set1_L3_fil_buff2\"] = \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_fil_buff2/Ev/Lev3_AB/Run/\"\n",
    "# runs_to_plot[\"6_set1_L3_fil_buff4\"] = \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_fil_buff4/Ev/Lev3_AB/Run/\"\n",
    "# runs_to_plot[\"6_set1_L3_fil_buff0_14012\"] = \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_fil_buff0_14012/Ev/Lev3_AB/Run/\"\n",
    "# runs_to_plot[\"6_set1_L3_fil_buff2_14012\"] = \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_fil_buff2_14012/Ev/Lev3_AB/Run/\"\n",
    "# runs_to_plot[\"6_set1_L3_fil_buff4_14012\"] = \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_fil_buff4_14012/Ev/Lev3_AB/Run/\"\n",
    "\n",
    "# runs_to_plot[\"6_set1_L3_FK_9443_C\"] = \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_FK_9443_C/Ev/Lev3_AB/Run/\"\n",
    "# runs_to_plot[\"6_set1_L3_FK_9443_All\"] = \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_FK_9443_All/Ev/Lev3_AB/Run/\"\n",
    "\n",
    "# runs_to_plot[\"6_set1_L3_FK_14012_C\"] = \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_FK_14012_C/Ev/Lev3_AB/Run/\"\n",
    "# runs_to_plot[\"6_set1_L3_FK_14012_All\"] = \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_FK_14012_All/Ev/Lev3_AB/Run/\"\n",
    "# runs_to_plot[\"6_set1_L3_FK_14012_All_11\"] = \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_FK_14012_All_11/Ev/Lev3_AB/Run/\"\n",
    "# runs_to_plot[\"6_set1_L3_FK_14012_C_9\"] = \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_FK_14012_C_9/Ev/Lev3_AB/Run/\"\n",
    "# runs_to_plot[\"6_set1_L3_FK_14012_C_13\"] = \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_FK_14012_C_13/Ev/Lev3_AB/Run/\"\n",
    "# runs_to_plot[\"6_set1_L3_FK_14012_C_copy\"] = \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_FK_14012_C_copy/Ev/Lev3_AB/Run/\"\n",
    "\n",
    "# runs_to_plot[\"6_set1_L3_EXP_FK_14012_5_6_30\"] = \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_EXP_FK_14012_5_6_30/Ev/Lev3_AB/Run/\"\n",
    "# runs_to_plot[\"6_set1_L3_EXP_FK_14012_10_6_30\"] = \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_EXP_FK_14012_10_6_30/Ev/Lev3_AB/Run/\"\n",
    "# runs_to_plot[\"6_set1_L3_EXP_FK_14012_1_6_30\"] = \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_EXP_FK_14012_1_6_30/Ev/Lev3_AB/Run/\"\n",
    "# runs_to_plot[\"6_set1_L3_EXP_FK_14012_5_4_30\"] = \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_EXP_FK_14012_5_4_30/Ev/Lev3_AB/Run/\"\n",
    "# runs_to_plot[\"6_set1_L3_EXP_FK_14012_5_2_30\"] = \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_EXP_FK_14012_5_2_30/Ev/Lev3_AB/Run/\"\n",
    "\n",
    "\n",
    "# data_file_path = \"FailedTStepperDiag.dat\"\n",
    "# data_file_path = \"GhCe_Norms.dat\"\n",
    "# data_file_path = \"GhCe.dat\"\n",
    "# data_file_path = \"NormalizedGhCe.dat\"\n",
    "# data_file_path = \"GhCe_Linf.dat\"\n",
    "# data_file_path = \"GhCe.dat\"\n",
    "# data_file_path = \"NormalizedGhCe_Linf.dat\"\n",
    "# data_file_path = \"1Con.dat\"\n",
    "# data_file_path = \"2Con.dat\"\n",
    "data_file_path = \"3Con.dat\"\n",
    "# data_file_path = \"kappaErr_Linf.dat\"\n",
    "# data_file_path = \"psiErr_Linf.dat\"\n",
    "# data_file_path = \"TStepperDiag.dat\"\n",
    "\n",
    "column_names, runs_data_dict = load_data_from_levs(runs_to_plot,data_file_path)\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_to_plot = {}\n",
    "\n",
    "# runs_to_plot[\"73_gd_master_new_code\"] =  \"/net/panfs/SXS/himanshu/gauge_stuff/gauge_driver_runs/runs/73_gd_master_new_code/Ev/Lev1_A?/Run/\"\n",
    "# runs_to_plot[\"119_gd_SUKS_3_20\"] =  \"/net/panfs/SXS/himanshu/gauge_stuff/gauge_driver_runs/runs/119_gd_SUKS_3_20/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"120W_gd_SUKS1_3_20\"] =  \"/net/panfs/SXS/himanshu/gauge_stuff/gauge_driver_runs/runs/120W_gd_SUKS1_3_20/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"AccTest_q1ns_Lev9\"] =  \"/net/panfs/SXS/himanshu/gauge_stuff/gauge_driver_runs/runs/AccTest_q1ns_Lev9/Ev/Lev9_A?/Run/\"\n",
    "# runs_to_plot[\"77_gd_Kerr_q3\"] =  \"/net/panfs/SXS/himanshu/gauge_stuff/gauge_driver_runs/runs/77_gd_Kerr_q3/Ev_Kerr/Lev1_A?/Run/\"\n",
    "# runs_to_plot[\"77_gd_Kerr_q3\"] =  \"/net/panfs/SXS/himanshu/gauge_stuff/gauge_driver_runs/runs/120W_gd_SUKS1_3_20/Ev/Lev3_A?/Run/\"\n",
    "\n",
    "\n",
    "# data_file_path = \"ConstraintNorms/GhCe.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCeExt.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCeExt_L2.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCeExt_Norms.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCe_L2.dat\"\n",
    "data_file_path = \"ConstraintNorms/GhCe_Linf.dat\"\n",
    "# data_file_path = \"ConstraintNorms/Linf.dat\"\n",
    "# data_file_path = \"ConstraintNorms/Constraints_Linf.dat\"\n",
    "# data_file_path = \"ConstraintNorms/NormalizedGhCe_Linf.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCe_Norms.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCe_VolL2.dat\"\n",
    "# data_file_path = \"ConstraintNorms/NormalizedGhCe_Linf.dat\"\n",
    "# data_file_path = \"ConstraintNorms/NormalizedGhCe_Norms.dat\"\n",
    "# data_file_path = \"CharSpeedNorms/CharSpeeds_Min_SliceLFF.SphereA0.dat\"\n",
    "# data_file_path = \"MinimumGridSpacing.dat\"\n",
    "# data_file_path = \"GrAdjustMaxTstepToDampingTimes.dat\"\n",
    "# data_file_path = \"GrAdjustSubChunksToDampingTimes.dat\"\n",
    "# data_file_path = \"DiagAhSpeedA.dat\"\n",
    "# data_file_path = \"ApparentHorizons/AhA.dat\"\n",
    "# data_file_path = \"ApparentHorizons/AhB.dat\" \n",
    "# data_file_path = \"ApparentHorizons/MinCharSpeedAhA.dat\"\n",
    "# data_file_path = \"ApparentHorizons/RescaledRadAhA.dat\"\n",
    "# data_file_path = \"ApparentHorizons/AhACoefs.dat\"\n",
    "# data_file_path = \"ApparentHorizons/AhBCoefs.dat\"\n",
    "# data_file_path = \"ApparentHorizons/Trajectory_AhB.dat\"\n",
    "# data_file_path = \"ApparentHorizons/HorizonSepMeasures.dat\"\n",
    "\n",
    "# data_file_path = \"ApparentHorizons/Horizons.h5@AhA\"\n",
    "# data_file_path = \"TStepperDiag.dat\"\n",
    "# data_file_path = \"TimeInfo.dat\"\n",
    "# data_file_path = \"Hist-FuncSkewAngle.txt\"\n",
    "# data_file_path = \"Hist-FuncCutX.txt\"\n",
    "# data_file_path = \"Hist-FuncExpansionFactor.txt\"\n",
    "# data_file_path = \"Hist-FuncLambdaFactorA0.txt\"\n",
    "# data_file_path = \"Hist-FuncLambdaFactorA.txt\"\n",
    "# data_file_path = \"Hist-FuncLambdaFactorB0.txt\"\n",
    "# data_file_path = \"Hist-FuncLambdaFactorB.txt\"\n",
    "# data_file_path = \"Hist-FuncQuatRotMatrix.txt\"\n",
    "# data_file_path = \"Hist-FuncSkewAngle.txt\"\n",
    "# data_file_path = \"Hist-FuncSmoothCoordSep.txt\"\n",
    "# data_file_path = \"Hist-FuncSmoothMinDeltaRNoLam00AhA.txt\"\n",
    "# data_file_path = \"Hist-FuncSmoothMinDeltaRNoLam00AhB.txt\"\n",
    "# data_file_path = \"Hist-FuncSmoothRAhA.txt\"\n",
    "# data_file_path = \"Hist-FuncSmoothRAhB.txt\"\n",
    "# data_file_path = \"Hist-FuncTrans.txt\"\n",
    "# data_file_path = \"Hist-GrDomain.txt\"\n",
    "# data_file_path = \"Profiler.h5\"\n",
    "column_names, runs_data_dict = load_data_from_levs(runs_to_plot,data_file_path)\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_to_plot = {}\n",
    "\n",
    "# runs_to_plot[\"high_accuracy_L3_tol8_wrong\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev3_tol8/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L3_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L4_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev4_Ringdown/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L5_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev5_Ringdown/Lev5_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"high_accuracy_L0\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev0_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L1\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev1_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L2\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev2_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L3\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L4\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L5\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L45\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev45_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L55\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev55_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L6\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev6_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"high_accuracy_L45n\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_seg_runs/new_L45_L55/Ev/Lev45_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L55n\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_seg_runs/new_L45_L55/Ev/Lev55_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"high_accuracy_L4n_no_tol_change\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_seg_runs/main_L4_to_L55/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L45n_no_tol_change\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_seg_runs/main_L4_to_L55/Ev/Lev45_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L5n_no_tol_change\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_seg_runs/main_L4_to_L55/Ev/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L55n_no_tol_change\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_seg_runs/main_L4_to_L55/Ev/Lev55_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"high_accuracy_L5_three_tier\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev5_big_gaussian/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L5_three_tier_constra\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev5_big_gaussian_constra/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L5_three_tier_constra200\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev5_big_gaussian_constra_200/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"L3_step_bound_gauss_error\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L3_step_bound_gauss_error/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_step_bound_gauss_error_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L3_step_bound_gauss_error/Ev/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"Lev5_big_gaussian_ah_tol10\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev5_big_gaussian_ah_tol10/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"Lev5_big_gaussian_ah_tol100\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev5_big_gaussian_ah_tol100/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"Lev5_bg_ah100_cd_01_uamr\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev5_bg_ah100_cd_01_uamr_full/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"Lev5_bg_ah100_lapse\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev5_bg_ah100_lapse_full/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"Lev5_bg_ah100_lapse_uamr\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev5_bg_ah100_lapse_uamr_full/Lev5_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"high_accuracy_L0_main\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/Ev/Lev0_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L1_main\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/Ev/Lev1_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L2_main\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/Ev/Lev2_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L3_main\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L4_main\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L5_main\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/Ev/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L45_main\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/Ev/Lev45_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L55_main\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/Ev/Lev55_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L6_main\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/Ev/Lev6_A?/Run/\"\n",
    "\n",
    "\n",
    "# runs_to_plot[\"ode_impro_Lev0\"] = '/groups/sxs/hchaudha/spec_runs/Lev01_test/new_ode_tol/high_accuracy_L35/Ev/Lev0_A?/Run/'\n",
    "# runs_to_plot[\"ode_impro_Lev1\"] = '/groups/sxs/hchaudha/spec_runs/Lev01_test/new_ode_tol/high_accuracy_L35/Ev/Lev1_A?/Run/'\n",
    "# runs_to_plot[\"ode_impro_Lev2\"] = '/groups/sxs/hchaudha/spec_runs/Lev01_test/new_ode_tol/high_accuracy_L35/Ev/Lev2_A?/Run/'\n",
    "# runs_to_plot[\"ode_impro_Lev3\"] = '/groups/sxs/hchaudha/spec_runs/Lev01_test/new_ode_tol/high_accuracy_L35/Ev/Lev3_A?/Run/'\n",
    "# runs_to_plot[\"ode_impro_Lev4\"] = '/groups/sxs/hchaudha/spec_runs/Lev01_test/new_ode_tol/high_accuracy_L35/Ev/Lev4_A?/Run/'\n",
    "# runs_to_plot[\"ode_impro_Lev5\"] = '/groups/sxs/hchaudha/spec_runs/Lev01_test/new_ode_tol/high_accuracy_L35/Ev/Lev5_A?/Run/'\n",
    "# runs_to_plot[\"main_Lev0\"] = '/groups/sxs/hchaudha/spec_runs/Lev01_test/old_ode_tol/high_accuracy_L35_master/Ev/Lev0_A?/Run/'\n",
    "# runs_to_plot[\"main_Lev2\"] = '/groups/sxs/hchaudha/spec_runs/Lev01_test/old_ode_tol/high_accuracy_L35_master/Ev/Lev2_A?/Run/'\n",
    "# runs_to_plot[\"main_Lev1\"] = '/groups/sxs/hchaudha/spec_runs/Lev01_test/old_ode_tol/high_accuracy_L35_master/Ev/Lev1_A?/Run/'\n",
    "\n",
    "# runs_to_plot[\"ode_impro_Lev0_rd\"] = '/groups/sxs/hchaudha/spec_runs/Lev01_test/new_ode_tol/high_accuracy_L35/Ev/Lev0_Ringdown/Lev0_A?/Run/'\n",
    "# runs_to_plot[\"ode_impro_Lev2_rd\"] = '/groups/sxs/hchaudha/spec_runs/Lev01_test/new_ode_tol/high_accuracy_L35/Ev/Lev2_Ringdown/Lev2_A?/Run/'\n",
    "# runs_to_plot[\"ode_impro_Lev1_rd\"] = '/groups/sxs/hchaudha/spec_runs/Lev01_test/new_ode_tol/high_accuracy_L35/Ev/Lev1_Ringdown/Lev1_A?/Run/'\n",
    "# runs_to_plot[\"main_Lev0_rd\"] = '/groups/sxs/hchaudha/spec_runs/Lev01_test/old_ode_tol/high_accuracy_L35_master/Ev/Lev0_Ringdown/Lev0_A?/Run/'\n",
    "# runs_to_plot[\"main_Lev2_rd\"] = '/groups/sxs/hchaudha/spec_runs/Lev01_test/old_ode_tol/high_accuracy_L35_master/Ev/Lev2_Ringdown/Lev2_A?/Run/'\n",
    "# runs_to_plot[\"main_Lev1_rd\"] = '/groups/sxs/hchaudha/spec_runs/Lev01_test/old_ode_tol/high_accuracy_L35_master/Ev/Lev1_Ringdown/Lev1_A?/Run/'\n",
    "\n",
    "# runs_to_plot[\"6_set1_L3s0\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L3/Ev/Lev0_A?/Run/\"\n",
    "# runs_to_plot[\"6_set1_L3s1\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L3/Ev/Lev1_A?/Run/\"\n",
    "# runs_to_plot[\"6_set1_L3s2\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L3/Ev/Lev2_A?/Run/\"\n",
    "# runs_to_plot[\"6_set1_L3s3\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L3/Ev/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"6_set1_L3s3_fil6\"] =  \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_5517_6/Ev/Lev3_A[B-]/Run/\"\n",
    "# runs_to_plot[\"6_set1_L3s3_fil8\"] =  \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_5517_8/Ev/Lev3_A[B-]/Run/\"\n",
    "# runs_to_plot[\"6_set1_L3s3_fil10\"] =  \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_5517_10/Ev/Lev3_A[B-]/Run/\"\n",
    "\n",
    "# runs_to_plot[\"6_set1_L3_template_all\"] =  \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_template_all/Ev/Lev3_A[B-]/Run/\"\n",
    "# runs_to_plot[\"6_set1_L3_template_1_29\"] =  \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_template_1_29/Ev/Lev3_A[B-]/Run/\"\n",
    "\n",
    "# runs_to_plot[\"6_set1_L3s3_5517_CCopy\"] =  \"/groups/sxs/hchaudha/spec_runs/19_filtered_checkpoint_runs/6_set1_L3_5517_CCopy/Ev/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"6_set2_L3s2\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set2_L3/Ev/Lev2_A?/Run/\"\n",
    "# runs_to_plot[\"6_set2_L3s3\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set2_L3/Ev/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"24_allcd_gaussEx_10_800\"] =  \"/groups/sxs/hchaudha/spec_runs/24_allcd_gaussEx_10_800/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"24_allcd_gaussEx_5_800\"] =  \"/groups/sxs/hchaudha/spec_runs/24_allcd_gaussEx_5_800/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"25_allcd_gaussEx_2_60\"] =  \"/groups/sxs/hchaudha/spec_runs/25_allcd_gaussEx_2_60/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"25_allcd_gaussEx_2_800\"] =  \"/groups/sxs/hchaudha/spec_runs/25_allcd_gaussEx_2_800/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"25_allcd_gaussEx_5_60\"] =  \"/groups/sxs/hchaudha/spec_runs/25_allcd_gaussEx_5_60/Ev/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"6_set3_L3s0\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set3_L3/Ev/Lev0_A?/Run/\"\n",
    "# runs_to_plot[\"6_set3_L3s1\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set3_L3/Ev/Lev1_A?/Run/\"\n",
    "# runs_to_plot[\"6_set3_L3s2\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set3_L3/Ev/Lev2_A?/Run/\"\n",
    "# runs_to_plot[\"6_set3_L3s3\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set3_L3/Ev/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"6_set1_L6s0\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/Ev/Lev0_A?/Run/\"\n",
    "runs_to_plot[\"6_set1_L6s1\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/Ev/Lev1_A?/Run/\"\n",
    "runs_to_plot[\"6_set1_L6s2\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/Ev/Lev2_A?/Run/\"\n",
    "runs_to_plot[\"6_set1_L6s3\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/Ev/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"6_set1_L6s4\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/Ev/Lev4_A?/Run/\"\n",
    "runs_to_plot[\"6_set1_L6s5\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/Ev/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"6_set1_L6s6\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/Ev/Lev6_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"set1_L6s4_cd10\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/set1_L6s4_cd10/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"set1_L6s4_cd100\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/set1_L6s4_cd100/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"set1_L6s4_cd200\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/set1_L6s4_cd200/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"set1_L6s4_cd500\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/set1_L6s4_cd500/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"set1_L6s4_cd100_AMRL6\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/set1_L6s4_cd100_AMRL6/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"set1_L6s4_cd100_AMRL7\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/set1_L6s4_cd100_AMRL7/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"set1_L6s4_cd100_AMRL8\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/set1_L6s4_cd100_AMRL8/Ev/Lev4_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"6_set2_L6s4\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set2_L6/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"6_set2_L6s5\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set2_L6/Ev/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"6_set2_L6s6\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set2_L6/Ev/Lev6_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"6_set3_L6s4\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set3_L6/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"6_set3_L6s5\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set3_L6/Ev/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"6_set3_L6s6\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set3_L6/Ev/Lev6_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"6_set1_L6s3_CAMR\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6_vars/L6s3_CAMR/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"6_set1_L6s3_min_L\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6_vars/L6s3_min_L/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"6_set1_L6s3_min_LR\"] =  \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6_vars/L6s3_min_LR/Ev/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"7_constAMR_set1_L6_base_0\"] =  \"/groups/sxs/hchaudha/spec_runs/7_constAMR_set1_L6_base/Ev/Lev0_A?/Run/\"\n",
    "# runs_to_plot[\"7_constAMR_set1_L6_base_1\"] =  \"/groups/sxs/hchaudha/spec_runs/7_constAMR_set1_L6_base/Ev/Lev1_A?/Run/\"\n",
    "# runs_to_plot[\"7_constAMR_set1_L6_base_2\"] =  \"/groups/sxs/hchaudha/spec_runs/7_constAMR_set1_L6_base/Ev/Lev2_A?/Run/\"\n",
    "# runs_to_plot[\"7_constAMR_set1_L6_base_3\"] =  \"/groups/sxs/hchaudha/spec_runs/7_constAMR_set1_L6_base/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"7_constAMR_set1_L6_base_4\"] =  \"/groups/sxs/hchaudha/spec_runs/7_constAMR_set1_L6_base/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"7_constAMR_set1_L6_base_5\"] =  \"/groups/sxs/hchaudha/spec_runs/7_constAMR_set1_L6_base/Ev/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"7_constAMR_set1_L6_base_6\"] =  \"/groups/sxs/hchaudha/spec_runs/7_constAMR_set1_L6_base/Ev/Lev6_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"8_constAMR_set1_L6_base_0\"] =  \"/groups/sxs/hchaudha/spec_runs/8_constAMR_set1_L6_base/Ev/Lev0_A?/Run/\"\n",
    "# runs_to_plot[\"8_constAMR_set1_L6_base_1\"] =  \"/groups/sxs/hchaudha/spec_runs/8_constAMR_set1_L6_base/Ev/Lev1_A?/Run/\"\n",
    "# runs_to_plot[\"8_constAMR_set1_L6_base_2\"] =  \"/groups/sxs/hchaudha/spec_runs/8_constAMR_set1_L6_base/Ev/Lev2_A?/Run/\"\n",
    "# runs_to_plot[\"8_constAMR_set1_L6_base_3\"] =  \"/groups/sxs/hchaudha/spec_runs/8_constAMR_set1_L6_base/Ev/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"9_set1_L3s3_01\"] =  \"/groups/sxs/hchaudha/spec_runs/9_const_damp_var/set1_L3s3_01/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"9_set1_L3s3_001\"] =  \"/groups/sxs/hchaudha/spec_runs/9_const_damp_var/set1_L3s3_001/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"9_set1_L3s3_10\"] =  \"/groups/sxs/hchaudha/spec_runs/9_const_damp_var/set1_L3s3_10/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"9_set1_L3s3_100\"] =  \"/groups/sxs/hchaudha/spec_runs/9_const_damp_var/set1_L3s3_100/Ev/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"10_4000M_CAMR_set1_L6_base0\"] =  \"/groups/sxs/hchaudha/spec_runs/10_4000M_CAMR_set1_L6_base/Ev/Lev0_A?/Run/\"\n",
    "# runs_to_plot[\"10_4000M_CAMR_set1_L6_base1\"] =  \"/groups/sxs/hchaudha/spec_runs/10_4000M_CAMR_set1_L6_base/Ev/Lev1_A?/Run/\"\n",
    "# runs_to_plot[\"10_4000M_CAMR_set1_L6_base2\"] =  \"/groups/sxs/hchaudha/spec_runs/10_4000M_CAMR_set1_L6_base/Ev/Lev2_A?/Run/\"\n",
    "# runs_to_plot[\"10_4000M_CAMR_set1_L6_base3\"] =  \"/groups/sxs/hchaudha/spec_runs/10_4000M_CAMR_set1_L6_base/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"10_4000M_CAMR_set1_L6_base4\"] =  \"/groups/sxs/hchaudha/spec_runs/10_4000M_CAMR_set1_L6_base/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"10_4000M_CAMR_set1_L6_base5\"] =  \"/groups/sxs/hchaudha/spec_runs/10_4000M_CAMR_set1_L6_base/Ev/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"10_4000M_CAMR_set1_L6_base6\"] =  \"/groups/sxs/hchaudha/spec_runs/10_4000M_CAMR_set1_L6_base/Ev/Lev6_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"11_4000M_CAMR_set1_L6_base0\"] =  \"/groups/sxs/hchaudha/spec_runs/11_4000M_CAMR_set1_L6_maxExt/Ev/Lev0_A?/Run/\"\n",
    "# runs_to_plot[\"11_4000M_CAMR_set1_L6_base1\"] =  \"/groups/sxs/hchaudha/spec_runs/11_4000M_CAMR_set1_L6_maxExt/Ev/Lev1_A?/Run/\"\n",
    "# runs_to_plot[\"11_4000M_CAMR_set1_L6_base2\"] =  \"/groups/sxs/hchaudha/spec_runs/11_4000M_CAMR_set1_L6_maxExt/Ev/Lev2_A?/Run/\"\n",
    "# runs_to_plot[\"11_4000M_CAMR_set1_L6_base3\"] =  \"/groups/sxs/hchaudha/spec_runs/11_4000M_CAMR_set1_L6_maxExt/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"11_4000M_CAMR_set1_L6_base4\"] =  \"/groups/sxs/hchaudha/spec_runs/11_4000M_CAMR_set1_L6_maxExt/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"11_4000M_CAMR_set1_L6_base5\"] =  \"/groups/sxs/hchaudha/spec_runs/11_4000M_CAMR_set1_L6_maxExt/Ev/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"11_4000M_CAMR_set1_L6_base6\"] =  \"/groups/sxs/hchaudha/spec_runs/11_4000M_CAMR_set1_L6_maxExt/Ev/Lev6_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"12_set1_L3_1500\"] =  \"/groups/sxs/hchaudha/spec_runs/12_set1_L3_1500/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"12_set1_L3_2000\"] =  \"/groups/sxs/hchaudha/spec_runs/12_set1_L3_2000/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"12_set1_L3_2500\"] =  \"/groups/sxs/hchaudha/spec_runs/12_set1_L3_2500/Ev/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"13_set1_L3_3000\"] =  \"/groups/sxs/hchaudha/spec_runs/13_set1_L3_3000/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"13_set1_L4_1500\"] =  \"/groups/sxs/hchaudha/spec_runs/13_set1_L4_1500/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"13_set1_L4_3000\"] =  \"/groups/sxs/hchaudha/spec_runs/13_set1_L4_3000/Ev/Lev4_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"14_set1_L4_1500_cd5\"] =  \"/groups/sxs/hchaudha/spec_runs/14_set1_L4_1500_cd5/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"14_set1_L4_1500_cd10\"] =  \"/groups/sxs/hchaudha/spec_runs/14_set1_L4_1500_cd10/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"14_set1_L4_1500_cd25\"] =  \"/groups/sxs/hchaudha/spec_runs/14_set1_L4_1500_cd25/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"14_set1_L4_1500_cd50\"] =  \"/groups/sxs/hchaudha/spec_runs/14_set1_L4_1500_cd50/Ev/Lev4_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"15_set1_L4_1500_JY\"] =  \"/groups/sxs/hchaudha/spec_runs/15_set1_L4_1500_JY/Ev/Lev4_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"16_set1_L3\"] = \"/groups/sxs/hchaudha/spec_runs/16_set1_L3/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"16_set1_L3_HP32\"] = \"/groups/sxs/hchaudha/spec_runs/16_set1_L3_HP32/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"16_set1_L3_HP28\"] = \"/groups/sxs/hchaudha/spec_runs/16_set1_L3_HP28/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"16_set1_L3_HP32_AF\"] = \"/groups/sxs/hchaudha/spec_runs/16_set1_L3_HP32_AF/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"17_BDI_32_SAE_NONE\"] = \"/groups/sxs/hchaudha/spec_runs/17_BDI_32_SAE_NONE/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"17_BDI_32_SAE_32\"] = \"/groups/sxs/hchaudha/spec_runs/17_BDI_32_SAE_32/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"17_BDI_32_SAE_32_AF\"] = \"/groups/sxs/hchaudha/spec_runs/17_BDI_32_SAE_32_AF/Ev/Lev4_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"17_set_main_q3_15_L3\"] = \"/groups/sxs/hchaudha/spec_runs/17_set_main_q3_15_L3/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"17_set_main_99_15_L3\"] = \"/groups/sxs/hchaudha/spec_runs/17_set_main_99_15_L3/Ev/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"17_set_main_q3_18_L3\"] = \"/groups/sxs/hchaudha/spec_runs/17_set_main_q3_18_L3/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"17_set1_q3_18_L3\"] = \"/groups/sxs/hchaudha/spec_runs/17_set1_q3_18_L3/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"17_set3_q3_18_L3\"] = \"/groups/sxs/hchaudha/spec_runs/17_set3_q3_18_L3/Ev/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"17_set_main_99_18_L3\"] = \"/groups/sxs/hchaudha/spec_runs/17_set_main_99_18_L3/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"17_set1_99_18_L3\"] = \"/groups/sxs/hchaudha/spec_runs/17_set1_99_18_L3/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"17_set3_99_18_L3\"] = \"/groups/sxs/hchaudha/spec_runs/17_set3_99_18_L3/Ev/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"17_main_9_18_L3\"] = \"/groups/sxs/hchaudha/spec_runs/17_main_9_18_L3/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"17_set1_9_18_L3\"] = \"/groups/sxs/hchaudha/spec_runs/17_set1_9_18_L3/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"17_set3_9_18_L3\"] = \"/groups/sxs/hchaudha/spec_runs/17_set3_9_18_L3/Ev/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"17_main_9_18_L3_correct\"] = \"/groups/sxs/hchaudha/spec_runs/17_main_9_18_L3_correct/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"17_set1_9_18_L3_correct\"] = \"/groups/sxs/hchaudha/spec_runs/17_set1_9_18_L3_correct/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"17_set3_9_18_L3_correct\"] = \"/groups/sxs/hchaudha/spec_runs/17_set3_9_18_L3_correct/Ev/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"18_set1_L3_junk_resolved\"] = \"/groups/sxs/hchaudha/spec_runs/18_set1_L3_junk_resolved/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"20_set1_L3_fine_cylinders\"] = \"/groups/sxs/hchaudha/spec_runs/20_set1_L3_fine_cylinders/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"21_set1_L3_fine_cylinders_minExtent\"] = \"/groups/sxs/hchaudha/spec_runs/21_set1_L3_fine_cylinders_minExtent/Ev/Lev3_??/Run/\"\n",
    "\n",
    "# runs_to_plot[\"22_set1_L1_long\"] = \"/groups/sxs/hchaudha/spec_runs/22_set1_L1_long/Ev/Lev1_??/Run/\"\n",
    "# runs_to_plot[\"L1_AC_L3\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L1_AC_L3/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"L1_AC_L2\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L1_AC_L2/Ev/Lev2_??/Run/\"\n",
    "# runs_to_plot[\"L1_AC_L1\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L1_AC_L1/Ev/Lev1_??/Run/\"\n",
    "\n",
    "# runs_to_plot[\"L3_AC_L3_cd_const_high\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_cd_const_high/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L3_cd_const_low\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_cd_const_low/Ev/Lev3_??/Run/\"\n",
    "\n",
    "# runs_to_plot[\"22_L3_AC_L3_no_res_C\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_no_res_C/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"22_L3_AC_L3_res_10_C\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_res_10_C/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L1\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L1/Ev/Lev1_??/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L2\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L2/Ev/Lev2_??/Run/\"\n",
    "# runs_to_plot[\"22_set1_L3_long\"] = \"/groups/sxs/hchaudha/spec_runs/22_set1_L3_long/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L4\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L4/Ev/Lev4_??/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L5\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L5/Ev/Lev5_??/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L6\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L6/Ev/Lev6_??/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L7\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L7/Ev/Lev7_??/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L8\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L8/Ev/Lev8_??/Run/\"\n",
    "\n",
    "# runs_to_plot[\"L3_AC_L3_3_01\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_3_01/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L3_3_02\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_3_02/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L3_5_04\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_5_04/Ev/Lev3_??/Run/\"\n",
    "\n",
    "# runs_to_plot[\"26_set1_L6_long\"] = \"/groups/sxs/hchaudha/spec_runs/26_set1_L6_long/Ev/Lev6_??/Run/\"\n",
    "# runs_to_plot[\"26_main_L6_long\"] = \"/groups/sxs/hchaudha/spec_runs/26_main_L6_long/Ev/Lev6_??/Run/\"\n",
    "\n",
    "# runs_to_plot[\"28_set1_cd_junk_5\"] = \"/groups/sxs/hchaudha/spec_runs/28_set1_cd_junk_5/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"28_set1_cd_junk_1\"] = \"/groups/sxs/hchaudha/spec_runs/28_set1_cd_junk_1/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"28_set1_cd_junk_01\"] = \"/groups/sxs/hchaudha/spec_runs/28_set1_cd_junk_01/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"28_set1_cd_junk_001\"] = \"/groups/sxs/hchaudha/spec_runs/28_set1_cd_junk_001/Ev/Lev3_??/Run/\"\n",
    "\n",
    "# runs_to_plot[\"29_set1_L3_ID_diff_12\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/29_set1_L3_ID_diff_12/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"29_set1_L3_ID_diff_8\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/29_set1_L3_ID_diff_8/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"29_set1_L3_ID_diff_4\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/29_set1_L3_ID_diff_4/Ev/Lev3_??/Run/\"\n",
    "\n",
    "# runs_to_plot[\"29_set1_L3_ID_diff_0\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/29_set1_L3_ID_diff_0/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"29_set1_L3_ID_diff_1\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/29_set1_L3_ID_diff_1/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"29_set1_L3_ID_diff_2\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/29_set1_L3_ID_diff_2/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"29_set1_L3_ID_diff_3\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/29_set1_L3_ID_diff_3/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"29_set1_L3_ID_diff_4_2\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/29_set1_L3_ID_diff_4_2/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"29_set1_L3_ID_diff_4\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/29_set1_L3_ID_diff_4/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"29_set1_L3_ID_diff_5\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/29_set1_L3_ID_diff_5/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"29_set1_L3_ID_diff_6\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/29_set1_L3_ID_diff_6/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"29_set1_L3_ID_diff_7\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/29_set1_L3_ID_diff_7/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"29_set1_L3_ID_diff_8\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/29_set1_L3_ID_diff_8/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"29_set1_L3_ID_diff_12\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/29_set1_L3_ID_diff_12/Ev/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"L1_AC_L1_AK\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L1_AC_L1/Ev/Lev1_AK/Run/\"\n",
    "# runs_to_plot[\"46K_L1_3000_cd10\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/22_segs_res/46K_L1_3000_cd10/Ev/Lev1_AK/Run/\"\n",
    "# runs_to_plot[\"46K_L2_3000_cd10\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/22_segs_res/46K_L2_3000_cd10/Ev/Lev2_AK/Run/\"\n",
    "# runs_to_plot[\"46K_L3_3000_cd10\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/22_segs_res/46K_L3_3000_cd10/Ev/Lev3_AK/Run/\"\n",
    "# runs_to_plot[\"46K_L4_3000_cd10\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/22_segs_res/46K_L4_3000_cd10/Ev/Lev4_AK/Run/\"\n",
    "# runs_to_plot[\"46K_L5_3000_cd10\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/22_segs_res/46K_L5_3000_cd10/Ev/Lev5_AK/Run/\"\n",
    "# runs_to_plot[\"46K_L6_3000_cd10\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/22_segs_res/46K_L6_3000_cd10/Ev/Lev6_AK/Run/\"\n",
    "# runs_to_plot[\"46K_L6_3000_cd100\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/22_segs_res/46K_L6_3000_cd100/Ev/Lev6_AK/Run/\"\n",
    "\n",
    "# runs_to_plot[\"46K_L6_3000_c_AB_2\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/22_segs_res/46K_L6_3000_c_AB_2/Ev/Lev6_AK/Run/\"\n",
    "# runs_to_plot[\"46K_L6_3000_c_AB_4\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/22_segs_res/46K_L6_3000_c_AB_4/Ev/Lev6_AK/Run/\"\n",
    "# runs_to_plot[\"46K_L6_3000_c_AB_6\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/22_segs_res/46K_L6_3000_c_AB_6/Ev/Lev6_AK/Run/\"\n",
    "# runs_to_plot[\"46K_L6_3000_c_AB_10\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/22_segs_res/46K_L6_3000_c_AB_10/Ev/Lev6_AK/Run/\"\n",
    "# runs_to_plot[\"46K_L6_3000_c_AB_12\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/22_segs_res/46K_L6_3000_c_AB_12/Ev/Lev6_AK/Run/\"\n",
    "\n",
    "# runs_to_plot[\"46K_L6_3000_c_AB_4_L\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/22_segs_res/46K_L6_3000_c_AB_4_L/Ev/Lev6_AK/Run/\"\n",
    "# runs_to_plot[\"46K_L6_3000_c_AB_2_L\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/22_segs_res/46K_L6_3000_c_AB_2_L/Ev/Lev6_AK/Run/\"\n",
    "# runs_to_plot[\"46K_L6_3000_c_AB_1_L\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/22_segs_res/46K_L6_3000_c_AB_1_L/Ev/Lev6_AK/Run/\"\n",
    "# runs_to_plot[\"46K_L6_3000_c_AB_1_L2\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/22_segs_res/46K_L6_3000_c_AB_1_L2/Ev/Lev6_AK/Run/\"\n",
    "# runs_to_plot[\"46K_L6_3000_c_AB_0_L2\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/22_segs_res/46K_L6_3000_c_AB_0_L2/Ev/Lev6_AK/Run/\"\n",
    "\n",
    "# runs_to_plot[\"set1_L3_Rn1\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/33_const_inn_dom_runs/set1_L3_Rn1/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"set1_L3_Rn2\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/33_const_inn_dom_runs/set1_L3_Rn2/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"set1_L3_Lmin18\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/33_const_inn_dom_runs/set1_L3_Lmin18/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"set1_L3_Lmin20_Rn2\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/33_const_inn_dom_runs/set1_L3_Lmin20_Rn2/Ev/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"RM_1_Lev3\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/RM_1_Lev3/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"RM_0_Lev6\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/RM_0_test/Ev_456/Lev6_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"30_RM_set1_L1\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/30_RM_set1_L1/Ev/Lev1_A?/Run/\"\n",
    "# runs_to_plot[\"30_RM_set1_L3\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/30_RM_set1_L3/Ev/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"RM_L3s3_k0\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/30_segs_res/L3s3_k0/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"RM_L3s3_k0_cd10\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/30_segs_res/L3s3_k0_cd10/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"RM_L3s3_k0_cd100\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/30_segs_res/L3s3_k0_cd100/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"RM_L3s4_k0\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/30_segs_res/L3s4_k0/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"RM_L3s5_k0\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/30_segs_res/L3s5_k0/Ev/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"RM_0_Lev6\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/RM_0_test/Ev_456/Lev6_A?/Run/\"\n",
    "# runs_to_plot[\"RM_1_Lev3\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/RM_1_Lev3/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"RM_1_Lev6\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/RM_1_Lev6/Ev/Lev6_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"31_RM_set1_L1\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/31_RM_set1_L1/Ev/Lev1_A?/Run/\"\n",
    "# runs_to_plot[\"31_RM_set1_L2\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/31_RM_set1_L2/Ev/Lev2_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"31_segs_L1s0\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/31_segs/L1s0/Ev/Lev0_AC/Run/\"\n",
    "# runs_to_plot[\"31_segs_L1s1\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/31_segs/L1s1/Ev/Lev1_AC/Run/\"\n",
    "# runs_to_plot[\"31_segs_L1s2\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/31_segs/L1s2/Ev/Lev2_AC/Run/\"\n",
    "# runs_to_plot[\"31_segs_L1s3\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/31_segs/L1s3/Ev/Lev3_AC/Run/\"\n",
    "\n",
    "# runs_to_plot[\"L1s3_cdg1_10\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/31_segs/L1s3_cdg1_10/Ev/Lev3_AC/Run/\"\n",
    "# runs_to_plot[\"L1s3_cdg1_100\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/31_segs/L1s3_cdg1_100/Ev/Lev3_AC/Run/\"\n",
    "# runs_to_plot[\"L1s3_cdg1_250\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/31_segs/L1s3_cdg1_250/Ev/Lev3_AC/Run/\"\n",
    "\n",
    "# runs_to_plot[\"set1_L3_Rn1\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/33_const_inn_dom_runs/set1_L3_Rn1/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"set1_L3_Rn2\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/33_const_inn_dom_runs/set1_L3_Rn2/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"set1_L3_Lmin18\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/33_const_inn_dom_runs/set1_L3_Lmin18/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"set1_L3_Lmin20_Rn2\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/33_const_inn_dom_runs/set1_L3_Lmin20_Rn2/Ev/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"main_L6_AM_ode_MQOS\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/26_segs_res/main_L6_AM_ode_MQOS/Ev/Lev6_A?/Run/\"\n",
    "# runs_to_plot[\"set1_L6_AK_ode_MQOS\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/26_segs_res/set1_L6_AK_ode_MQOS/Ev/Lev6_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"34_master_L16_1\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/34_master_L16/Ev/Lev1_A?/Run/\"\n",
    "# runs_to_plot[\"34_master_L16_2\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/34_master_L16/Ev/Lev2_A?/Run/\"\n",
    "# runs_to_plot[\"34_master_L16_3\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/34_master_L16/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"34_master_L16_4\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/34_master_L16/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"34_master_L16_5\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/34_master_L16/Ev/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"34_master_L16_6\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/34_master_L16/Ev/Lev6_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"set1_L6_AG_cd_0100\"] = \"/groups/sxs/hchaudha/spec_runs/26_segs/set1_L6_AG_cd_0100/Ev/Lev6_A[G-Z]/Run/\"\n",
    "# runs_to_plot[\"set1_L6_AG_cd_100\"] = \"/groups/sxs/hchaudha/spec_runs/26_segs/set1_L6_AG_cd_100/Ev/Lev6_A[G-Z]/Run/\"\n",
    "# runs_to_plot[\"set1_L6_AK_cd_0100\"] = \"/groups/sxs/hchaudha/spec_runs/26_segs/set1_L6_AK_cd_0100/Ev/Lev6_A[K-Z]/Run/\"\n",
    "# runs_to_plot[\"set1_L6_AK_cd_100\"] = \"/groups/sxs/hchaudha/spec_runs/26_segs/set1_L6_AK_cd_100/Ev/Lev6_A[K-Z]/Run/\"\n",
    "# runs_to_plot[\"set1_L6_AK_S2_L20\"] = \"/groups/sxs/hchaudha/spec_runs/26_segs/set1_L6_AK_S2_L20/Ev/Lev6_A[K-Z]/Run/\"\n",
    "# runs_to_plot[\"set1_L6_AK_S2_L20_cd_10\"] = \"/groups/sxs/hchaudha/spec_runs/26_segs/set1_L6_AK_S2_L20_cd_10/Ev/Lev6_A[K-Z]/Run/\"\n",
    "# runs_to_plot[\"set1_L6_AK_S2_L20_cd_100\"] = \"/groups/sxs/hchaudha/spec_runs/26_segs/set1_L6_AK_S2_L20_cd_100/Ev/Lev6_A[K-Z]/Run/\"\n",
    "# runs_to_plot[\"set1_L6_AK_S2_L20_cd_1000\"] = \"/groups/sxs/hchaudha/spec_runs/26_segs/set1_L6_AK_S2_L20_cd_1000/Ev/Lev6_A[K-Z]/Run/\"\n",
    "# runs_to_plot[\"set1_L6_AK_S2_L20_cd_10000\"] = \"/groups/sxs/hchaudha/spec_runs/26_segs/set1_L6_AK_S2_L20_cd_10000/Ev/Lev6_A[K-Z]/Run/\"\n",
    "# runs_to_plot[\"set1_L6_AK_S2_L20_no_Rmin\"] = \"/groups/sxs/hchaudha/spec_runs/26_segs/set1_L6_AK_S2_L20_no_Rmin/Ev/Lev6_A[K-Z]/Run/\"\n",
    "\n",
    "# runs_to_plot[\"set1_L6_AG_cd_0100\"] = \"/groups/sxs/hchaudha/spec_runs/26_segs/set1_L6_AG_cd_0100/Ev/Lev6_A[G-Z]/Run/\"\n",
    "# runs_to_plot[\"set1_L6_AG_cd_100\"] = \"/groups/sxs/hchaudha/spec_runs/26_segs/set1_L6_AG_cd_100/Ev/Lev6_A[G-Z]/Run/\"\n",
    "# runs_to_plot[\"set1_L6_AK_cd_0100\"] = \"/groups/sxs/hchaudha/spec_runs/26_segs/set1_L6_AK_cd_0100/Ev/Lev6_A[K-Z]/Run/\"\n",
    "# runs_to_plot[\"set1_L6_AK_cd_100\"] = \"/groups/sxs/hchaudha/spec_runs/26_segs/set1_L6_AK_cd_100/Ev/Lev6_A[K-Z]/Run/\"\n",
    "# runs_to_plot[\"set1_L6_AK_S2_L20\"] = \"/groups/sxs/hchaudha/spec_runs/26_segs/set1_L6_AK_S2_L20/Ev/Lev6_A[K-Z]/Run/\"\n",
    "# runs_to_plot[\"set1_L6_AK_S2_L20_no_Rmin\"] = \"/groups/sxs/hchaudha/spec_runs/26_segs/set1_L6_AK_S2_L20_no_Rmin/Ev/Lev6_A[K-Z]/Run/\"\n",
    "\n",
    "# runs_to_plot[\"set1_L6_AG_cd_0100\"] = \"/groups/sxs/hchaudha/spec_runs/26_segs/set1_L6_AG_cd_0100/Ev/Lev6_??/Run/\"\n",
    "# runs_to_plot[\"set1_L6_AG_cd_100\"] = \"/groups/sxs/hchaudha/spec_runs/26_segs/set1_L6_AG_cd_100/Ev/Lev6_??/Run/\"\n",
    "# runs_to_plot[\"set1_L6_AK_cd_0100\"] = \"/groups/sxs/hchaudha/spec_runs/26_segs/set1_L6_AK_cd_0100/Ev/Lev6_??/Run/\"\n",
    "# runs_to_plot[\"set1_L6_AK_cd_100\"] = \"/groups/sxs/hchaudha/spec_runs/26_segs/set1_L6_AK_cd_100/Ev/Lev6_??/Run/\"\n",
    "# runs_to_plot[\"set1_L6_AK_S2_L20\"] = \"/groups/sxs/hchaudha/spec_runs/26_segs/set1_L6_AK_S2_L20/Ev/Lev6_??/Run/\"\n",
    "# runs_to_plot[\"set1_L6_AK_S2_L20_no_Rmin\"] = \"/groups/sxs/hchaudha/spec_runs/26_segs/set1_L6_AK_S2_L20_no_Rmin/Ev/Lev6_??/Run/\"\n",
    "\n",
    "# runs_to_plot[\"L3_AC_L3_minL17\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_minL17/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L3_minL19\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_minL19/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L3_minL21\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_minL21/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L3_01_cd_asymp\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_01_cd_asymp/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L3_single_exp\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_single_exp/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L3_single_Exp_large_sigma\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_single_Exp_large_sigma/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L3_cd_const_low\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_cd_const_low/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L3_cd_const_high\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_cd_const_high/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L3_sigma2\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_sigma2/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L3_sigma05\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_sigma05/Ev/Lev3_??/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L3_sigma1_const\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_sigma1_const/Ev/Lev3_??/Run/\"\n",
    "\n",
    "# runs_to_plot[\"L3_AC_L3_AB_L8\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_AB_L8/Ev/Lev3_AC/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L3_AB_R7\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_AB_R7/Ev/Lev3_AC/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L3_AB0_L16\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_AB0_L16/Ev/Lev3_AC/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L3_AB0_L15\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_AB0_L15/Ev/Lev3_AC/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L3_ps_10\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_ps_10/Ev/Lev3_AC/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L3_ps_01\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_ps_01/Ev/Lev3_AC/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L3_BCSC_8\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_BCSC_8/Ev/Lev3_AC/Run/\"\n",
    "# runs_to_plot[\"L3_AC_L3_BCSC_12\"] = \"/groups/sxs/hchaudha/spec_runs/22_segs/L3_AC_L3_BCSC_12/Ev/Lev3_AC/Run/\"\n",
    "\n",
    "# runs_to_plot[\"119_gd_SUKS_3_20\"] = \"/net/panfs/SXS/himanshu/gauge_stuff/gauge_driver_runs/runs/119_gd_SUKS_3_20/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"120W_gd_SUKS1_3_20\"] = \"/net/panfs/SXS/himanshu/gauge_stuff/gauge_driver_runs/runs/120W_gd_SUKS1_3_20/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"120_gd_SUKS1_3_20\"] = \"/net/panfs/SXS/himanshu/gauge_stuff/gauge_driver_runs/runs/120_gd_SUKS1_3_20/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"119_gd_SUKS_3_20\"] = \"/net/panfs/SXS/himanshu/gauge_stuff/gauge_driver_runs/runs/119_gd_SUKS_3_20/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"67_master_mr3\"] = \"/net/panfs/SXS/himanshu/gauge_stuff/gauge_driver_runs/runs/67_master_mr3/Ev/Lev1_A?/Run/\"\n",
    "# runs_to_plot[\"119_gd_SUKS_3_20\"] = \"/net/panfs/SXS/himanshu/gauge_stuff/gauge_driver_runs/runs/119_gd_SUKS_3_20/Ev/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"34_master_L16\"] = \"/resnick/groups/sxs/hchaudha/spec_runs/34_master_L16/Ev/Lev6_A?/Run/\"\n",
    "\n",
    "# data_file_path = \"ConstraintNorms/GhCe.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCeExt.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCeExt_L2.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCeExt_Norms.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCe_L2.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCe_Linf.dat\"\n",
    "# data_file_path = \"ConstraintNorms/Linf.dat\"\n",
    "# data_file_path = \"ConstraintNorms/Constraints_Linf.dat\"\n",
    "# data_file_path = \"ConstraintNorms/NormalizedGhCe_Linf.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCe_Norms.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCe_VolL2.dat\"\n",
    "# data_file_path = \"ConstraintNorms/NormalizedGhCe_Linf.dat\"\n",
    "# data_file_path = \"ConstraintNorms/NormalizedGhCe_Norms.dat\"\n",
    "# data_file_path = \"CharSpeedNorms/CharSpeeds_Min_SliceLFF.SphereA0.dat\"\n",
    "# data_file_path = \"MinimumGridSpacing.dat\"\n",
    "# data_file_path = \"GrAdjustMaxTstepToDampingTimes.dat\"\n",
    "# data_file_path = \"GrAdjustSubChunksToDampingTimes.dat\"\n",
    "# data_file_path = \"DiagAhSpeedA.dat\"\n",
    "# data_file_path = \"ApparentHorizons/AhA.dat\"\n",
    "# data_file_path = \"ApparentHorizons/AhB.dat\" \n",
    "# data_file_path = \"ApparentHorizons/MinCharSpeedAhA.dat\"\n",
    "# data_file_path = \"ApparentHorizons/RescaledRadAhA.dat\"\n",
    "# data_file_path = \"ApparentHorizons/AhACoefs.dat\"\n",
    "# data_file_path = \"ApparentHorizons/AhBCoefs.dat\"\n",
    "data_file_path = \"ApparentHorizons/Trajectory_AhB.dat\"\n",
    "# data_file_path = \"ApparentHorizons/HorizonSepMeasures.dat\"\n",
    "\n",
    "# data_file_path = \"ApparentHorizons/Horizons.h5@AhA\"\n",
    "# data_file_path = \"ApparentHorizons/Horizons.h5@AhB\"\n",
    "# data_file_path = \"TStepperDiag.dat\"\n",
    "# data_file_path = \"TimeInfo.dat\"\n",
    "# data_file_path = \"Hist-FuncSkewAngle.txt\"\n",
    "# data_file_path = \"Hist-FuncCutX.txt\"\n",
    "# data_file_path = \"Hist-FuncExpansionFactor.txt\"\n",
    "# data_file_path = \"Hist-FuncLambdaFactorA0.txt\"\n",
    "# data_file_path = \"Hist-FuncLambdaFactorA.txt\"\n",
    "# data_file_path = \"Hist-FuncLambdaFactorB0.txt\"\n",
    "# data_file_path = \"Hist-FuncLambdaFactorB.txt\"\n",
    "# data_file_path = \"Hist-FuncQuatRotMatrix.txt\"\n",
    "# data_file_path = \"Hist-FuncSkewAngle.txt\"\n",
    "# data_file_path = \"Hist-FuncSmoothCoordSep.txt\"\n",
    "# data_file_path = \"Hist-FuncSmoothMinDeltaRNoLam00AhA.txt\"\n",
    "# data_file_path = \"Hist-FuncSmoothMinDeltaRNoLam00AhB.txt\"\n",
    "# data_file_path = \"Hist-FuncSmoothRAhA.txt\"\n",
    "# data_file_path = \"Hist-FuncSmoothRAhB.txt\"\n",
    "# data_file_path = \"Hist-FuncTrans.txt\"\n",
    "# data_file_path = \"Hist-GrDomain.txt\"\n",
    "# data_file_path = \"Profiler.h5\"\n",
    "column_names, runs_data_dict = load_data_from_levs(runs_to_plot,data_file_path)\n",
    "print(column_names)\n",
    "print(runs_data_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_avg_len=0\n",
    "save_path = None\n",
    "diff_base = None\n",
    "constant_shift_val_time = None\n",
    "plot_abs_diff = False\n",
    "y_axis_list = None\n",
    "x_axis = 't(M)'\n",
    "\n",
    "plot_abs_diff = True\n",
    "# diff_base = '29_set1_L3_ID_diff_8'\n",
    "# diff_base = '6_set1_L3s3'\n",
    "# diff_base = '46K_L6_3000_cd10'\n",
    "# diff_base = '22_set1_L3_long'\n",
    "# diff_base = 'L3_AC_L8'\n",
    "# diff_base = 'L1_AC_L3'\n",
    "# diff_base = 'high_accuracy_L5'\n",
    "# diff_base = 'high_accuracy_L5_main'\n",
    "# diff_base = 'Lev5_big_gaussian_ah_tol100'\n",
    "diff_base = '6_set1_L6s5'\n",
    "# diff_base = '6_set1_L3s3'\n",
    "# diff_base = '34_master_L16_5'\n",
    "# add_max_and_min_val(runs_data_dict)\n",
    "# y_axis = 'max_val'\n",
    "# y_axis = 'min_val'\n",
    "\n",
    "# constant_shift_val_time = 7206\n",
    "# constant_shift_val_time = 1200\n",
    "\n",
    "# y_axis = 'Linf(GhCe)'\n",
    "# y_axis = 'L2(GhCe)'\n",
    "# y_axis = 'VolLp(GhCe)'\n",
    "# y_axis = 'VolLp(GhCeExt)'\n",
    "# y_axis = 'Linf(GhCeExt)'\n",
    "# y_axis = 'L2(NormalizedGhCe)'\n",
    "# y_axis = 'Linf(NormalizedGhCe)'\n",
    "# y_axis = 'VolLp(NormalizedGhCe)'\n",
    "# y_axis = 'CharSpeed'\n",
    "# y_axis = 'Linf(GhCeExt)'\n",
    "y_axis = 'Linf(NormalizedGhCe) on SphereA4'\n",
    "y_axis = 'Linf(NormalizedGhCe) on SphereB0'\n",
    "# y_axis = 'Linf(NormalizedGhCe) on SphereA3'\n",
    "y_axis = 'Linf(NormalizedGhCe) on SphereC1'\n",
    "# y_axis = 'Linf(NormalizedGhCe) on SphereC6'\n",
    "# y_axis = 'Linf(NormalizedGhCe) on CylinderCB1.0.0'\n",
    "# y_axis = 'Linf(NormalizedGhCe) on CylinderEB1.0.0'\n",
    "# y_axis = 'Linf(NormalizedGhCe) on CylinderSMB1.0'\n",
    "# y_axis = 'Linf(NormalizedGhCe) on CylinderSMB0.0'\n",
    "# y_axis = 'Linf(NormalizedGhCe) on FilledCylinderCA0'\n",
    "# y_axis = 'Linf(NormalizedGhCe) on FilledCylinderCA1'\n",
    "# y_axis = 'Linf(GhCe) on SphereA6'\n",
    "# y_axis = 'Linf(GhCe) on SphereA0'\n",
    "y_axis = 'Linf(GhCe) on SphereC6'\n",
    "y_axis = 'Linf(GhCe) on SphereC28' \n",
    "# y_axis = 'Linf(GhCe) on SphereC49' \n",
    "# y_axis = 'Linf(GhCe) on CylinderCA0.0.0'\n",
    "# y_axis = 'Linf(GhCe) on CylinderSMB1.0'\n",
    "# y_axis = 'Linf(GhCe) on FilledCylinderMB0'\n",
    "# y_axis = 'Linf(GhCe) on FilledCylinderEA0'\n",
    "# y_axis = 'Linf(GhCe) on FilledCylinderCA0'\n",
    "# y_axis = 'Linf(GhCe) on FilledCylinderCA1'\n",
    "# y_axis = 'Linf(1Conz) on SphereC0'\n",
    "# y_axis = 'Linf(3Conyxx) on SphereC12'\n",
    "# y_axis = 'Linf(2Conxx) on SphereC12'\n",
    "# y_axis = 'MinimumGridSpacing[CylinderCB1.0.0]'\n",
    "# y_axis = 'MinimumGridSpacing[SphereA0]'\n",
    "# y_axis = 'MinimumGridSpacing[SphereC0]'\n",
    "# y_axis = 'MinimumGridSpacing[SphereB0]'\n",
    "\n",
    "# y_axis = 'Linf(sqrt(psiErr^2)) on SphereC8'\n",
    "# y_axis = 'Linf(sqrt(psiErr^2)) on SphereC15'\n",
    "# y_axis = 'Linf(sqrt(kappaErr^2)) on SphereC0'\n",
    "# y_axis = 'Linf(sqrt(kappaErr^2)) on SphereC6'\n",
    "# y_axis = 'Linf(GhCe) on SphereC15'\n",
    "# y_axis = 'Linf(GhCe) on SphereD0'\n",
    "# y_axis = 'Linf(NormalizedGhCe) on SphereE5'\n",
    "# y_axis = 'Linf(GhCe) on SphereC8'\n",
    "# y_axis = 'Linf(GhCe) on SphereE5'\n",
    "# y_axis = 'Linf(sqrt(3Con^2)) on SphereD1'\n",
    "# y_axis = 'dt'\n",
    "\n",
    "\n",
    "# y_axis = 'Linf(3Conzzz) on CylinderSMB0.0'\n",
    "\n",
    "# y_axis = 'MPI::MPwait_cum'\n",
    "# x_axis = 't'\n",
    "# y_axis = 'T [hours]'\n",
    "# y_axis = 'dt/dT'\n",
    "\n",
    "# x_axis = 't'\n",
    "# y_axis = 'da'\n",
    "# y_axis = 'CoordSepHorizons'\n",
    "# y_axis = 'ProperSepHorizons'\n",
    "# y_axis = 'Tx'\n",
    "# y_axis = 'SmoothCoordSep'\n",
    "# y_axis = 'dSmoothCoordSep'\n",
    "# y_axis = 'd2SmoothCoordSep'\n",
    "# y_axis = 'InertialCenter_x'\n",
    "# y_axis = 'InertialCenter_y'\n",
    "# y_axis = 'InertialCenter_z'\n",
    "# y_axis = 'SphereC5_L'\n",
    "# y_axis = 'SphereA0_L'\n",
    "# y_axis = 'SphereC29_R'\n",
    "# y_axis = 'FilledCylinderMB0_R'\n",
    "# y_axis = 'FilledCylinderMB0_M'\n",
    "# y_axis = 'FilledCylinderMB0_L'\n",
    "# y_axis = 'MinimumGridSpacing[SphereA0]'\n",
    "# y_axis = 'MinimumGridSpacing[CylinderSMB0.0]'\n",
    "# y_axis = 'dt/dT'\n",
    "\n",
    "# x_axis = 'time'\n",
    "# y_axis = 'NumIterations'\n",
    "# y_axis = 'Residual'\n",
    "# y_axis = 'ArealMass'\n",
    "y_axis = 'ChristodoulouMass'\n",
    "# y_axis = 'CoordCenterInertial_0'\n",
    "# y_axis = 'CoordSpinChiInertial_2'\n",
    "# y_axis = 'CoordSpinChiMagInertial'\n",
    "# y_axis = 'DimensionfulInertialCoordSpin_0'\n",
    "# y_axis = 'DimensionfulInertialCoordSpinMag'\n",
    "# y_axis = 'DimensionfulInertialSpin_0'\n",
    "# y_axis = 'DimensionfulInertialSpinMag'\n",
    "# y_axis = 'SpinFromShape_2'\n",
    "# y_axis = 'chiMagInertial'\n",
    "# y_axis = 'max(r)'\n",
    "# y_axis = 'min(r)'\n",
    "\n",
    "\n",
    "# y_axis = 'courant factor'\n",
    "# y_axis = 'error/1e-08'\n",
    "# y_axis = 'NfailedSteps'\n",
    "# y_axis = 'NumRhsEvaluations in this segment'\n",
    "# y_axis = 'dt'\n",
    "# y_axis = 'FilledCylinderMA1_L'\n",
    "# y_axis = 'SphereC6_L'\n",
    "# y_axis = 'SphereA0_R'\n",
    "y_axis = 'InertialCenter_x'\n",
    "\n",
    "minT = 0\n",
    "# minT = 85\n",
    "# minT = 470\n",
    "# minT = 1100\n",
    "# minT = 1400\n",
    "# minT = 3000\n",
    "# minT = 3200\n",
    "# minT = 4000\n",
    "# minT = 6500\n",
    "# minT = 7200\n",
    "# minT = 9260\n",
    "# minT = 5266\n",
    "# minT = 9700\n",
    "# minT = 46500\n",
    "\n",
    "maxT = 400000\n",
    "# maxT = minT+30\n",
    "# maxT = 700\n",
    "# maxT = 1200\n",
    "# maxT = 4000\n",
    "# moving_avg_len = 50\n",
    "# moving_avg_len = 10\n",
    "# maxT = 47100\n",
    "# maxT = 46600\n",
    "\n",
    "\n",
    "# y_axis_list = [\"SphereC0_L\",\"SphereC0_R\"]\n",
    "# y_axis_list = [f\"SphereC{i}_R\" for i in range(30)]\n",
    "# y_axis_list = [\"SphereC0_L\",\"SphereC1_L\",\"SphereC2_L\",\"SphereC4_L\",\"SphereC8_L\",\"SphereC16_L\",\"SphereC29_L\"]\n",
    "# y_axis_list = [\"SphereC4_L\",\"SphereC16_L\",\"SphereC29_L\"]\n",
    "# y_axis_list = [f'Linf(GhCe) on SphereA{i}' for i in [0]]\n",
    "# y_axis_list = [\"SphereC0_R\",'CylinderSMA0.0_R','FilledCylinderMA0_R','SphereA0_R']\n",
    "# y_axis_list = ['SphereA0_L','SphereA1_L','SphereA2_L','SphereA3_L','SphereA4_L']\n",
    "# y_axis_list = ['Linf(GhCe) on CylinderSMA0.0','Linf(GhCe) on FilledCylinderMA0','Linf(GhCe) on SphereA0']\n",
    "# y_axis_list = [\n",
    "#   'Linf(NormalizedGhCe) on SphereA0',\n",
    "  # 'Linf(NormalizedGhCe) on SphereA1',\n",
    "  # 'Linf(NormalizedGhCe) on SphereA2',\n",
    "  # 'Linf(NormalizedGhCe) on SphereA3',\n",
    "  # 'Linf(NormalizedGhCe) on SphereA4',\n",
    "  # 'Linf(NormalizedGhCe) on CylinderSMA0.0',29_set1_L3_ID_diff_0\n",
    "  # 'Linf(NormalizedGhCe) on FilledCylinderMA0',\n",
    "  # 'Linf(NormalizedGhCe) on SphereC0',\n",
    "  # 'Linf(NormalizedGhCe) on SphereC1',\n",
    "  # 'Linf(NormalizedGhCe) on SphereC2',\n",
    "  # 'Linf(NormalizedGhCe) on SphereC4',\n",
    "  # 'Linf(NormalizedGhCe) on SphereC8',\n",
    "  # 'Linf(NormalizedGhCe) on SphereC12',\n",
    "  # 'Linf(NormalizedGhCe) on SphereC16',\n",
    "  # 'Linf(NormalizedGhCe) on SphereC20',\n",
    "  # 'Linf(NormalizedGhCe) on SphereC24',\n",
    "  # 'Linf(NormalizedGhCe) on SphereC28',\n",
    "  # ]\n",
    "# y_axis_list = [f'Linf(1Con{v}) on SphereC0' for v in ['t','x','y','z']]\n",
    "# y_axis_list = [f'Linf(sqrt(kappaErr^2)) on SphereC{i}' for i in range(0,12)]\n",
    "# y_axis_list = [f'Linf(NormalizedGhCe) on SphereC{i}' for i in range(5,45,10)]\n",
    "# y_axis_list = [f'Linf(NormalizedGhCe) on SphereA{i}' for i in range(0,5)]\n",
    "# y_axis_list = [f'Linf(GhCe) on SphereA{i}' for i in range(0,5)]\n",
    "# y_axis_list = [f'Linf(GhCe) on SphereC{i}' for i in range(55)]\n",
    "# y_axis_list = ['MinimumGridSpacing[CylinderSMA0.0]','MinimumGridSpacing[FilledCylinderMA0]','MinimumGridSpacing[SphereA0]']\n",
    "# y_axis_list = [i for i in column_names if ('SphereA' in i)]\n",
    "# y_axis_inc_list = [f\"SphereC{i}$\" for i in range(0,45,5)]\n",
    "# y_axis_list = []\n",
    "# for col in column_names:\n",
    "#   for i in y_axis_inc_list :\n",
    "#     if re.search(i,col):\n",
    "#       y_axis_list.append(col)\n",
    "# print(y_axis_list)\n",
    "\n",
    "plot_fun = lambda x,y,label : plt.plot(x,y,label=label)\n",
    "# plot_fun = lambda x,y,label : plt.plot(x,y,label=label,marker='x')\n",
    "plot_fun = lambda x,y,label : plt.semilogy(x,y,label=label)\n",
    "\n",
    "# plot_fun = lambda x,y,label : plt.semilogy(x,y,label=label,marker='x')\n",
    "# plot_fun = lambda x,y,label : plt.loglog(x,y,label=label) \n",
    "# plot_fun = lambda x,y,label : plt.scatter(x,y,label=label,s=10,marker=\"x\",alpha=0.4)\n",
    "# save_path = \"/groups/sxs/hchaudha/rough/high_acc_plots/\"\n",
    "# save_path = \"/groups/sxs/hchaudha/rough/plots/\"\n",
    "# save_path = \"/home/hchaudha/notes/spec_accuracy/figures/\"\n",
    "# save_path = \"/home/hchaudha/notes/spec_accuracy/L5_comparisons/\"\n",
    "# save_path = \"/home/hchaudha/notes/spec_accuracy/L5_comparisons/L15_no_tol/\"\n",
    "legend_dict = {}\n",
    "for key in runs_data_dict.keys():\n",
    "  legend_dict[key] = None\n",
    "\n",
    "# legend_dict = {\n",
    "#     'high_accuracy_L1_main':\"Old Level 1\",\n",
    "#     'high_accuracy_L2_main':\"Old Level 2\",\n",
    "#     'high_accuracy_L3_main':\"Old Level 3\",\n",
    "#     'high_accuracy_L4_main':\"Old Level 4\",\n",
    "#     'high_accuracy_L5_main':\"Old Level 5\",\n",
    "#     '6_set1_L6s1':'New Level 1',\n",
    "#     '6_set1_L6s2':'New Level 2',\n",
    "#     '6_set1_L6s3':'New Level 3',\n",
    "#     '6_set1_L6s4':'New Level 4',\n",
    "#     '6_set1_L6s5':'New Level 5',\n",
    "#     'high_accuracy_L1':\"New Level 1\",\n",
    "#     'high_accuracy_L2':\"New Level 2\",\n",
    "#     'high_accuracy_L3':\"New Level 3\",\n",
    "#     'high_accuracy_L4':\"New Level 4\",\n",
    "#     'high_accuracy_L5':\"New Level 5\",\n",
    "#  }\n",
    "\n",
    "append_to_title = \"\"\n",
    "if '@' in data_file_path:\n",
    "  append_to_title = \" HorizonBH=\"+data_file_path.split('@')[-1]\n",
    "\n",
    "# with plt.style.context('default'):\n",
    "with plt.style.context('ggplot'):\n",
    "    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    line_styles = ['-', '--', '-.', ':']\n",
    "    combined_cycler = cycler(linestyle=line_styles)*cycler(color=colors)\n",
    "    plt.rcParams['axes.prop_cycle'] = combined_cycler\n",
    "#   plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "#   plt.rcParams[\"figure.figsize\"] = (4,4)\n",
    "    plt.rcParams[\"figure.figsize\"] = (10,8)\n",
    "    #   plt.rcParams[\"figure.figsize\"] = (6,6)\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "    # plt.ylim(1e-10,1e-4)\n",
    "    if y_axis_list is None:\n",
    "        plot_graph_for_runs(runs_data_dict, x_axis, y_axis, minT, maxT, legend_dict=legend_dict, save_path=save_path, moving_avg_len=moving_avg_len, plot_fun=plot_fun, diff_base=diff_base, plot_abs_diff=plot_abs_diff,constant_shift_val_time=constant_shift_val_time,append_to_title=append_to_title)\n",
    "    else:\n",
    "        plot_graph_for_runs_wrapper(runs_data_dict, x_axis, y_axis_list, minT, maxT, legend_dict=legend_dict, save_path=save_path, moving_avg_len=moving_avg_len, plot_fun=plot_fun, diff_base=diff_base, plot_abs_diff=plot_abs_diff,constant_shift_val_time=constant_shift_val_time,append_to_title=append_to_title)\n",
    "\n",
    "\n",
    "#   plt.title(\"\")\n",
    "#   plt.ylabel(\"Constraint Violations near black holes\")\n",
    "#   plt.tight_layout()\n",
    "#   plt.legend(loc='upper right')\n",
    "#   plt.ylim(1e-8, 1e-5)\n",
    "#   plt.ylim(1e-12, 1e-6)\n",
    "#   save_name = \"main_ode_impro_const_new_no_avg.png\"\n",
    "\n",
    "#   save_name = Path(f\"/groups/sxs/hchaudha/scripts/report/figures/{save_name}\")\n",
    "#   if save_name.exists():\n",
    "#     raise Exception(\"Change name\")\n",
    "#   plt.savefig(save_name,dpi=600)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intergrate over something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_data(y,t, t_new):\n",
    "    return CubicSpline(t, y, extrapolate=False)(t_new)\n",
    "\n",
    "diff_base = diff_base\n",
    "\n",
    "t = runs_data_dict[diff_base][\"t(M)\"]\n",
    "filter_t = (t >= 1200) & (t <= 4000)\n",
    "\n",
    "runs_data_dict[diff_base].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_int = {}\n",
    "\n",
    "# y_axis = y_axis\n",
    "y_axis = 'InertialCenter_y'\n",
    "\n",
    "sorted_keys = sorted(list(runs_data_dict.keys()))\n",
    "for key1,key2 in itertools.combinations(sorted_keys,2):\n",
    "    y1 = runs_data_dict[key1][y_axis]\n",
    "    t1 = runs_data_dict[key1][\"t(M)\"]\n",
    "    y2 = runs_data_dict[key2][y_axis]\n",
    "    t2 = runs_data_dict[key2][\"t(M)\"]\n",
    "\n",
    "    diff = interpolate_data(y1,t1,t) - interpolate_data(y2,t2,t)\n",
    "\n",
    "    diff = np.abs(diff)\n",
    "    diff_int[key1+\"@\"+key2] = sp.integrate.simpson(diff, t)\n",
    "\n",
    "\n",
    "diff_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff_int_dict = {\n",
    "#     \"6_set1_L3s3@6_set1_L6s3\": 0.5904914314854431,\n",
    "#     \"6_set1_L3s3@set1_L3_Lmin18\": 0.07586265249510982,\n",
    "#     \"6_set1_L3s3@set1_L3_Lmin20_Rn2\": 0.14597432777822972,\n",
    "#     \"6_set1_L3s3@set1_L3_Rn1\": 0.16399535217293953,\n",
    "#     \"6_set1_L3s3@set1_L3_Rn2\": 0.7631437242931903,\n",
    "#     \"6_set1_L6s3@set1_L3_Lmin18\": 0.5829596680990325,\n",
    "#     \"6_set1_L6s3@set1_L3_Lmin20_Rn2\": 0.5555129757999464,\n",
    "#     \"6_set1_L6s3@set1_L3_Rn1\": 0.6738907077920757,\n",
    "#     \"6_set1_L6s3@set1_L3_Rn2\": 0.5124556155924358,\n",
    "#     \"set1_L3_Lmin18@set1_L3_Lmin20_Rn2\": 0.07161362180244356,\n",
    "#     \"set1_L3_Lmin18@set1_L3_Rn1\": 0.23708175332892933,\n",
    "#     \"set1_L3_Lmin18@set1_L3_Rn2\": 0.6937645674370789,\n",
    "#     \"set1_L3_Lmin20_Rn2@set1_L3_Rn1\": 0.3063443176736044,\n",
    "#     \"set1_L3_Lmin20_Rn2@set1_L3_Rn2\": 0.6293492770902831,\n",
    "#     \"set1_L3_Rn1@set1_L3_Rn2\": 0.9241188942694976,\n",
    "# }\n",
    "\n",
    "# mistmatch_dict = {\n",
    "#     \"6_set1_L3s3_250@6_set1_L6s3_250\": 8.263000101741527e-08,\n",
    "#     \"6_set1_L3s3_250@set1_L3_Lmin18_0250\": 4.944271141896937e-09,\n",
    "#     \"6_set1_L3s3_250@set1_L3_Lmin20_Rn2_0250\": 2.9353787444230103e-08,\n",
    "#     \"6_set1_L3s3_250@set1_L3_Rn1_0250\": 2.2413967635607665e-08,\n",
    "#     \"6_set1_L3s3_250@set1_L3_Rn2_0250\": 5.089199756428868e-07,\n",
    "#     \"6_set1_L6s3_250@set1_L3_Lmin18_0250\": 4.9261210443930505e-08,\n",
    "#     \"6_set1_L6s3_250@set1_L3_Lmin20_Rn2_0250\": 3.6005761619160595e-08,\n",
    "#     \"6_set1_L6s3_250@set1_L3_Rn1_0250\": 1.8347656387738748e-07,\n",
    "#     \"6_set1_L6s3_250@set1_L3_Rn2_0250\": 1.9460927976791162e-07,\n",
    "#     \"set1_L3_Lmin18_0250@set1_L3_Lmin20_Rn2_0250\": 1.5678881874531597e-08,\n",
    "#     \"set1_L3_Lmin18_0250@set1_L3_Rn1_0250\": 4.5833091451745286e-08,\n",
    "#     \"set1_L3_Lmin18_0250@set1_L3_Rn2_0250\": 4.185693935451287e-07,\n",
    "#     \"set1_L3_Lmin20_Rn2_0250@set1_L3_Rn1_0250\": 9.435300648070634e-08,\n",
    "#     \"set1_L3_Lmin20_Rn2_0250@set1_L3_Rn2_0250\": 3.213837766369929e-07,\n",
    "#     \"set1_L3_Rn1_0250@set1_L3_Rn2_0250\": 7.330173252470396e-07,\n",
    "# }\n",
    "\n",
    "diff_int_dict = diff_int\n",
    "\n",
    "mistmatch_dict = {\n",
    "    \"6_set1_L3s0_350@6_set1_L3s1_350\": 2.7274773134911763e-07,\n",
    "    \"6_set1_L3s0_350@6_set1_L3s2_350\": 3.776543475116111e-07,\n",
    "    \"6_set1_L3s0_350@6_set1_L3s3_350\": 1.0060113403446269e-07,\n",
    "    \"6_set1_L3s0_350@6_set1_L6s0_350\": 9.01936680386715e-08,\n",
    "    \"6_set1_L3s0_350@6_set1_L6s1_350\": 9.515063704038605e-08,\n",
    "    \"6_set1_L3s0_350@6_set1_L6s2_350\": 1.519169055597966e-07,\n",
    "    \"6_set1_L3s0_350@6_set1_L6s3_350\": 2.716959231705252e-08,\n",
    "    \"6_set1_L3s0_350@6_set1_L6s4_350\": 2.2659693076362022e-08,\n",
    "    \"6_set1_L3s0_350@6_set1_L6s5_350\": 2.2975536416883118e-08,\n",
    "    \"6_set1_L3s0_350@6_set1_L6s6_350\": 2.301702460693713e-08,\n",
    "    \"6_set1_L3s1_350@6_set1_L3s2_350\": 1.0466733576707037e-08,\n",
    "    \"6_set1_L3s1_350@6_set1_L3s3_350\": 4.2599279547739245e-08,\n",
    "    \"6_set1_L3s1_350@6_set1_L6s0_350\": 6.305915970989931e-07,\n",
    "    \"6_set1_L3s1_350@6_set1_L6s1_350\": 9.019005361889241e-08,\n",
    "    \"6_set1_L3s1_350@6_set1_L6s2_350\": 3.9238139359505696e-08,\n",
    "    \"6_set1_L3s1_350@6_set1_L6s3_350\": 2.356670911139411e-07,\n",
    "    \"6_set1_L3s1_350@6_set1_L6s4_350\": 3.053337854952709e-07,\n",
    "    \"6_set1_L3s1_350@6_set1_L6s5_350\": 3.1088789547799376e-07,\n",
    "    \"6_set1_L3s1_350@6_set1_L6s6_350\": 3.064145698406696e-07,\n",
    "    \"6_set1_L3s2_350@6_set1_L3s3_350\": 9.012763545635827e-08,\n",
    "    \"6_set1_L3s2_350@6_set1_L6s0_350\": 7.890000225687393e-07,\n",
    "    \"6_set1_L3s2_350@6_set1_L6s1_350\": 1.4495758841805508e-07,\n",
    "    \"6_set1_L3s2_350@6_set1_L6s2_350\": 8.308510333482057e-08,\n",
    "    \"6_set1_L3s2_350@6_set1_L6s3_350\": 3.3845158602732285e-07,\n",
    "    \"6_set1_L3s2_350@6_set1_L6s4_350\": 4.194359870140521e-07,\n",
    "    \"6_set1_L3s2_350@6_set1_L6s5_350\": 4.262451834539994e-07,\n",
    "    \"6_set1_L3s2_350@6_set1_L6s6_350\": 4.2123834360251333e-07,\n",
    "    \"6_set1_L3s3_350@6_set1_L6s0_350\": 3.504195265892563e-07,\n",
    "    \"6_set1_L3s3_350@6_set1_L6s1_350\": 2.5524163128345397e-08,\n",
    "    \"6_set1_L3s3_350@6_set1_L6s2_350\": 1.7369315784119697e-08,\n",
    "    \"6_set1_L3s3_350@6_set1_L6s3_350\": 8.690626372005504e-08,\n",
    "    \"6_set1_L3s3_350@6_set1_L6s4_350\": 1.2711172534574817e-07,\n",
    "    \"6_set1_L3s3_350@6_set1_L6s5_350\": 1.306849435007101e-07,\n",
    "    \"6_set1_L3s3_350@6_set1_L6s6_350\": 1.280023661921839e-07,\n",
    "    \"6_set1_L6s0_350@6_set1_L6s1_350\": 2.91229218428991e-07,\n",
    "    \"6_set1_L6s0_350@6_set1_L6s2_350\": 3.9855557878373395e-07,\n",
    "    \"6_set1_L6s0_350@6_set1_L6s3_350\": 1.0504377850596728e-07,\n",
    "    \"6_set1_L6s0_350@6_set1_L6s4_350\": 6.380344558300393e-08,\n",
    "    \"6_set1_L6s0_350@6_set1_L6s5_350\": 6.184867762547775e-08,\n",
    "    \"6_set1_L6s0_350@6_set1_L6s6_350\": 6.405562902966565e-08,\n",
    "    \"6_set1_L6s1_350@6_set1_L6s2_350\": 2.451111839201128e-08,\n",
    "    \"6_set1_L6s1_350@6_set1_L6s3_350\": 6.24380738244462e-08,\n",
    "    \"6_set1_L6s1_350@6_set1_L6s4_350\": 9.015381251729723e-08,\n",
    "    \"6_set1_L6s1_350@6_set1_L6s5_350\": 9.333880553083197e-08,\n",
    "    \"6_set1_L6s1_350@6_set1_L6s6_350\": 9.178174832348812e-08,\n",
    "    \"6_set1_L6s2_350@6_set1_L6s3_350\": 9.752612454954731e-08,\n",
    "    \"6_set1_L6s2_350@6_set1_L6s4_350\": 1.4599906981335007e-07,\n",
    "    \"6_set1_L6s2_350@6_set1_L6s5_350\": 1.4986410135579923e-07,\n",
    "    \"6_set1_L6s2_350@6_set1_L6s6_350\": 1.4651175458999814e-07,\n",
    "    \"6_set1_L6s3_350@6_set1_L6s4_350\": 5.90683423089412e-09,\n",
    "    \"6_set1_L6s3_350@6_set1_L6s5_350\": 6.6436127958907136e-09,\n",
    "    \"6_set1_L6s3_350@6_set1_L6s6_350\": 5.842024099543951e-09,\n",
    "    \"6_set1_L6s4_350@6_set1_L6s5_350\": 8.111076439520268e-11,\n",
    "    \"6_set1_L6s4_350@6_set1_L6s6_350\": 8.583557298843883e-11,\n",
    "    \"6_set1_L6s5_350@6_set1_L6s6_350\": 3.584695992864188e-11,\n",
    "}\n",
    "\n",
    "\n",
    "x = [val for val in diff_int_dict.values()]\n",
    "y = [val for val in mistmatch_dict.values()]\n",
    "# x = np.abs(x)\n",
    "plt.ylabel(\"Mismatch\")\n",
    "plt.xlabel(f\"Int Diff: {y_axis}\")\n",
    "plt.scatter(x, y, marker=\"o\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = runs_data_dict[diff_base][\"t(M)\"]\n",
    "filter_t = (t >= 1200) & (t <= 4000)\n",
    "diff_int = {}\n",
    "for key in runs_data_dict.keys():\n",
    "    if key == diff_base:\n",
    "        continue\n",
    "    # diff_int[key] = sp.integrate.simpson(runs_data_dict[key][f\"diff_{y_axis}\"],t)\n",
    "    diff_int[f\"abs_{key}\"] = sp.integrate.simpson(np.array(runs_data_dict[key][f\"diff_abs_{y_axis}\"])[filter_t],t[filter_t])\n",
    "#   print(f\"{key} : {runs_data_dict[key].keys()}\")\n",
    "\n",
    "diff_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot individual cons for all subdomains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cons_for_all_domains(runs_data_dict, main_fig_save_folder):\n",
    "    def get_domain_names(column_names):\n",
    "        domain_names = []\n",
    "        for name in column_names:\n",
    "            if \"on\" in name:\n",
    "                temp = name.split(\" on\")[1].strip()\n",
    "                if temp not in domain_names:\n",
    "                    domain_names.append(temp)\n",
    "        return sorted(list(domain_names))\n",
    "\n",
    "    main_fig_save_folder.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "    for key in runs_data_dict.keys():\n",
    "        fig_save_folder = main_fig_save_folder / key\n",
    "        fig_save_folder.mkdir(parents=False, exist_ok=False)\n",
    "\n",
    "        data = runs_data_dict[key]\n",
    "\n",
    "        domain_names = get_domain_names(column_names)\n",
    "\n",
    "        with plt.style.context(\"ggplot\"):\n",
    "            plt.rcParams[\"figure.figsize\"] = (6, 6)\n",
    "            plt.rcParams[\"figure.autolayout\"] = True\n",
    "            for domain in domain_names:\n",
    "                for cons in [r\"1Con\", r\"2Con\", r\"3Con\"]:\n",
    "                    domain_col_list = filter_by_regex(\n",
    "                        regex=[domain], col_list=data.columns\n",
    "                    )\n",
    "                    domain_col_list = filter_by_regex(\n",
    "                        regex=[cons], col_list=domain_col_list\n",
    "                    )\n",
    "                    norm_factor = np.sqrt(len(domain_col_list))\n",
    "                    plt.plot(\n",
    "                        data[\"t(M)\"],\n",
    "                        np.sqrt(np.sum(data[domain_col_list] ** 2, axis=1))\n",
    "                        / norm_factor,\n",
    "                        label=cons,\n",
    "                    )\n",
    "                plt.gca().set_prop_cycle(\n",
    "                    None\n",
    "                )  # Resets the color cycle for the current axes\n",
    "                for cons in [r\"1Con\", r\"2Con\", r\"3Con\"]:\n",
    "                    domain_col_list = filter_by_regex(\n",
    "                        regex=[domain], col_list=data.columns\n",
    "                    )\n",
    "                    domain_col_list = filter_by_regex(\n",
    "                        regex=[cons], col_list=domain_col_list\n",
    "                    )\n",
    "                    plt.plot(\n",
    "                        data[\"t(M)\"],\n",
    "                        np.max(np.abs(data[domain_col_list]), axis=1),\n",
    "                        label=\"Linf\" + cons,\n",
    "                        linestyle=\"--\",\n",
    "                    )\n",
    "                plt.title(f\"{key}: {domain}\")\n",
    "                plt.xlabel(\"t(M)\")\n",
    "                plt.ylabel(\"log(norm(cons))\")\n",
    "                plt.yscale(\"log\")\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(fig_save_folder / f\"{domain}.png\", dpi=400) \n",
    "                plt.close()\n",
    "                print(f\"Saved {key}: {domain} plot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_fig_save_folder = Path(\n",
    "#     \"/resnick/groups/sxs/hchaudha/spec_runs/del/temp_figures/del4\"\n",
    "# )\n",
    "# plot_cons_for_all_domains(\n",
    "#     # runs_data_dict,\n",
    "#     {\"L1_AC_L1_AK_47K\":runs_data_dict[\"L1_AC_L1_AK\"]},\n",
    "#     main_fig_save_folder=main_fig_save_folder,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save all y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_avg_len=0\n",
    "save_path = None\n",
    "diff_base = None\n",
    "constant_shift_val_time = None\n",
    "plot_abs_diff = False\n",
    "y_axis_list = None\n",
    "x_axis = 't(M)'\n",
    "\n",
    "plot_abs_diff = True\n",
    "\n",
    "minT = 0\n",
    "maxT = 40000\n",
    "maxT = 2000\n",
    "maxT = 47005\n",
    "# maxT = 46600\n",
    "\n",
    "plot_fun = lambda x,y,label : plt.plot(x,y,label=label)\n",
    "# plot_fun = lambda x,y,label : plt.plot(x,y,label=label,marker='x')\n",
    "plot_fun = lambda x,y,label : plt.semilogy(x,y,label=label)\n",
    "\n",
    "legend_dict = {}\n",
    "for key in runs_data_dict.keys():\n",
    "    legend_dict[key] = None\n",
    "\n",
    "append_to_title = \"\"\n",
    "if '@' in data_file_path:\n",
    "    append_to_title = \" HorizonBH=\"+data_file_path.split('@')[-1]\n",
    "\n",
    "\n",
    "main_folder_path = Path(\"/resnick/groups/sxs/hchaudha/spec_runs/del/figures3\")\n",
    "for y_axis in column_names:\n",
    "    # if \"SphereC\" in y_axis:\n",
    "    #     continue\n",
    "    if y_axis == 't(M)':\n",
    "        continue\n",
    "    with plt.style.context('ggplot'):\n",
    "        colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "        line_styles = ['-', '--', '-.', ':']\n",
    "        combined_cycler = cycler(linestyle=line_styles)*cycler(color=colors)\n",
    "        plt.rcParams['axes.prop_cycle'] = combined_cycler\n",
    "        plt.rcParams[\"figure.figsize\"] = (6,6)\n",
    "        plt.rcParams[\"figure.autolayout\"] = True\n",
    "        # plt.ylim(1e-10,1e-4)\n",
    "        if y_axis_list is None:\n",
    "            plot_graph_for_runs(runs_data_dict, x_axis, y_axis, minT, maxT, legend_dict=legend_dict, save_path=save_path, moving_avg_len=moving_avg_len, plot_fun=plot_fun, diff_base=diff_base, plot_abs_diff=plot_abs_diff,constant_shift_val_time=constant_shift_val_time,append_to_title=append_to_title)\n",
    "        else:\n",
    "            plot_graph_for_runs_wrapper(runs_data_dict, x_axis, y_axis_list, minT, maxT, legend_dict=legend_dict, save_path=save_path, moving_avg_len=moving_avg_len, plot_fun=plot_fun, diff_base=diff_base, plot_abs_diff=plot_abs_diff,constant_shift_val_time=constant_shift_val_time,append_to_title=append_to_title)\n",
    "\n",
    "\n",
    "        #   plt.title(\"\")\n",
    "        #   plt.ylabel(\"Constraint Violations near black holes\")\n",
    "        #   plt.tight_layout()\n",
    "        #   plt.legend(loc='upper right')\n",
    "        #   plt.ylim(1e-8, 1e-5)\n",
    "        #   plt.ylim(1e-12, 1e-6)\n",
    "        #   save_name = \"main_ode_impro_const_new_no_avg.png\"\n",
    "\n",
    "        save_name = main_folder_path/f\"{y_axis}\"\n",
    "        if save_name.exists():\n",
    "            raise Exception(\"Change name\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{save_name}.png\",dpi=300)\n",
    "        plt.close()\n",
    "        print(y_axis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noise in things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = runs_data_dict['26_set1_L6_long'].copy()\n",
    "# data = runs_data_dict['26_main_L6_long'].copy()\n",
    "data = runs_data_dict['6_set1_L6s6'].copy()\n",
    "# data = runs_data_dict['6_set1_L3s3'].copy()\n",
    "# data = runs_data_dict['high_accuracy_L3_main'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_key = 'ArealMass'\n",
    "scipy_or_np = 'scipy'\n",
    "window = 6 # Choose appropriate window size\n",
    "moving_avg_len = None\n",
    "moving_avg_len = 50\n",
    "# scipy_or_np = 'np'\n",
    "\n",
    "for key in runs_data_dict:\n",
    "    # if 'L3_AC' not in key:\n",
    "    #     if '22_set1_L3_long' not in key:\n",
    "    #         continue\n",
    "    if 'L3_AC' not in key:\n",
    "        if '22_set1_L3_long' not in key:\n",
    "            continue\n",
    "    data = runs_data_dict[key].copy()\n",
    "    t = np.array(data['t(M)'])\n",
    "    x = np.array(data[y_key])\n",
    "\n",
    "    if scipy_or_np == 'scipy':\n",
    "        running_mean = uniform_filter1d(x, size=window, mode='nearest')\n",
    "        noise_estimate = x - running_mean\n",
    "    elif scipy_or_np == 'np':\n",
    "        running_mean = np.convolve(x,np.ones(window), mode='valid')/window\n",
    "        t = t[window//2-1:-window//2]\n",
    "        noise_estimate = x[window//2-1:-window//2] - running_mean\n",
    "    else:\n",
    "        raise Exception(f\"Invalid scipy_or_np value: {scipy_or_np}\")\n",
    "\n",
    "    if moving_avg_len is None:\n",
    "        plt.plot(t,np.abs(noise_estimate), label=key)\n",
    "    else:\n",
    "        t = t[moving_avg_len//2-1:-moving_avg_len//2]\n",
    "        y = np.convolve(np.abs(noise_estimate),np.ones(moving_avg_len), mode='valid')/moving_avg_len\n",
    "        plt.plot(t,y, label=key)\n",
    "\n",
    "title = f\"Noise estimate {y_key}, window={int(window*0.5)}M\"\n",
    "if moving_avg_len is not None:\n",
    "    title += f\", moving_avg_len={moving_avg_len//2}M\"\n",
    "plt.title(title)\n",
    "plt.xlabel(\"t(M)\")\n",
    "plt.ylabel(f\"Noise estimate: {y_key}\")\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "# plt.ylim(1e-13,1e-6)\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array(data['t(M)'])\n",
    "x = np.array(data['ArealMass'])\n",
    "# x is your time series\n",
    "window = 25 # Choose appropriate window size\n",
    "running_mean = uniform_filter1d(x, size=window, mode='nearest')\n",
    "noise_estimate = x - running_mean\n",
    "\n",
    "# running_mean = np.convolve(x,np.ones(window), mode='valid')/window\n",
    "# t = t[window//2-1:-window//2]\n",
    "# noise_estimate = x[window//2-1:-window//2] - running_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t,np.abs(noise_estimate))\n",
    "plt.yscale('log')\n",
    "plt.ylim(1e-13,1e-6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data.columns\n",
    "cols  = [col for col in cols if 'SphereC13' in col]\n",
    "# cols  = [col for col in cols if '2Con' in col]\n",
    "fil_data = data[cols]\n",
    "# fil_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = data['t(M)']\n",
    "\n",
    "colors = ['r','b','g']\n",
    "plt.plot(t,fil_data[[col for col in cols if '1Con' in col]].min(axis=1),color=colors[0])\n",
    "plt.plot(t,fil_data[[col for col in cols if '1Con' in col]].max(axis=1),color=colors[0])\n",
    "plt.plot(t,fil_data[[col for col in cols if '1Con' in col]].median(axis=1),label='1Con',color=colors[0])\n",
    "\n",
    "plt.plot(t,fil_data[[col for col in cols if '2Con' in col]].min(axis=1),color=colors[1])\n",
    "plt.plot(t,fil_data[[col for col in cols if '2Con' in col]].max(axis=1),color=colors[1])\n",
    "plt.plot(t,fil_data[[col for col in cols if '2Con' in col]].median(axis=1),label='2Con',color=colors[1])\n",
    "\n",
    "all_but_zero = list(set([col for col in cols if '3Con' in col])  - set([col for col in cols if '3Cont' in col] ))\n",
    "plt.plot(t,fil_data[all_but_zero].min(axis=1),color=colors[2])\n",
    "plt.plot(t,fil_data[all_but_zero].max(axis=1),color=colors[2])\n",
    "plt.plot(t,fil_data[all_but_zero].median(axis=1),label='3Con',color=colors[2])\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = data['t(M)']\n",
    "# y = fil_data['Linf(3Conzyy) on SphereC10']\n",
    "# y = fil_data['Linf(2Conzy) on SphereC10']\n",
    "y = fil_data.min(axis=1)\n",
    "# y = fil_data['Linf(1Cont) on SphereC10']\n",
    "plt.plot(t,fil_data.min(axis=1),label='min')\n",
    "plt.plot(t,fil_data.max(axis=1),label='max')\n",
    "plt.plot(t,fil_data.median(axis=1),label='median')\n",
    "# plt.plot(t,y)\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain vals vs time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_data_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_runs = {}\n",
    "for key,val in runs_to_plot.items():\n",
    "  # if \"15_\" not in key:\n",
    "  #   filtered_runs[key] = val\n",
    "  #   continue\n",
    "  filtered_runs[key] = val\n",
    "  \n",
    "filtered_runs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data_file_path = \"ConstraintNorms/GhCe.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCeExt.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCeExt_L2.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCeExt_Norms.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCe_L2.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCe_Linf.dat\"\n",
    "# data_file_path = \"ConstraintNorms/Linf.dat\"\n",
    "# data_file_path = \"ConstraintNorms/Constraints_Linf.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCe_VolL2.dat\"\n",
    "data_file_path = \"ConstraintNorms/NormalizedGhCe_Linf.dat\"\n",
    "\n",
    "# data_file_path = \"GhCe.dat\"\n",
    "# data_file_path = \"NormalizedGhCe.dat\"\n",
    "# data_file_path = \"GhCe_Norms.dat\"\n",
    "# data_file_path = \"kappaErr_Linf.dat\"\n",
    "# data_file_path = \"psiErr_Linf.dat\"\n",
    "\n",
    "column_names, runs_data_dict_ghce = load_data_from_levs(filtered_runs,data_file_path)\n",
    "# column_names, runs_data_dict_ghce = load_data_from_levs(runs_to_plot,data_file_path)\n",
    "print(runs_data_dict_ghce.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = '6_set1_L3_EXP_FK_14012_10_6_30'\n",
    "# key = '16_set1_L3'\n",
    "# key = '22_set1_L3_long'\n",
    "key = '6_set1_L3s3'\n",
    "# key = '67_master_mr3'\n",
    "key = '6_set1_L6s5'\n",
    "key = '46K_L6_3000_cd10'\n",
    "key = '46K_L6_3000_cd100'\n",
    "key = '26_main_L6_long'\n",
    "# key = '26_set1_L6_long'\n",
    "\n",
    "repeated_symmetric = False\n",
    "# repeated_symmetric = True\n",
    "\n",
    "minT = 0\n",
    "# minT = 10\n",
    "# minT = 480\n",
    "# minT = 1210\n",
    "# minT = 3080\n",
    "# minT = 9700\n",
    "# minT = 10000\n",
    "# minT = 46500\n",
    "\n",
    "maxT = 400000\n",
    "# maxT = 4000\n",
    "# maxT = 590\n",
    "# maxT = 3500\n",
    "# maxT = 200\n",
    "# maxT = 800\n",
    "# maxT = 2\n",
    "\n",
    "data = limit_by_col_val(minT,maxT,'t(M)',runs_data_dict_ghce[key])\n",
    "data = data.iloc[::1].dropna(axis=1, how='all')\n",
    "\n",
    "domain_col_list = filter_by_regex(regex=[\"Sphere\",\"Cylinder\"],col_list=data.columns)\n",
    "domain_col_list = filter_by_regex(regex=[\"Linf\"],col_list=domain_col_list)\n",
    "# domain_col_list = filter_by_regex(regex=[\"SphereC\"],col_list=domain_col_list)\n",
    "# domain_col_list = filter_by_regex(regex=[\"SphereC[4-9]\"],col_list=domain_col_list)\n",
    "# domain_col_list = filter_by_regex(regex=[\"SphereC[2][0-9]\"],col_list=data.columns)\n",
    "# domain_col_list = filter_by_regex(regex=[\"SphereC\"],col_list=domain_col_list,exclude=True)\n",
    "# domain_col_list = filter_by_regex(regex=[\"1Conx\"],col_list=domain_col_list)\n",
    "visual_data = data[domain_col_list]\n",
    "visual_data = np.log10(visual_data)\n",
    "# print(visual_data.columns)\n",
    "# Plot using imshow\n",
    "\n",
    "column_names = list(visual_data.columns)\n",
    "# column_names = [i.split(\" \")[-1] for i in visual_data.columns]\n",
    "column_names = return_sorted_domain_names(column_names, repeated_symmetric=repeated_symmetric)\n",
    "# print(column_names)\n",
    "\n",
    "vmin_log,vmax_log = None,None\n",
    "# vmin_log,vmax_log = -12.417834211445228 , -8.343109330686875\n",
    "# vmin_log,vmax_log = -13.918609670001128 , -2.3373560237057256\n",
    "# vmin_log,vmax_log = -9 , None\n",
    "if vmin_log is None:\n",
    "  vmin_log = visual_data.min().min()\n",
    "if vmax_log is None:\n",
    "  vmax_log = visual_data.max().max()\n",
    "print(vmin_log,\",\",vmax_log)\n",
    "\n",
    "print(len(domain_col_list))\n",
    "\n",
    "if repeated_symmetric:\n",
    "    visual_data[\"Excision\"] = [np.nan for i in range(len(data['t(M)']))]\n",
    "\n",
    "plt.figure(figsize=(18, 10))\n",
    "imshow_plot = plt.imshow(\n",
    "    visual_data[column_names], \n",
    "    aspect='auto', \n",
    "    cmap='RdYlGn_r', \n",
    "    origin='lower',interpolation='none',\n",
    "    vmin=vmin_log, \n",
    "    vmax=vmax_log\n",
    ")\n",
    "\n",
    "if repeated_symmetric:\n",
    "    # Set x-ticks and labels\n",
    "    plt.xticks(\n",
    "        ticks=np.arange(len(column_names)), \n",
    "        labels=[i.split(\" \")[-1] for i in column_names], \n",
    "        rotation=90\n",
    "    )\n",
    "else:\n",
    "    # Set x-ticks and labels\n",
    "    plt.xticks(\n",
    "        ticks=np.arange(len(visual_data.columns)), \n",
    "        labels=[i.split(\" \")[-1] for i in column_names], \n",
    "        rotation=90\n",
    "    )\n",
    "\n",
    "ytick_step = 1\n",
    "ytick_step = len(visual_data) // 10  # Show about 10 ticks\n",
    "plt.yticks(\n",
    "    ticks=np.arange(0, len(visual_data), ytick_step), \n",
    "    labels=data['t(M)'][::ytick_step].astype(int)\n",
    ")\n",
    "\n",
    "# Create colorbar\n",
    "colorbar = plt.colorbar(imshow_plot, label=f'{column_names[0].split(\" \")[0]}')\n",
    "\n",
    "# Determine colorbar ticks that align with the fixed vmin and vmax\n",
    "# tick_vals = np.linspace(vmin_log, vmax_log, num=5)\n",
    "\n",
    "# Set these ticks on the colorbar\n",
    "# colorbar.set_ticks(tick_vals)\n",
    "\n",
    "# Convert ticks back to the original scale for labeling\n",
    "# colorbar.set_ticklabels([f'{10**val:.2e}' for val in tick_vals])\n",
    "\n",
    "plt.ylabel('t(M)')\n",
    "plt.title(f'{key}')\n",
    "plt.tight_layout() \n",
    "\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### APS plot version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = '6_set1_L3_EXP_FK_14012_10_6_30'\n",
    "# key = '16_set1_L3'\n",
    "key = '22_set1_L3_long'\n",
    "# key = 'L3_AC_L4'\n",
    "key = '28_set1_cd_junk_1'\n",
    "# key = 'high_accuracy_L5'\n",
    "# key = '22_set1_L3_long'\n",
    "# key = 'set1_L6_AK_cd_100'\n",
    "# key = '6_set1_L6s6'\n",
    "\n",
    "repeated_symmetric = False\n",
    "# repeated_symmetric = True\n",
    "num_Excision = 3\n",
    "\n",
    "\n",
    "minT = 0\n",
    "# minT = .2\n",
    "# minT = 480\n",
    "# minT = 1400\n",
    "# minT = 3000\n",
    "# minT = 3500\n",
    "# minT = 7000\n",
    "\n",
    "maxT = 40000\n",
    "maxT = 4000\n",
    "# maxT = 590\n",
    "# maxT = 3500\n",
    "# maxT = 340\n",
    "# maxT = 800\n",
    "# maxT = 2\n",
    "\n",
    "data = limit_by_col_val(minT,maxT,'t(M)',runs_data_dict_ghce[key])\n",
    "data = data.iloc[::1].dropna(axis=1, how='all')\n",
    "\n",
    "domain_col_list = filter_by_regex(regex=[\"Sphere\",\"Cylinder\"],col_list=data.columns)\n",
    "domain_col_list = filter_by_regex(regex=[\"Linf\"],col_list=domain_col_list)\n",
    "# domain_col_list = filter_by_regex(regex=[\"SphereC\"],col_list=domain_col_list)\n",
    "# domain_col_list = filter_by_regex(regex=[\"SphereC[4-9]\"],col_list=domain_col_list)\n",
    "# domain_col_list = filter_by_regex(regex=[\"SphereC[2][0-9]\"],col_list=data.columns)\n",
    "# domain_col_list = filter_by_regex(regex=[\"SphereC\"],col_list=domain_col_list,exclude=True)\n",
    "# domain_col_list = filter_by_regex(regex=[\"1Conx\"],col_list=domain_col_list)\n",
    "visual_data = data[domain_col_list]\n",
    "visual_data = np.log10(visual_data)\n",
    "# print(visual_data.columns)\n",
    "# Plot using imshow\n",
    "\n",
    "column_names = list(visual_data.columns)\n",
    "# column_names = [i.split(\" \")[-1] for i in visual_data.columns]\n",
    "column_names = return_sorted_domain_names(column_names, repeated_symmetric=repeated_symmetric, num_Excision=num_Excision)\n",
    "# print(column_names)\n",
    "\n",
    "vmin_log,vmax_log = None,None\n",
    "# vmin_log,vmax_log = -12.417834211445228 , -8.343109330686875\n",
    "# vmin_log,vmax_log = -10.183427321168152 , -2.540360921276033\n",
    "vmin_log,vmax_log = -10.2 , -0.10\n",
    "# vmin_log,vmax_log = -9 , None\n",
    "if vmin_log is None:\n",
    "  vmin_log = visual_data.min().min()\n",
    "if vmax_log is None:\n",
    "  vmax_log = visual_data.max().max()\n",
    "print(vmin_log,\",\",vmax_log)\n",
    "\n",
    "print(len(domain_col_list))\n",
    "\n",
    "if repeated_symmetric:\n",
    "    visual_data[\"Excision\"] = [np.nan for i in range(len(data['t(M)']))]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "imshow_plot = ax.imshow(\n",
    "    visual_data[column_names], \n",
    "    aspect='auto', \n",
    "    cmap='RdYlGn_r', \n",
    "    origin='lower',\n",
    "    interpolation='none',\n",
    "    vmin=vmin_log, \n",
    "    vmax=vmax_log\n",
    ")\n",
    "\n",
    "# Set x-ticks and labels\n",
    "# ax.set_xticks(np.arange(len(visual_data.columns)))\n",
    "# ax.set_xticklabels([i.split(\" \")[-1] for i in column_names], rotation=90)\n",
    "\n",
    "ax.tick_params(left=False, bottom=True, labelleft=False, labelbottom=True)\n",
    "\n",
    "# ytick_step = 1\n",
    "# ytick_step = len(visual_data) // 10  # Show about 10 ticks\n",
    "# ax.set_yticks(np.arange(0, len(visual_data), ytick_step))\n",
    "# ax.set_yticklabels(data['t(M)'][::ytick_step].astype(int))\n",
    "\n",
    "# Create colorbar\n",
    "colorbar = fig.colorbar(imshow_plot, ax=ax, label=f'log(constraint violation)')\n",
    "\n",
    "# Determine colorbar ticks that align with the fixed vmin and vmax\n",
    "# tick_vals = np.linspace(vmin_log, vmax_log, num=5)\n",
    "\n",
    "# Set these ticks on the colorbar\n",
    "# colorbar.set_ticks(tick_vals)\n",
    "\n",
    "# Convert ticks back to the original scale for labeling\n",
    "# colorbar.set_ticklabels([f'{10**val:.2e}' for val in tick_vals])\n",
    "\n",
    "ax.set_ylabel(r'time $\\longrightarrow$')\n",
    "# ax.set_xlabel(r'subdomain number')\n",
    "ax.set_xticks(\n",
    "    ticks=[0,44,65,len(column_names)], \n",
    "    labels=['Outer Boundary','bhA','bhB','Outer Boundary'], \n",
    "    # rotation=90\n",
    ")\n",
    "ax.set_xticks(\n",
    "    ticks=[0,44,54,65,len(column_names)], \n",
    "    labels=['Outer Boundary','bhA','center','bhB','Outer Boundary'], \n",
    "    # rotation=90\n",
    ")\n",
    "# ax.set_title(f'{key}')\n",
    "plt.tight_layout()\n",
    "# annotation1 = ax.annotate('bh A', \n",
    "#             xy=(0.2,0.1),             # point to annotate\n",
    "#             xytext=(0.1,0.5),  # text position\n",
    "#             xycoords = 'subfigure fraction',\n",
    "#             arrowprops=dict(facecolor='black', shrink=0.05)\n",
    "#             )\n",
    "# annotation2 = ax.annotate('bh B', \n",
    "#             xy=(0.36,0.1),             # point to annotate\n",
    "#             xytext=(0.25,0.5),  # text position\n",
    "#             xycoords = 'subfigure fraction',\n",
    "#             arrowprops=dict(facecolor='black', shrink=0.05)\n",
    "#             )\n",
    "# annotation3 = ax.annotate('Outer boundary', \n",
    "#             xy=(0.82,0.1),             # point to annotate\n",
    "#             xytext=(0.6,0.5),  # text position\n",
    "#             xycoords = 'subfigure fraction',\n",
    "#             arrowprops=dict(facecolor='black', shrink=0.05)\n",
    "#             )\n",
    "\n",
    "\n",
    "ax.grid(False)\n",
    "\n",
    "# save_name = \"Extra_L5_set1_sym.png\"\n",
    "# save_name = \"Extra_L6_set1_sym_lim.png\"\n",
    "# save_name = \"Extra_L5_main_sym.png\"\n",
    "# save_name = Path(f\"/groups/sxs/hchaudha/scripts/report/figures/{save_name}\")\n",
    "# if save_name.exists():\n",
    "#     raise Exception(\"Change name\")\n",
    "# plt.savefig(save_name,dpi=600,bbox_inches='tight', bbox_extra_artists=[annotation1, annotation2, annotation3])\n",
    "# plt.savefig(save_name,dpi=600,bbox_inches='tight')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### take diff between two different runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(runs_data_dict_ghce.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key1 = '46K_L6_3000_cd100'\n",
    "key1 = '26_main_L6_long'\n",
    "# key1 = '22_set1_L1_long'\n",
    "# key2 = '30_RM_set1_L3'\n",
    "key2 = '26_main_L6_long'\n",
    "key2 = '26_set1_L6_long'\n",
    "# key2 = '28_set1_cd_junk_1'\n",
    "# key2 = '28_set1_cd_junk_1'\n",
    "\n",
    "\n",
    "minT = 0\n",
    "# minT = 480\n",
    "# minT = 3080\n",
    "# minT = 7360\n",
    "# minT = 46500\n",
    "# minT = 46600\n",
    "\n",
    "maxT = 400000\n",
    "# maxT = 150\n",
    "# maxT = 590\n",
    "# maxT = 1500\n",
    "# maxT = 8500\n",
    "\n",
    "data1 = limit_by_col_val(minT,maxT,'t(M)',runs_data_dict_ghce[key1])\n",
    "data1 = data1.iloc[::1].dropna(axis=1, how='all')\n",
    "\n",
    "data2 = limit_by_col_val(minT,maxT,'t(M)',runs_data_dict_ghce[key2])\n",
    "data2 = data2.iloc[::1].dropna(axis=1, how='all')\n",
    "\n",
    "\n",
    "# Set column 't(M)' as the index for both data1 and data2\n",
    "data1.set_index('t(M)', inplace=True)\n",
    "data2.set_index('t(M)', inplace=True)\n",
    "\n",
    "# Now, 't(M)' is the index, and you can perform the subtraction safely\n",
    "data = data1.copy()\n",
    "for col in data.columns:\n",
    "    # Negative(green) is good\n",
    "    data[col] = (np.log10(data2[col]) - np.log10(data1[col]))\n",
    "    # data[col] = (data2[col] - data1[col])*2/(data2[col] + data1[col])\n",
    "    # Modify each value: set to 1 if positive, 0 if negative or zero\n",
    "    # => red(1) if more error and green(0) is less\n",
    "    # data[col] = np.where(data[col] > 0, 1, 0)\n",
    "\n",
    "# Reset the index if you need 't(M)' back as a column\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "domain_col_list = filter_by_regex(regex=[\"Sphere\",\"Cylinder\"],col_list=data.columns)\n",
    "# domain_col_list = filter_by_regex(regex=[\"SphereC\"],col_list=domain_col_list)\n",
    "# domain_col_list = filter_by_regex(regex=[\"SphereC\"],col_list=domain_col_list,exclude=True)\n",
    "# domain_col_list = filter_by_regex(regex=[\"1Conx\"],col_list=domain_col_list)\n",
    "\n",
    "visual_data = data[domain_col_list]\n",
    "# visual_data = np.log10(visual_data)\n",
    "# Plot using imshow\n",
    "\n",
    "column_names = list(visual_data.columns)\n",
    "# column_names = [i.split(\" \")[-1] for i in visual_data.columns]\n",
    "column_names = return_sorted_domain_names(column_names)\n",
    "print(column_names)\n",
    "\n",
    "vmin_log,vmax_log = None,None\n",
    "# vmin_log,vmax_log = -1,1\n",
    "\n",
    "if vmin_log is None:\n",
    "  # Get min non -inf value\n",
    "  temp = visual_data.copy()\n",
    "  temp.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "  vmin_log = temp.min().min()\n",
    "if vmax_log is None:\n",
    "  vmax_log = visual_data.max().max()\n",
    "print(vmin_log,\",\",vmax_log)\n",
    "\n",
    "# vmin_log, vmax_log = vmin_log, -vmin_log # This is to make it so white is 0, green is good and red is bad\n",
    "\n",
    "# Emphasize the bad parts, i.e. more color resolution for the red part\n",
    "# vmin_log, vmax_log = -vmax_log, vmax_log # This is to make it so white is 0, green is good and red is bad\n",
    "\n",
    "# max_val = max(abs(vmin_log),abs(vmax_log))\n",
    "# vmin_log, vmax_log = -max_val, max_val # This is to make it so white is 0, green is good and red is bad\n",
    "# print(vmin_log,\",\",vmax_log)\n",
    "\n",
    "\n",
    "# Example colormap centered around zero\n",
    "plt.figure(figsize=(15, 10))\n",
    "divnorm = mcolors.TwoSlopeNorm(vmin=vmin_log, vcenter=0, vmax=vmax_log)\n",
    "\n",
    "imshow_plot = plt.imshow(\n",
    "    visual_data[column_names], \n",
    "    aspect='auto', \n",
    "    cmap='RdYlGn_r',\n",
    "    origin='lower',interpolation='none',\n",
    "    norm=divnorm\n",
    ")\n",
    "\n",
    "# Set x-ticks and labels\n",
    "plt.xticks(\n",
    "    ticks=np.arange(len(visual_data.columns)), \n",
    "    labels=[i.split(\" \")[-1] for i in column_names], \n",
    "    rotation=90\n",
    ")\n",
    "\n",
    "ytick_step = len(visual_data) // 10  # Show about 10 ticks\n",
    "plt.yticks(\n",
    "    ticks=np.arange(0, len(visual_data), ytick_step), \n",
    "    labels=data['t(M)'][::ytick_step].astype(int)\n",
    ")\n",
    "\n",
    "# Create colorbar\n",
    "colorbar = plt.colorbar(imshow_plot, label=f'{column_names[0].split(\" \")[0]}')\n",
    "\n",
    "plt.ylabel('t(M)')\n",
    "plt.title(f'{key2}(Green better) - {key1}')\n",
    "plt.tight_layout()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "minT = 0\n",
    "minT = 1200\n",
    "# minT = 4000\n",
    "\n",
    "maxT = 40000\n",
    "maxT = 2300\n",
    "# data = limit_by_col_val(minT,maxT,'t(M)',runs_data_dict_ghce[key1])\n",
    "data = limit_by_col_val(minT,maxT,'t(M)',runs_data_dict_ghce[key2])\n",
    "data = data.iloc[::1].dropna(axis=1, how='all')\n",
    "\n",
    "domain_col_list = filter_by_regex(regex=[\"Sphere\",\"Cylinder\"],col_list=data.columns)\n",
    "# domain_col_list = filter_by_regex(regex=[\"Cylinder\"],col_list=data.columns)\n",
    "# domain_col_list = filter_by_regex(regex=[r\"SphereC14\"],col_list=domain_col_list)\n",
    "# domain_col_list = filter_by_regex(regex=[r\"3Con\"],col_list=domain_col_list)\n",
    "\n",
    "data = data[domain_col_list+[\"t(M)\"]]\n",
    "\n",
    "max_info = {}\n",
    "for col in data.columns:\n",
    "    # Get the maximum value\n",
    "    max_val = data[col].max()\n",
    "    # Get all indices where the maximum value occurs\n",
    "    idx_max_list = data[data[col] == max_val].index\n",
    "    times = data.loc[idx_max_list, 't(M)']\n",
    "    values = data.loc[idx_max_list, col]\n",
    "\n",
    "    max_info[col] = {\n",
    "        \"max_idx_list\": idx_max_list,\n",
    "        \"max_val_times\": list(times),\n",
    "        \"max_vals\": list(values),\n",
    "    }\n",
    "    for time, val in zip(max_info[col]['max_val_times'], max_info[col]['max_vals']):\n",
    "        if val > 1e-9:\n",
    "            print(col, time, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minT = 0\n",
    "# minT = 1200\n",
    "key = \"6_set1_L6s2\"\n",
    "maxT = 40000\n",
    "maxT = 4000\n",
    "data = limit_by_col_val(minT,maxT,'t(M)',runs_data_dict_ghce[key])\n",
    "data = data.iloc[::1].dropna(axis=1, how='all')\n",
    "\n",
    "domain_col_list = filter_by_regex(regex=[\"Sphere\",\"Cylinder\"],col_list=data.columns)\n",
    "\n",
    "# Calculate maximum error at each time point\n",
    "data['max_error'] = data[domain_col_list].max(axis=1)\n",
    "\n",
    "# Calculate L2 norm (Euclidean norm) of the errors at each time point\n",
    "data['l2_norm'] = np.sqrt((data[domain_col_list] ** 2).sum(axis=1))\n",
    "\n",
    "plt.plot(data['t(M)'], data[\"max_error\"],label=\"max_error\")\n",
    "# plt.plot(data['t(M)'], data[\"l2_norm\"],label=\"l2_norm\")\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.title(key)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For domain resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = \"MinimumGridSpacing.dat\"\n",
    "data_file_path = \"Hist-GrDomain.txt\"\n",
    "\n",
    "# data_file_path = \"Hist-Domain.txt\"\n",
    "\n",
    "\n",
    "column_names, runs_data_dict_domain = load_data_from_levs(runs_to_plot,data_file_path)\n",
    "print(column_names)\n",
    "print(runs_data_dict_domain.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = '12_set1_L3_1500'\n",
    "# key = '13_set1_L4_1500'\n",
    "# key = '13_set1_L4_3000'\n",
    "# key = '13_set1_L3_3000'\n",
    "key = '17_set_main_q3_18_L3'\n",
    "key = 'L3_AC_L3_minL17'\n",
    "key = '23_nobounds_AMR'\n",
    "key = '22_set1_L3_long'\n",
    "key = '6_set1_L6s5'\n",
    "# key = '26_main_L6_long'\n",
    "# key = '26_set1_L6_long'\n",
    "# data = runs_data_dict[key].iloc[1:8000:10].dropna(axis=1, how='all')\n",
    "\n",
    "# key = '15_AMR_Lev0_455'\n",
    "\n",
    "minT = 0\n",
    "# minT = 1400\n",
    "# minT = 3400\n",
    "# minT = 10000\n",
    "maxT = 40000\n",
    "# maxT = 1200\n",
    "# maxT = 4000\n",
    "data = limit_by_col_val(minT,maxT,'t(M)',runs_data_dict_domain[key])\n",
    "data = data.iloc[::].dropna(axis=1, how='all')\n",
    "domain_col_list = filter_by_regex(regex=[\"Sphere\",\"Cylinder\"],col_list=data.columns)\n",
    "domain_col_list = filter_by_regex(regex=[\"SphereC\"],col_list=domain_col_list)\n",
    "# domain_col_list = filter_by_regex(regex=[\"SphereC[4-9]\"],col_list=domain_col_list)\n",
    "# domain_col_list = filter_by_regex(regex=[\"SphereC[2][0-9]\"],col_list=data.columns)\n",
    "# domain_col_list = filter_by_regex(regex=[\"SphereC\"],col_list=domain_col_list,exclude=True)\n",
    "\n",
    "\n",
    "visual_data = data[domain_col_list]\n",
    "if \"Grid\" in data_file_path:\n",
    "  visual_data = np.log10(visual_data)\n",
    "# Plot using imshow\n",
    "\n",
    "column_names = list(visual_data.columns)\n",
    "if \"Grid\" not in data_file_path:\n",
    "#   column_names = [i for i in column_names if (\"_R\" in i)]\n",
    "  column_names = [i for i in column_names if (\"_L\" in i)]\n",
    "  # column_names = [i for i in column_names if (\"_M\" in i)]\n",
    "column_names = return_sorted_domain_names(column_names)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(visual_data[column_names], aspect='auto', cmap='RdYlGn_r', origin='lower',interpolation='none')\n",
    "\n",
    "# Set x-ticks and labels\n",
    "plt.xticks(ticks=np.arange(len(column_names)), labels=[get_domain_name(i) for i in column_names], rotation=90)\n",
    "# plt.xticks(ticks=np.arange(len(column_names)), labels=[i.split(\"_\")[0] for i in column_names], rotation=90)\n",
    "\n",
    "# ytick_step = len(visual_data)\n",
    "ytick_step = len(visual_data) // 5  # show about 10 ticks\n",
    "# ytick_step = len(visual_data)   # show about 10 ticks\n",
    "# if ytick_step < 1:\n",
    "#   ytick_step = 10\n",
    "plt.yticks(ticks=np.arange(0, len(visual_data), ytick_step), \n",
    "           labels=data['t(M)'][::ytick_step].astype(int))\n",
    "# plt.yticks(ticks=np.arange(len(visual_data)), labels=data['t(M)'].astype(int))\n",
    "\n",
    "# plt.colorbar(label=f'{column_names[0].split(\" \")[0]}')\n",
    "plt.colorbar()\n",
    "# plt.xlabel('Features')\n",
    "plt.ylabel('t(M)')\n",
    "plt.title(f'{key}')\n",
    "plt.tight_layout()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot L,M and R on the same graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming return_sorted_domain_names and get_domain_name are predefined functions\n",
    "# Here's how you can create three subplots with three different column_name filters:\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(15, 15), constrained_layout=True)\n",
    "column_name_filters = [\"_R\", \"_L\", \"_M\"]\n",
    "column_name = [\"Extent 0\", \"Extent 1\", \"Extent 2\"]\n",
    "\n",
    "for i, ax, filter_suffix in zip(range(len(column_name_filters)),axes, column_name_filters):\n",
    "    filtered_column_names = [i for i in visual_data.columns if (filter_suffix in i)]\n",
    "    filtered_column_names = return_sorted_domain_names(filtered_column_names)\n",
    "    \n",
    "    im = ax.imshow(visual_data[filtered_column_names], aspect='auto', cmap='RdYlGn_r', origin='lower',interpolation='none',)\n",
    "    \n",
    "    if filter_suffix == \"_M\":\n",
    "      # Set x-ticks and labels\n",
    "      ax.set_xticks(np.arange(len(filtered_column_names)))\n",
    "      ax.set_xticklabels([get_domain_name(i) for i in filtered_column_names], rotation=90)\n",
    "    \n",
    "    # Set y-ticks and labels\n",
    "    ytick_step = len(visual_data) // 5  # show about 10 ticks\n",
    "    ax.set_yticks(np.arange(0, len(visual_data), ytick_step))\n",
    "    ax.set_yticklabels(data['t(M)'][::ytick_step].astype(int))\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_ylabel('t(M)')\n",
    "    ax.set_title(f'{key} : {column_name[i]}, {filter_suffix[-1]}')\n",
    "    \n",
    "    # Add a colorbar to each subplot\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    \n",
    "    ax.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 2d grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_file_path = \"ConstraintNorms/GhCe.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCeExt.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCeExt_L2.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCeExt_Norms.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCe_L2.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCe_Linf.dat\"\n",
    "# data_file_path = \"ConstraintNorms/Linf.dat\"\n",
    "# data_file_path = \"ConstraintNorms/Constraints_Linf.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCe_Norms.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCe_VolL2.dat\"\n",
    "data_file_path = \"ConstraintNorms/NormalizedGhCe_Linf.dat\"\n",
    "# data_file_path = \"ConstraintNorms/NormalizedGhCe_Norms.dat\"\n",
    "column_names, runs_data_dict_ghce = load_data_from_levs(runs_to_plot,data_file_path)\n",
    "print(runs_data_dict_ghce.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in runs_data_dict_ghce.keys():\n",
    "for key in [\"high_accuracy_L5\"]:\n",
    "  t_list = np.arange(0,4000,100)\n",
    "  save_path = Path(\"/home/hchaudha/notes/spec_accuracy/del/domain_plots\")\n",
    "  save_path = save_path/key\n",
    "  if not save_path.exists():\n",
    "    save_path.mkdir()\n",
    "  print(save_path)\n",
    "\n",
    "  nA=5\n",
    "  rA=nA*1.5\n",
    "  center_xA=rA + 2\n",
    "  RA=rA+5\n",
    "  rC=RA*2\n",
    "  nC=30\n",
    "  RC=rC+nC\n",
    "\n",
    "  saved_path_list_for_gif = []\n",
    "  for t in t_list:\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    pandas_dict = runs_data_dict_ghce[key].iloc[2*t,:].to_dict()\n",
    "    time = pandas_dict.pop(\"t(M)\")\n",
    "    # domain_color_local,sm = scalar_to_color(pandas_dict,(-11,-4),color_map='RdYlGn_r')\n",
    "    domain_color_local,sm = scalar_to_color(pandas_dict,(-8,-4),color_map='RdYlGn_r')\n",
    "    domain_color_local,sm = scalar_to_color(pandas_dict,color_map='RdYlGn_r')\n",
    "\n",
    "    patches_class = BBH_domain_sym_ploy(center_xA=center_xA, rA=rA, RA=RA, rC=rC, RC=RC, nA=nA, nC=nC, color_dict=domain_color_local) \n",
    "    for patch in patches_class.patches:\n",
    "      ax.add_patch(patch)\n",
    "\n",
    "    ax.set_xlim(-RC, RC)\n",
    "    ax.set_ylim(-RC, RC)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    cbar = plt.colorbar(sm, ax=ax)\n",
    "    cbar.set_label(f\"{list(pandas_dict.keys())[0].split(' ')[0]}\")\n",
    "    plt.title(f\"{key}  t(M) = {time}\")\n",
    "    plt.tight_layout()\n",
    "    img_save_path = save_path/f\"{t}.png\"\n",
    "    print(img_save_path)\n",
    "    plt.savefig(img_save_path)\n",
    "    saved_path_list_for_gif.append(img_save_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "  def create_gif(filenames, output_gif='output.gif'):\n",
    "    images = []\n",
    "    # Read each image file using iio.imread and append it to the list\n",
    "    for filename in filenames:\n",
    "        images.append(iio.imread(filename))\n",
    "    # Write the images as an animated GIF using iio.imwrite\n",
    "    iio.imwrite(output_gif, images, duration=500, loop=0)\n",
    "\n",
    "  create_gif(saved_path_list_for_gif,output_gif=save_path/'output.gif')\n",
    "  print(f\"Gif created: {save_path/'output.gif'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0,4000,100)\n",
    "data1 = runs_data_dict[key].iloc[2*t,:].to_dict()\n",
    "data1['t(M)'].values()\n",
    "for key in data1.keys():\n",
    "  print(key.split(\" \")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"high_accuracy_L5\"\n",
    "t_list = np.arange(0,4000,100)\n",
    "t_list = [2000]\n",
    "\n",
    "# save_path = Path(\"/home/hchaudha/notes/spec_accuracy/del/domain_plots\")/key\n",
    "# if not save_path.exists():\n",
    "#   save_path.mkdir()\n",
    "# print(save_path)\n",
    "\n",
    "save_path = None\n",
    "\n",
    "nA=4\n",
    "rA=nA*1.5\n",
    "center_xA=rA + 2\n",
    "RA=rA+5\n",
    "rC=RA*2\n",
    "nC=30\n",
    "RC=rC+nC\n",
    "\n",
    "\n",
    "for t in t_list:\n",
    "  fig, ax = plt.subplots(figsize=(12, 10))\n",
    "  pandas_dict = runs_data_dict[key].iloc[2*t,:].to_dict()\n",
    "  time = pandas_dict.pop(\"t(M)\")\n",
    "  domain_color_local,sm = scalar_to_color(pandas_dict,(-10,-2),color_map='RdYlGn_r')\n",
    "\n",
    "  patches_class = BBH_domain_sym_ploy(center_xA=center_xA, rA=rA, RA=RA, rC=rC, RC=RC, nA=nA, nC=nC, color_dict=domain_color_local) \n",
    "  for patch in patches_class.patches:\n",
    "    ax.add_patch(patch)\n",
    "\n",
    "  ax.set_xlim(-RC, RC)\n",
    "  ax.set_ylim(-RC, RC)\n",
    "  ax.set_aspect('equal')\n",
    "\n",
    "  cbar = plt.colorbar(sm, ax=ax)\n",
    "  cbar.set_label(f\"{list(pandas_dict.keys())[0].split(' ')[0]}\")\n",
    "  plt.title(f\"{key}  t(M) = {time}\")\n",
    "  plt.tight_layout()\n",
    "  if save_path is not None:\n",
    "    print(save_path/f\"{t}.png\")\n",
    "    plt.savefig(save_path/f\"{t}.png\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot and save all y_axis in this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving_avg_len=25\n",
    "plt.close()\n",
    "save_path = Path(\"/home/hchaudha/notes/spec_accuracy/del/L35_all/\")\n",
    "for file in save_path.glob('*'):  # Use '*' to match all files\n",
    "    if file.is_file():  # Check if it's a file\n",
    "        file.unlink()  # Remove the file\n",
    "save_path = str(save_path)+\"/\"\n",
    "\n",
    "# save_path = \"/home/hchaudha/notes/spec_accuracy/uniAMR_comparisons/all_set3/\"\n",
    "diff_base = None\n",
    "\n",
    "plot_fun = lambda x,y,label : plt.semilogy(x,y,label=label,marker='x')\n",
    "plot_fun = lambda x,y,label : plt.semilogy(x,y,label=label)\n",
    "plot_fun = lambda x,y,label : plt.plot(x,y,label=label,marker='x')\n",
    "# plot_fun = lambda x,y,label : plt.plot(x,y,label=label)\n",
    "y_lower,y_upper=1e-8,1\n",
    "y_lower,y_upper=1e-13,1e-4\n",
    "y_lower,y_upper=1e-12,1e-3\n",
    "\n",
    "legend_dict = {}\n",
    "for key in runs_data_dict.keys():\n",
    "  legend_dict[key] = None\n",
    "\n",
    "minT = 0\n",
    "# minT = 1800\n",
    "maxT = 28000\n",
    "# maxT = 1200\n",
    "\n",
    "x_axis = 't(M)'\n",
    "\n",
    "for y_axis in column_names:\n",
    "  if y_axis == x_axis:\n",
    "    continue\n",
    "  try:\n",
    "    with plt.style.context('default'):\n",
    "      plt.rcParams[\"figure.figsize\"] = (12,10)\n",
    "      plt.rcParams[\"figure.autolayout\"] = True\n",
    "      # plt.ylim(y_lower,y_upper)\n",
    "      plot_graph_for_runs(runs_data_dict, x_axis, y_axis, minT, maxT, legend_dict=legend_dict, save_path=save_path, moving_avg_len=moving_avg_len, plot_fun=plot_fun, diff_base=diff_base)\n",
    "      plt.close()\n",
    "    print(f\"{y_axis} done!\")\n",
    "  except Exception as e:\n",
    "      print(f\"Error plotting {y_axis}: {str(e)}\")\n",
    "      continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = runs_data_dict['all_100_t2690_obs']\n",
    "minT = 2690\n",
    "minT = 2700\n",
    "maxT = 2710\n",
    "maxT = minT+2\n",
    "df = df[df['time']>minT]\n",
    "df = df[df['time']<maxT]\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# plt.plot(df['time'],df['Linf(GhCe) on SphereA0'],marker='x')\n",
    "# plt.plot(df['time'],df['Linf(GhCe) on SphereA4'],marker='x')\n",
    "plt.plot(df['time'],df['Linf(GhCe) on SphereA3'],marker='x')\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = runs_data_dict['all_100_t2690_obs_grid_tol_10']\n",
    "data = data[data['time after step']>2690]\n",
    "dt_arr = np.array(data.dt)\n",
    "averaged_dt = np.zeros_like(dt_arr)\n",
    "averaged_dt[0] =  dt_arr.mean() \n",
    "N = 100\n",
    "for i in range(len(dt_arr)-1):\n",
    "  averaged_dt[i+1] = averaged_dt[i]*(N-1)/(N)+dt_arr[i+1]*1/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['time after step'],averaged_dt)\n",
    "plt.plot(data['time after step'],dt_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot max/min value for a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_file_path=\"ConstraintNorms/GhCe_L2.dat\"\n",
    "data_file_path=\"ConstraintNorms/GhCe_Linf.dat\"\n",
    "data_file_path=\"ConstraintNorms/NormalizedGhCe_Linf.dat\"\n",
    "# data_file_path=\"ConstraintNorms/Constraints_Linf.dat\"\n",
    "# data_file_path=\"MinimumGridSpacing.dat\"\n",
    "# data_file_path=\"ConstraintNorms/GhCe_VolL2.dat\"\n",
    "# data_file_path = \"ConstraintNorms/NormalizedGhCe_Norms.dat\"\n",
    "column_names_linf2, runs_data_dict_linf2 = load_data_from_levs(runs_to_plot,data_file_path)\n",
    "# print(column_names_linf2)\n",
    "print(runs_data_dict_linf2.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_name = list(runs_data_dict_linf2.keys())[4]\n",
    "# run_name = '3555_1.0e-07_060'\n",
    "run_name = 'high_accuracy_L5'\n",
    "# run_name = 'high_accuracy_L3'\n",
    "# run_name = '6_set2_L3s3'\n",
    "# run_name = '6_set2_L6s6'\n",
    "# run_name = '6_set1_L3s0'\n",
    "# run_name = '6_set1_L3s3'\n",
    "df = runs_data_dict_linf2[run_name].copy()\n",
    "df = df.sort_values(by=df.columns[0])\n",
    "save_path = None\n",
    "# save_path = \"/home/hchaudha/notes/spec_accuracy/del/normalized_norms/all/\"\n",
    "\n",
    "max_or_min = \"MAX\"\n",
    "# max_or_min = \"MIN\"\n",
    "\n",
    "tmin=0\n",
    "# tmin= 1200\n",
    "# tmin= 2050\n",
    "tmin= 3000\n",
    "# tmin=9300\n",
    "# tmin=9372\n",
    "# tmin=2691\n",
    "tmax=50000\n",
    "tmax=4000\n",
    "# tmax=tmin+4\n",
    "# tmax=7000\n",
    "# tmax=2800\n",
    "\n",
    "df = df[df['t(M)']>=tmin]\n",
    "df = df[df['t(M)']<tmax]\n",
    "t_name = df.columns[0]\n",
    "y_axis = df.columns[1].split(\" \")[0]\n",
    "all_cols_but_t = df.columns[1:]\n",
    "all_cols_but_t = []\n",
    "\n",
    "def find_sphere_num(string:str):\n",
    "  match = re.search(r'Sphere([A-C])(\\d{1,2})', string)\n",
    "\n",
    "  if match:\n",
    "      letter = match.group(1)  # Extract the letter (A, B, or C)\n",
    "      number = match.group(2)  # Extract the number (one or two digits)\n",
    "      return letter,int(number)\n",
    "  else:\n",
    "      print(f\"No Sphere found in the {string}.\")\n",
    "      return None\n",
    "\n",
    "only_include = None\n",
    "# only_include = [\n",
    "#   r\"SphereA[0-9]\",\n",
    "#   r\"SphereB[0-9]\",\n",
    "#   r\"SphereC[0-9]$\",\n",
    "#   r\"SphereC1[0-9]\",\n",
    "#   r\"SphereC2[0-9]\",\n",
    "#   r\"CylinderE[A,B]\\d\",\n",
    "#   r\"CylinderC[A,B]\\d\",\n",
    "#   r\"CylinderSM[A,B]\\d\",\n",
    "#   r\"FilledCylinderE[A,B]\\d\",\n",
    "#   r\"FilledCylinderC[A,B]\\d\",\n",
    "#   r\"FilledCylinderM[A,B]\\d\",\n",
    "# ]\n",
    "# only_include = [\n",
    "#   r\"SphereC\\d$\",\n",
    "#   r\"SphereC\\d\\d$\",\n",
    "# ]\n",
    "exclude = None\n",
    "# exclude = [\n",
    "#    r\"SphereC[6-9]\",\n",
    "# ]\n",
    "# exclude = [\n",
    "#    r\"1Con\",\n",
    "#    r\"2Con\",\n",
    "# #    r\"3Con\",\n",
    "# ]\n",
    "def matches_any_pattern(label, patterns):\n",
    "    for pattern in patterns:\n",
    "        if re.search(pattern, label):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "SphereCMin,SphereCMax = 0,22\n",
    "# SphereCMin,SphereCMax = 30,50\n",
    "\n",
    "SphereAMin,SphereAMax = 0,9\n",
    "SphereBMin,SphereBMax = 0,9\n",
    "# SphereAMin,SphereAMax = 5,9\n",
    "# SphereBMin,SphereBMax = 5,9\n",
    "for i in df.columns[1:]:\n",
    "  # Things to include, this overrides things to exclude\n",
    "  if only_include is None:\n",
    "    pass\n",
    "  elif matches_any_pattern(i,only_include):\n",
    "    # We are allowed to include this domain\n",
    "    pass\n",
    "  else:\n",
    "    # We are not supposed to include this domain\n",
    "    continue\n",
    "  \n",
    "  if exclude is None:\n",
    "    pass\n",
    "  elif matches_any_pattern(i,exclude):\n",
    "    continue\n",
    "  else:\n",
    "    pass\n",
    "  # Things to exclude\n",
    "  if 'SphereC' in i:\n",
    "    _, sphereC_num = find_sphere_num(i)\n",
    "    if (sphereC_num < SphereCMin) or (sphereC_num > SphereCMax):\n",
    "      print(i)\n",
    "      continue\n",
    "  if 'SphereA' in i :\n",
    "    _, sphere_num = find_sphere_num(i)\n",
    "    if (sphere_num < SphereAMin) or (sphere_num > SphereAMax):\n",
    "      print(i)\n",
    "      continue\n",
    "  if 'SphereB' in i:\n",
    "    _, sphere_num = find_sphere_num(i)\n",
    "    if (sphere_num < SphereBMin) or (sphere_num > SphereBMax):\n",
    "      print(i)\n",
    "      continue\n",
    "  if 't(M)' in i:\n",
    "    print(i)\n",
    "    continue\n",
    "  all_cols_but_t.append(i)\n",
    "\n",
    "if max_or_min == \"MAX\":\n",
    "  # Find the maximum value across columns B, C, D, and F for each row\n",
    "  df['extreme_val'] = df[all_cols_but_t].max(axis=1)\n",
    "\n",
    "  # Determine which column had the maximum value\n",
    "  df['extreme_source'] = df[all_cols_but_t].idxmax(axis=1)\n",
    "\n",
    "if max_or_min == \"MIN\":\n",
    "  # Find the maximum value across columns B, C, D, and F for each row\n",
    "  df['extreme_val'] = df[all_cols_but_t].min(axis=1)\n",
    "\n",
    "  # Determine which column had the maximum value\n",
    "  df['extreme_source'] = df[all_cols_but_t].idxmin(axis=1)\n",
    "\n",
    "# List all columns that have at least one extreme value\n",
    "columns_with_extreme = df['extreme_source'].unique()\n",
    "\n",
    "# Generate a colormap for the columns with at least one extreme value\n",
    "num_colors = len(columns_with_extreme)\n",
    "colors = plt.get_cmap('tab20', num_colors)  # Using 'tab20' colormap\n",
    "color_map = {column: colors(i) for i, column in enumerate(columns_with_extreme)}\n",
    "\n",
    "# Plot max_BCD vs t with different colors for different sources\n",
    "plt.figure(figsize=(18, 10))\n",
    "for i,source in enumerate(columns_with_extreme):\n",
    "    subset = df[df['extreme_source'] == source]\n",
    "    if i%4 == 0:\n",
    "        plt.scatter(subset[t_name], subset['extreme_val'], color=color_map[source], label=source, s=10, marker=\"^\")\n",
    "    if i%4 == 1:\n",
    "        plt.scatter(subset[t_name], subset['extreme_val'], color=color_map[source], label=source, s=10, marker=\"v\")\n",
    "    if i%4 == 2:\n",
    "        plt.scatter(subset[t_name], subset['extreme_val'], color=color_map[source], label=source, s=10, marker=\">\")\n",
    "    if i%4 == 3:\n",
    "        plt.scatter(subset[t_name], subset['extreme_val'], color=color_map[source], label=source, s=10, marker=\"<\")\n",
    "\n",
    "plt.xlabel(t_name)\n",
    "plt.ylabel(y_axis)\n",
    "plt.yscale('log')\n",
    "plt.title(f'{max_or_min}:{y_axis} vs {t_name} for {run_name} : A_{SphereAMin}_{SphereAMax}_B_{SphereBMin}_{SphereBMax}_C_{SphereCMin}_{SphereCMax}')\n",
    "plt.legend()\n",
    "plt.grid(True)  \n",
    "plt.tight_layout()\n",
    "if save_path is None:\n",
    "   save_path = \"/groups/sxs/hchaudha/rough/\"\n",
    "plt.savefig(f\"{save_path}{run_name}_{max_or_min}:{y_axis}_vs_{t_name}_{tmin}_{tmax}_A_{SphereAMin}_{SphereAMax}_B_{SphereBMin}_{SphereBMax}_C_{SphereCMin}_{SphereCMax}.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots all ys for a single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_file_path=\"ConstraintNorms/GhCe_L2.dat\"\n",
    "data_file_path=\"ConstraintNorms/GhCe_Linf.dat\"\n",
    "data_file_path=\"ConstraintNorms/NormalizedGhCe_Linf.dat\"\n",
    "data_file_path=\"MinimumGridSpacing.dat\"\n",
    "# data_file_path=\"ConstraintNorms/GhCe_VolL2.dat\"\n",
    "column_names_linf3, runs_data_dict_linf3 = load_data_from_levs(runs_to_plot,data_file_path)\n",
    "# print(column_names_linf3)\n",
    "print(runs_data_dict_linf3.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_name = list(runs_data_dict_linf2.keys())[4]\n",
    "run_name = '119_gd_SUKS_3_20'\n",
    "# run_name = 'high_accuracy_L4'\n",
    "# run_name = 'high_accuracy_L3'\n",
    "# run_name = '6_set2_L3s3'\n",
    "# run_name = '6_set2_L6s6'\n",
    "# run_name = '6_set3_L6s6'\n",
    "df = runs_data_dict_linf3[run_name].copy()\n",
    "df = df.sort_values(by=df.columns[0])\n",
    "save_path = None\n",
    "# save_path = \"/home/hchaudha/notes/spec_accuracy/del/normalized_norms/all/\"\n",
    "\n",
    "# tmin= 1200\n",
    "tmin=0\n",
    "# tmin= 2050\n",
    "# tmin=9300\n",
    "# tmin=9372\n",
    "# tmin=2691\n",
    "tmax=50000\n",
    "# tmax=2000\n",
    "# tmax=tmin+4\n",
    "# tmax=7000\n",
    "# tmax=2800\n",
    "\n",
    "df = df[df['t(M)']>=tmin]\n",
    "df = df[df['t(M)']<tmax]\n",
    "t_name = df.columns[0]\n",
    "y_axis = df.columns[1].split(\" \")[0]\n",
    "all_cols_but_t = df.columns[1:]\n",
    "all_cols_but_t = []\n",
    "\n",
    "def find_sphere_num(string:str):\n",
    "  match = re.search(r'Sphere([A-C])(\\d{1,2})', string)\n",
    "\n",
    "  if match:\n",
    "      letter = match.group(1)  # Extract the letter (A, B, or C)\n",
    "      number = match.group(2)  # Extract the number (one or two digits)\n",
    "      return letter,int(number)\n",
    "  else:\n",
    "      print(f\"No Sphere found in the {string}.\")\n",
    "      return None\n",
    "\n",
    "SphereCMin,SphereCMax = 0,30\n",
    "# SphereCMin,SphereCMax = 30,50\n",
    "\n",
    "SphereAMin,SphereAMax = 1,9\n",
    "SphereBMin,SphereBMax = 1,9\n",
    "# SphereAMin,SphereAMax = 5,9\n",
    "# SphereBMin,SphereBMax = 5,9\n",
    "for i in df.columns[1:]:\n",
    "  if 'SphereC' in i:\n",
    "    _, sphereC_num = find_sphere_num(i)\n",
    "    if (sphereC_num < SphereCMin) or (sphereC_num > SphereCMax):\n",
    "      print(i)\n",
    "      continue\n",
    "  if 'SphereA' in i :\n",
    "    _, sphere_num = find_sphere_num(i)\n",
    "    if (sphere_num < SphereAMin) or (sphere_num > SphereAMax):\n",
    "      print(i)\n",
    "      continue\n",
    "  if 'SphereB' in i:\n",
    "    _, sphere_num = find_sphere_num(i)\n",
    "    if (sphere_num < SphereBMin) or (sphere_num > SphereBMax):\n",
    "      print(i)\n",
    "      continue\n",
    "  if 't(M)' in i:\n",
    "    print(i)\n",
    "    continue\n",
    "  all_cols_but_t.append(i)\n",
    "\n",
    "if max_or_min == \"MAX\":\n",
    "  # Find the maximum value across columns B, C, D, and F for each row\n",
    "  df['extreme_val'] = df[all_cols_but_t].max(axis=1)\n",
    "\n",
    "  # Determine which column had the maximum value\n",
    "  df['extreme_source'] = df[all_cols_but_t].idxmax(axis=1)\n",
    "\n",
    "if max_or_min == \"MIN\":\n",
    "  # Find the maximum value across columns B, C, D, and F for each row\n",
    "  df['extreme_val'] = df[all_cols_but_t].min(axis=1)\n",
    "\n",
    "  # Determine which column had the maximum value\n",
    "  df['extreme_source'] = df[all_cols_but_t].idxmin(axis=1)\n",
    "\n",
    "# List all columns that have at least one extreme value\n",
    "columns_with_extreme = df['extreme_source'].unique()\n",
    "\n",
    "# Generate a colormap for the columns with at least one extreme value\n",
    "num_colors = len(columns_with_extreme)\n",
    "colors = plt.get_cmap('tab20', num_colors)  # Using 'tab20' colormap\n",
    "color_map = {column: colors(i) for i, column in enumerate(columns_with_extreme)}\n",
    "\n",
    "# Plot max_BCD vs t with different colors for different sources\n",
    "plt.figure(figsize=(18, 10))\n",
    "for i,source in enumerate(columns_with_extreme):\n",
    "    subset = df[df['extreme_source'] == source]\n",
    "    if i%4 == 0:\n",
    "        plt.scatter(subset[t_name], subset['extreme_val'], color=color_map[source], label=source, s=10, marker=\"^\")\n",
    "    if i%4 == 1:\n",
    "        plt.scatter(subset[t_name], subset['extreme_val'], color=color_map[source], label=source, s=10, marker=\"v\")\n",
    "    if i%4 == 2:\n",
    "        plt.scatter(subset[t_name], subset['extreme_val'], color=color_map[source], label=source, s=10, marker=\">\")\n",
    "    if i%4 == 3:\n",
    "        plt.scatter(subset[t_name], subset['extreme_val'], color=color_map[source], label=source, s=10, marker=\"<\")\n",
    "\n",
    "plt.xlabel(t_name)\n",
    "plt.ylabel(y_axis)\n",
    "# plt.yscale('log')\n",
    "plt.title(f'{max_or_min}:{y_axis} vs {t_name} for {run_name} : A_{SphereAMin}_{SphereAMax}_B_{SphereBMin}_{SphereBMax}_C_{SphereCMin}_{SphereCMax}')\n",
    "plt.legend()\n",
    "plt.grid(True)  \n",
    "plt.tight_layout()\n",
    "if save_path is None:\n",
    "   save_path = \"/groups/sxs/hchaudha/rough/\"\n",
    "plt.savefig(f\"{save_path}{run_name}_{max_or_min}:{y_axis}_vs_{t_name}_{tmin}_{tmax}_A_{SphereAMin}_{SphereAMax}_B_{SphereBMin}_{SphereBMax}_C_{SphereCMin}_{SphereCMax}.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plots for h5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_to_plot = {}\n",
    "# runs_to_plot[\"high_accuracy_L0\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev0_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L1\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev1_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L2\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev2_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L3\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L4\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L45\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev45_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L5\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L55\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev55_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L6\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev6_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"high_accuracy_L5_three_tier\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev5_big_gaussian/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L5_three_tier_constra\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev5_big_gaussian_constra/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L5_three_tier_constra200\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev5_big_gaussian_constra_200/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"L3_step_bound_gauss_error\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L3_step_bound_gauss_error/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_step_bound_gauss_error_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L3_step_bound_gauss_error/Ev/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"Lev5_big_gaussian_ah_tol10\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev5_big_gaussian_ah_tol10/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"Lev5_big_gaussian_ah_tol100\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev5_big_gaussian_ah_tol100/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"Lev5_bg_ah100_cd_01_uamr\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev5_bg_ah100_cd_01_uamr_full/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"Lev5_bg_ah100_lapse\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev5_bg_ah100_lapse_full/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"Lev5_bg_ah100_lapse_uamr\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_variations/Lev5_bg_ah100_lapse_uamr_full/Lev5_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"high_accuracy_L0_main\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/Ev/Lev0_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L1_main\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/Ev/Lev1_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L2_main\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/Ev/Lev2_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L3_main\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L4_main\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L45_main\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/Ev/Lev45_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L5_main\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/Ev/Lev5_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L55_main\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/Ev/Lev55_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L6_main\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/Ev/Lev6_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"ode_impro_Lev0\"] = '/groups/sxs/hchaudha/spec_runs/Lev01_test/new_ode_tol/high_accuracy_L35/Ev/Lev0_A?/Run/'\n",
    "# runs_to_plot[\"ode_impro_Lev1\"] = '/groups/sxs/hchaudha/spec_runs/Lev01_test/new_ode_tol/high_accuracy_L35/Ev/Lev1_A?/Run/'\n",
    "# runs_to_plot[\"ode_impro_Lev2\"] = '/groups/sxs/hchaudha/spec_runs/Lev01_test/new_ode_tol/high_accuracy_L35/Ev/Lev2_A?/Run/'\n",
    "# runs_to_plot[\"ode_impro_Lev3\"] = '/groups/sxs/hchaudha/spec_runs/Lev01_test/new_ode_tol/high_accuracy_L35/Ev/Lev3_A?/Run/'\n",
    "# runs_to_plot[\"ode_impro_Lev4\"] = '/groups/sxs/hchaudha/spec_runs/Lev01_test/new_ode_tol/high_accuracy_L35/Ev/Lev4_A?/Run/'\n",
    "# runs_to_plot[\"ode_impro_Lev5\"] = '/groups/sxs/hchaudha/spec_runs/Lev01_test/new_ode_tol/high_accuracy_L35/Ev/Lev5_A?/Run/'\n",
    "# runs_to_plot[\"main_Lev0\"] = '/groups/sxs/hchaudha/spec_runs/Lev01_test/old_ode_tol/high_accuracy_L35_master/Ev/Lev0_A?/Run/'\n",
    "# runs_to_plot[\"main_Lev2\"] = '/groups/sxs/hchaudha/spec_runs/Lev01_test/old_ode_tol/high_accuracy_L35_master/Ev/Lev2_A?/Run/'\n",
    "# runs_to_plot[\"main_Lev1\"] = '/groups/sxs/hchaudha/spec_runs/Lev01_test/old_ode_tol/high_accuracy_L35_master/Ev/Lev1_A?/Run/'\n",
    "\n",
    "# runs_to_plot[\"high_accuracy_L5\"] =  \"/groups/sxs/hchaudha/spec_runs/truncation_error_diagnostics/high_accuracy_L35_Lev5/\"\n",
    "# runs_to_plot[\"high_accuracy_L5_main\"] =  \"/groups/sxs/hchaudha/spec_runs/truncation_error_diagnostics/high_accuracy_L35_master_Lev5/\"\n",
    "\n",
    "# runs_to_plot[\"6_set1_L3s0\"] = '/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L3/h5_files_Lev0/'\n",
    "# runs_to_plot[\"6_set1_L3s1\"] = '/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L3/h5_files_Lev1/'\n",
    "# runs_to_plot[\"6_set1_L3s2\"] = '/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L3/h5_files_Lev2/'\n",
    "# runs_to_plot[\"6_set1_L3s3\"] = '/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L3/h5_files_Lev3/'\n",
    "\n",
    "runs_to_plot[\"6_set1_L6s4\"] = '/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/h5_files_Lev4/'\n",
    "runs_to_plot[\"6_set1_L6s5\"] = '/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/h5_files_Lev5/'\n",
    "runs_to_plot[\"6_set1_L6s6\"] = '/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/h5_files_Lev6/'\n",
    "\n",
    "# runs_to_plot[\"6_set2_L6s4\"] = '/groups/sxs/hchaudha/spec_runs/6_segs/6_set2_L6/h5_files_Lev4/'\n",
    "# runs_to_plot[\"6_set2_L6s5\"] = '/groups/sxs/hchaudha/spec_runs/6_segs/6_set2_L6/h5_files_Lev5/'\n",
    "# runs_to_plot[\"6_set2_L6s6\"] = '/groups/sxs/hchaudha/spec_runs/6_segs/6_set2_L6/h5_files_Lev6/'\n",
    "\n",
    "# runs_to_plot[\"6_set3_L6s4\"] = '/groups/sxs/hchaudha/spec_runs/6_segs/6_set3_L6/h5_files_Lev4/'\n",
    "# runs_to_plot[\"6_set3_L6s5\"] = '/groups/sxs/hchaudha/spec_runs/6_segs/6_set3_L6/h5_files_Lev5/'\n",
    "# runs_to_plot[\"6_set3_L6s6\"] = '/groups/sxs/hchaudha/spec_runs/6_segs/6_set3_L6/h5_files_Lev6/'\n",
    "\n",
    "domain = \"SphereC5\"\n",
    "\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Bf0I1_ConvergenceFactor.dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Bf0I1_HighestThirdConvergenceFactor.dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Bf0I1_NumberOfFilteredModes.dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Bf0I1_NumberOfModes.dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Bf0I1_NumberOfNonFilteredNonZeroModes.dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Bf0I1_NumberOfPiledUpModes.dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Bf0I1_PowerInFilteredModes.dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Bf0I1_PowerInHighestUnfilteredModes.dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Bf0I1_PredictedTruncationErrorForOneLessMode.dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Bf0I1_RawConvergenceFactor.dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Bf0I1_SpectrumIsDegenerate.dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Bf0I1_TruncationError.dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Bf1S2_ConvergenceFactor.dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Bf1S2_HighestThirdConvergenceFactor.dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Bf1S2_NumberOfFilteredModes.dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Bf1S2_NumberOfModes.dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Bf1S2_NumberOfNonFilteredNonZeroModes.dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Bf1S2_NumberOfPiledUpModes.dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Bf1S2_PowerInFilteredModes.dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Bf1S2_PowerInHighestUnfilteredModes.dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Bf1S2_PredictedTruncationErrorForOneLessMode.dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Bf1S2_RawConvergenceFactor.dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Bf1S2_SpectrumIsDegenerate.dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Bf1S2_TruncationError.dat\"\n",
    "\n",
    "psi_or_kappa = \"psi\"\n",
    "psi_or_kappa = \"kappa\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir/Bf0I1(10 modes).dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir/Bf0I1(11 modes).dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir/Bf0I1(12 modes).dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir/Bf0I1(13 modes).dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir/Bf1S2(20 modes).dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir/Bf1S2(21 modes).dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir/Bf1S2(22 modes).dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir/Bf1S2(23 modes).dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir/Bf1S2(24 modes).dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir/Bf1S2(25 modes).dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir/Bf1S2(26 modes).dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir/Bf1S2(27 modes).dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir/Bf1S2(28 modes).dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir/Bf1S2(29 modes).dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir/Bf1S2(30 modes).dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir/Bf1S2(31 modes).dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir/Bf1S2(32 modes).dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir/Bf1S2(33 modes).dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir/Bf1S2(34 modes).dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir/Bf1S2(35 modes).dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir/Bf1S2(36 modes).dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir/Bf1S2(37 modes).dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir/Bf1S2(38 modes).dat\"\n",
    "# data_file_path = f\"extracted-PowerDiagnostics/{domain}.dir/Power{psi_or_kappa}.dir/Bf1S2(41 modes).dat\"\n",
    "\n",
    "# data_file_path = \"extracted-FilterDiagnostics/BoundaryFilters.dir/ExpChebCoef.dir/SliceLFF.SphereA0.dat\"\n",
    "# data_file_path = \"extracted-ProjectedCon/Subdomains.dir/SphereA0.dir/NormOf2Con.dat\"\n",
    "data_file_path = \"extracted-ProjectedCon/Subdomains.dir/SphereC5.dir/NormOf2Con.dat\"\n",
    "# data_file_path = \"extracted-PowerDiagnostics/SphereA0.dir/Bf0I1_NumberOfModes.dat\"\n",
    "# data_file_path = \"extracted-PowerDiagnostics/FilledCylinderEA0.dir/Bf0I1_TruncationError.dat\"\n",
    "# data_file_path = \"extracted-RhsExpense/CostPerProc.dir/Proc00.dat\"\n",
    "# data_file_path = \"extracted-RhsExpense/CostPerSubdomain.dir/SphereA0.dat\"\n",
    "# data_file_path = \"extracted-RhsExpense/CostPerSubdomain.dir/SphereB2.dat\"\n",
    "data_file_path = \"extracted-OrbitDiagnostics/OrbitalPhase.dat\"\n",
    "# data_file_path = \"extracted-AdjustGridExtents/SphereC5.dir/Bf0I1.dat\"\n",
    "# data_file_path = \"extracted-AdjustGridExtents/SphereA0.dir/Bf1S2.dat\"\n",
    "# data_file_path = \"extracted-AdjustGridExtents/SphereA0.dir/Extents.dat\"\n",
    "# data_file_path = \"extracted-AdjustGridExtents/SphereA0.dir/Size.dat\"\n",
    "# data_file_path = \"extracted-AdjustGridExtents/SphereC5.dir/Size.dat\"\n",
    "# data_file_path = \"extracted-ControlNthDeriv/ExpansionFactor.dir/a.dat\"\n",
    "# data_file_path = \"extracted-ControlNthDeriv/Trans.dir/Tx.dat\"\n",
    "# data_file_path = \"extracted-FilterDiagnostics/SubdomainFilters.dir/ExpChebCoef.dir/SphereB0.dat\"\n",
    "\n",
    "column_names, runs_data_dict = load_data_from_levs(runs_to_plot,data_file_path)\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_avg_len=0\n",
    "save_path = None\n",
    "diff_base = None\n",
    "x_axis = 't(M)'\n",
    "\n",
    "diff_base = '6_set1_L6s6'\n",
    "# diff_base = '6_set1_L6s5'\n",
    "# diff_base = '6_set1_L3s3'\n",
    "# diff_base = 'high_accuracy_L5'\n",
    "# diff_base = 'ode_impro_Lev3'\n",
    "# add_max_and_min_val(runs_data_dict)\n",
    "# y_axis = 'max_val'\n",
    "# y_axis = 'min_val'\n",
    "\n",
    "# y_axis = 'coef0'\n",
    "# y_axis = 'coef1'\n",
    "# y_axis = 'MaxTruncationError'\n",
    "# y_axis = 'ShiftedTruncErrorMax'\n",
    "# y_axis = 'TruncationErrorExcess'\n",
    "# y_axis = 'MaxNextTruncationError'\n",
    "# y_axis = 'MinNumberOfPiledUpModes'\n",
    "# y_axis = 'GridDiagPowerkappa'\n",
    "# y_axis = 'GridDiagPowerpsi'\n",
    "# y_axis = 'CostPerPoint'\n",
    "# y_axis = 'Cost'\n",
    "# y_axis = 'NumberOfPoints'\n",
    "# y_axis = 'Q'\n",
    "# y_axis = 'lambda'\n",
    "# y_axis = 'Bf0I1'\n",
    "y_axis = 'phi'\n",
    "# y_axis = 'Extent[0]'\n",
    "# y_axis = 'Size'\n",
    "\n",
    "minT = 0\n",
    "minT = 1100\n",
    "# minT = 2700\n",
    "# minT = 9100\n",
    "\n",
    "maxT = 40000\n",
    "# maxT = 2710\n",
    "maxT = 4000\n",
    "# maxT = 9300\n",
    "# maxT = 9375\n",
    "# moving_avg_len = 50\n",
    "# moving_avg_len = 10\n",
    "\n",
    "plot_fun = lambda x,y,label : plt.plot(x,y,label=label)\n",
    "# plot_fun = lambda x,y,label : plt.plot(x,y,label=label,marker='x')\n",
    "# plot_fun = lambda x,y,label : plt.semilogy(x,y,label=label)\n",
    "# plot_fun = lambda x,y,label : plt.semilogy(x,y,label=label,marker='x')\n",
    "# plot_fun = lambda x,y,label : plt.loglog(x,y,label=label) \n",
    "# plot_fun = lambda x,y,label : plt.scatter(x,y,label=label,s=10,marker=\"x\")\n",
    "# save_path = \"/groups/sxs/hchaudha/rough/high_acc_plots/\"\n",
    "# save_path = \"/groups/sxs/hchaudha/rough/plots/\"\n",
    "# save_path = \"/home/hchaudha/notes/spec_accuracy/L5_comparisons/power_diagon/\"\n",
    "legend_dict = {}\n",
    "for key in runs_data_dict.keys():\n",
    "  legend_dict[key] = None\n",
    "\n",
    "# legend_dict = { '3_DH_q1_ns_d18_L3': \"Lev3\",\n",
    "#                 '3_DH_q1_ns_d18_L6': \"Lev6\",\n",
    "#                 'all_10': \"Lev3_all_tols_10\",\n",
    "#                 'all_100': \"Lev3_all_tols_100\",\n",
    "#                 'near_bhs_10': \"Lev3_sphere_AB_tols_10\",\n",
    "#                 'near_bhs_100': \"Lev3_sphere_AB_tols_100\"}\n",
    "# legend_dict = {\n",
    "#  '3_DH_q1_ns_d18_L3':\"Lev3_ode_tol_1e-8\",\n",
    "#  '3_DH_q1_ns_d18_L3_tol9':\"Lev3_ode_tol_1e-9\",\n",
    "#  '3_DH_q1_ns_d18_L3_tol10':\"Lev3_ode_tol_1e-10\",\n",
    "#  '3_DH_q1_ns_d18_L3_tol11':\"Lev3_ode_tol_1e-11\",\n",
    "#  '3_DH_q1_ns_d18_L3_all_100_tol10':\"Lev3_AMR_tol_100_ode_tol_1e-11\",\n",
    "#  }\n",
    "title = data_file_path\n",
    "# for key in runs_data_dict:\n",
    "#   title = title + \"_\" + \n",
    "with plt.style.context('default'):\n",
    "  plt.rcParams[\"figure.figsize\"] = (12,10)\n",
    "  plt.rcParams[\"figure.autolayout\"] = True\n",
    "  plot_graph_for_runs(runs_data_dict, x_axis, y_axis, minT, maxT, legend_dict=legend_dict, save_path=save_path, moving_avg_len=moving_avg_len, plot_fun=plot_fun, diff_base=diff_base, title=title)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots for one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = Path(\"/groups/sxs/hchaudha/spec_runs/\")\n",
    "run_path = main_path/Path(\"3_DH_q1_ns_d18_L3/Ev/Lev3_A?/Run/\")\n",
    "# run_path = main_path/Path(\"3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs/Lev3_AD/Run/\")\n",
    "# run_path = main_path/Path(\"3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs/Lev3_AD/Run/\")\n",
    "# run_path = main_path/Path(\"3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_tol_9/Lev3_AD/Run/\")\n",
    "# run_path = main_path/Path(\"3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_tol_10/Lev3_AD/Run/\")\n",
    "# run_path = main_path/Path(\"3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_tol_11/Lev3_AD/Run/\")\n",
    "\n",
    "data_files = {}\n",
    "\n",
    "# data_files[\"GrAdjustSubChunksToDampingTimes\"] = {\"path\":run_path/\"GrAdjustSubChunksToDampingTimes.dat\", \"prefix\":None}\n",
    "# data_files[\"MemoryInfo\"] = {\"path\":run_path/\"MemoryInfo.dat\", \"prefix\":None}\n",
    "# data_files[\"MinimumGridSpacing\"] = {\"path\":run_path/\"MinimumGridSpacing.dat\", \"prefix\":None}\n",
    "# data_files[\"DiagInclinationAngle\"] = {\"path\":run_path/\"DiagInclinationAngle.dat\", \"prefix\":None}\n",
    "# data_files[\"ApparentHorizons/Trajectory_AhA\"] = {\"path\":run_path/\"ApparentHorizons/Trajectory_AhA.dat\", \"prefix\":\"AhA\"}\n",
    "# data_files[\"ApparentHorizons/MinCharSpeedAhA\"] = {\"path\":run_path/\"ApparentHorizons/MinCharSpeedAhA.dat\", \"prefix\":\"AhA\"}\n",
    "# data_files[\"ApparentHorizons/Trajectory_AhB\"] = {\"path\":run_path/\"ApparentHorizons/Trajectory_AhB.dat\", \"prefix\":\"AhB\"}\n",
    "# data_files[\"ApparentHorizons/SmoothCoordSepHorizon\"] = {\"path\":run_path/\"ApparentHorizons/SmoothCoordSepHorizon.dat\", \"prefix\":None}\n",
    "# data_files[\"ApparentHorizons/MinCharSpeedAhB\"] = {\"path\":run_path/\"ApparentHorizons/MinCharSpeedAhB.dat\", \"prefix\":\"AhB\"}\n",
    "# data_files[\"ApparentHorizons/RescaledRadAhB\"] = {\"path\":run_path/\"ApparentHorizons/RescaledRadAhB.dat\", \"prefix\":\"AhB\"}\n",
    "# data_files[\"ApparentHorizons/AhACoefs\"] = {\"path\":run_path/\"ApparentHorizons/AhACoefs.dat\", \"prefix\":\"AhA\"}\n",
    "# data_files[\"ApparentHorizons/AhB\"] = {\"path\":run_path/\"ApparentHorizons/AhB.dat\", \"prefix\":\"AhB\"}\n",
    "# data_files[\"ApparentHorizons/HorizonSepMeasures\"] = {\"path\":run_path/\"ApparentHorizons/HorizonSepMeasures.dat\", \"prefix\":None}\n",
    "# data_files[\"ApparentHorizons/AhA\"] = {\"path\":run_path/\"ApparentHorizons/AhA.dat\", \"prefix\":\"AhA\"}\n",
    "# data_files[\"ApparentHorizons/RescaledRadAhA\"] = {\"path\":run_path/\"ApparentHorizons/RescaledRadAhA.dat\", \"prefix\":\"AhA\"}\n",
    "# data_files[\"ApparentHorizons/AhBCoefs\"] = {\"path\":run_path/\"ApparentHorizons/AhBCoefs.dat\", \"prefix\":\"AhB\"}\n",
    "# data_files[\"TimeInfo\"] = {\"path\":run_path/\"TimeInfo.dat\", \"prefix\":None}\n",
    "# data_files[\"GrAdjustMaxTstepToDampingTimes\"] = {\"path\":run_path/\"GrAdjustMaxTstepToDampingTimes.dat\", \"prefix\":None}\n",
    "# data_files[\"FailedTStepperDiag\"] = {\"path\":run_path/\"FailedTStepperDiag.dat\", \"prefix\":None}\n",
    "# data_files[\"DiagAhSpeedA\"] = {\"path\":run_path/\"DiagAhSpeedA.dat\", \"prefix\":\"AhA\"}\n",
    "# data_files[\"DiagAhSpeedB\"] = {\"path\":run_path/\"DiagAhSpeedB.dat\", \"prefix\":\"AhB\"}\n",
    "# data_files[\"CharSpeedNorms/CharSpeeds_Max_SliceLFF.SphereA0\"] = {\"path\":run_path/\"CharSpeedNorms/CharSpeeds_Max_SliceLFF.SphereA0.dat\", \"prefix\":\"Max_A0\"}\n",
    "# data_files[\"CharSpeedNorms/CharSpeeds_Min_SliceLFF.SphereA0\"] = {\"path\":run_path/\"CharSpeedNorms/CharSpeeds_Min_SliceLFF.SphereA0.dat\", \"prefix\":\"Min_A0\"}\n",
    "# data_files[\"CharSpeedNorms/CharSpeeds_Min_SliceLFF.SphereB0\"] = {\"path\":run_path/\"CharSpeedNorms/CharSpeeds_Min_SliceLFF.SphereB0.dat\", \"prefix\":\"Max_B0\"}\n",
    "# data_files[\"CharSpeedNorms/CharSpeeds_Max_SliceLFF.SphereB0\"] = {\"path\":run_path/\"CharSpeedNorms/CharSpeeds_Max_SliceLFF.SphereB0.dat\", \"prefix\":\"Min_B0\"}\n",
    "# data_files[\"CharSpeedNorms/CharSpeeds_Max_SliceUFF.SphereC29\"] = {\"path\":run_path/\"CharSpeedNorms/CharSpeeds_Max_SliceUFF.SphereC29.dat\", \"prefix\":\"Max_C29\"}\n",
    "# data_files[\"CharSpeedNorms/CharSpeeds_Min_SliceUFF.SphereC29\"] = {\"path\":run_path/\"CharSpeedNorms/CharSpeeds_Min_SliceUFF.SphereC29.dat\", \"prefix\":\"Min_C29\"}\n",
    "# data_files[\"ConstraintNorms/NormalizedGhCe_Norms\"] = {\"path\":run_path/\"ConstraintNorms/NormalizedGhCe_Norms.dat\", \"prefix\":None}\n",
    "# data_files[\"ConstraintNorms/GhCeExt_Norms\"] = {\"path\":run_path/\"ConstraintNorms/GhCeExt_Norms.dat\", \"prefix\":None}\n",
    "# data_files[\"ConstraintNorms/GhCe_L2\"] = {\"path\":run_path/\"ConstraintNorms/GhCe_L2.dat\", \"prefix\":None}\n",
    "# data_files[\"ConstraintNorms/GhCeExt_L2\"] = {\"path\":run_path/\"ConstraintNorms/GhCeExt_L2.dat\", \"prefix\":None}\n",
    "# data_files[\"ConstraintNorms/GhCeExt\"] = {\"path\":run_path/\"ConstraintNorms/GhCeExt.dat\", \"prefix\":None}\n",
    "# data_files[\"ConstraintNorms/GhCe\"] = {\"path\":run_path/\"ConstraintNorms/GhCe.dat\", \"prefix\":None}\n",
    "# data_files[\"ConstraintNorms/GhCe_VolL2\"] = {\"path\":run_path/\"ConstraintNorms/GhCe_VolL2.dat\", \"prefix\":None}\n",
    "data_files[\"ConstraintNorms/GhCe_Norms\"] = {\"path\":run_path/\"ConstraintNorms/GhCe_Norms.dat\", \"prefix\":None}\n",
    "# data_files[\"ConstraintNorms/NormalizedGhCe_Linf\"] = {\"path\":run_path/\"ConstraintNorms/NormalizedGhCe_Linf.dat\", \"prefix\":None}\n",
    "# data_files[\"ConstraintNorms/GhCe_Linf\"] = {\"path\":run_path/\"ConstraintNorms/GhCe_Linf.dat\", \"prefix\":None}\n",
    "data_files[\"TStepperDiag\"] = {\"path\":run_path/\"TStepperDiag.dat\", \"prefix\":None}\n",
    "# data_files[\"DiagCutXCorrection\"] = {\"path\":run_path/\"DiagCutXCorrection.dat\", \"prefix\":None}\n",
    "\n",
    "\n",
    "# data = read_dat_file_across_AA(str(data_files['TimeInfo']['path']))\n",
    "for key in data_files:\n",
    "  data_files[key][\"dataframe\"] = read_dat_file_across_AA(str(data_files[key]['path']))\n",
    "  cols = list(data_files[key][\"dataframe\"].columns)\n",
    "  # Make new cols names s.t. the first cols is 't' and add prefix as required\n",
    "  new_cols = []\n",
    "  new_cols.append('t')\n",
    "  if data_files[key]['prefix'] is None:\n",
    "    [new_cols.append(name) for name in cols[1:]]\n",
    "  else:\n",
    "    [new_cols.append(name+\"_\"+data_files[key]['prefix']) for name in cols[1:]]\n",
    "  data_files[key][\"dataframe\"].columns = new_cols\n",
    "\n",
    "  # Set 't' to be a index and copy it into a column called 'time'\n",
    "  # data_files[key][\"dataframe\"][\"time\"] = data_files[key][\"dataframe\"][\"t\"]\n",
    "  # data_files[key][\"dataframe\"].set_index('t', inplace=True)\n",
    "\n",
    "combined = None\n",
    "for key in data_files:\n",
    "  if combined is None:\n",
    "    combined = data_files[key][\"dataframe\"]\n",
    "    continue\n",
    "  else:\n",
    "    combined  = pd.merge(combined,data_files[key][\"dataframe\"],on='t',how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.sort_values(by='t')\n",
    "for i in combined.columns:\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(combined['t'],combined['SuggestedDampingTime'])\n",
    "\n",
    "x_val = 't'\n",
    "plot_list=[\n",
    "  # ('Linf(GhCe) on SphereA0','semilogy',None,None,'x'),\n",
    "  ('Linf(GhCe)','semilogy',None,None,'.'),\n",
    "  ('dt','plot',1e-3,None,None),\n",
    "]\n",
    "\n",
    "for i in plot_list:\n",
    "  y_val, plot_type, mul_factor, add_factor, marker = i\n",
    "\n",
    "  label = y_val\n",
    "  if mul_factor is not None:\n",
    "    label = f\"{label}*{mul_factor}\"\n",
    "  else:\n",
    "    mul_factor = 1\n",
    "  if add_factor is not None:\n",
    "    label = f\"{label}+{add_factor}\"\n",
    "  else:\n",
    "    add_factor = 0\n",
    "\n",
    "\n",
    "  match plot_type:\n",
    "    case 'semilogy':\n",
    "      plt.semilogy(combined[x_val],combined[y_val]*mul_factor+add_factor,marker=marker,label=label)\n",
    "    case 'plot':\n",
    "      plt.plot(combined[x_val],combined[y_val]*mul_factor+add_factor,marker=marker,label=label)\n",
    "\n",
    "title = str(run_path).split('/')[-4]\n",
    "save_name = str(run_path).split('/')[-4]\n",
    "for i in str(run_path).split('/')[-3:-1]:\n",
    "  title = title +\"/\" +i\n",
    "  save_name = save_name+\"&\"+i\n",
    "\n",
    "\n",
    "plt.title(title)\n",
    "plt.xlabel(x_val)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"/groups/sxs/hchaudha/rough/plots/{save_name}.png\",dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def check_duplicate_rows(df, subset_column):\n",
    "    \"\"\"\n",
    "    Function to find and print duplicate rows in a DataFrame based on a specified column.\n",
    "    It checks if all duplicate rows are identical or if they differ in some columns.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to check for duplicates.\n",
    "    subset_column (str): The column to check for duplicate values.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Get rows with duplicate values in the subset column\n",
    "    duplicate_mask = df.duplicated(subset=[subset_column], keep=False)\n",
    "    duplicate_rows = df[duplicate_mask]\n",
    "    \n",
    "    # Sort the duplicate rows by the subset column to group them together\n",
    "    duplicate_rows_sorted = duplicate_rows.sort_values(by=subset_column)\n",
    "    \n",
    "    # Iterate through groups of rows with the same value in the subset column\n",
    "    for _, group in duplicate_rows_sorted.groupby(subset_column):\n",
    "        if len(group) > 1:\n",
    "            print(f\"\\nRows with '{subset_column}' value: {group[subset_column].iloc[0]}\")\n",
    "            \n",
    "            # Check if all rows in the group are identical\n",
    "            identical = all(group.iloc[0].equals(row) for _, row in group.iterrows())\n",
    "            if identical:\n",
    "                print(\"All rows are identical:\")\n",
    "                print(group)\n",
    "            else:\n",
    "                print(\"Rows differ in some columns:\")\n",
    "                print(group)\n",
    "                \n",
    "                # Optionally, show which columns differ\n",
    "                differing_columns = group.columns[group.nunique() > 1].tolist()\n",
    "                print(f\"Columns with differences: {differing_columns}\")\n",
    "\n",
    "\n",
    "\n",
    "# Usage example\n",
    "# combined = pd.read_csv('your_data.csv')  # Load your data\n",
    "# check_duplicate_rows(combined, 't')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicate_rows(combined,'t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def check_duplicate_rows(df, subset_column):\n",
    "    # Get rows with duplicate values in the subset column\n",
    "    duplicate_mask = df.duplicated(subset=[subset_column], keep=False)\n",
    "    duplicate_rows = df[duplicate_mask]\n",
    "    \n",
    "    # Sort the duplicate rows by the subset column to group them together\n",
    "    duplicate_rows_sorted = duplicate_rows.sort_values(by=subset_column)\n",
    "    \n",
    "    # Iterate through groups of rows with the same value in the subset column\n",
    "    for _, group in duplicate_rows_sorted.groupby(subset_column):\n",
    "        if len(group) > 1:\n",
    "            print(f\"\\nRows with '{subset_column}' value: {group[subset_column].iloc[0]}\")\n",
    "            \n",
    "            # Check if all rows in the group are identical\n",
    "            if group.drop_duplicates().shape[0] == 1:\n",
    "                print(\"All rows are identical:\")\n",
    "                print(group)\n",
    "            else:\n",
    "                print(\"Rows differ in some columns:\")\n",
    "                print(group)\n",
    "                \n",
    "                # Show which columns differ\n",
    "                differing_columns = group.columns[group.nunique() > 1].tolist()\n",
    "                print(f\"Columns with differences: {differing_columns}\")\n",
    "                \n",
    "                # Show the differences\n",
    "                for col in differing_columns:\n",
    "                    if col != subset_column:\n",
    "                        print(f\"\\nDifferences in column '{col}':\")\n",
    "                        print(group[['t', col]])\n",
    "\n",
    "# Usage example\n",
    "# combined = pd.read_csv('your_data.csv')  # Load your data\n",
    "check_duplicate_rows(combined, 't')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.sort_values(by='t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['MinCharSpeedAhA[7]_AhA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = []\n",
    "for i in data:\n",
    "  print(i.columns.is_unique)\n",
    "  col_names = col_names+list(i.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(col_names), len(set(col_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(data,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linf plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_to_plot = {}\n",
    "runs_to_plot[\"t6115_tol8_linf\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol8_linf/Lev3_A?/Run/\"\n",
    "\n",
    "data_file_path=\"ConstraintNorms/Linf.dat\"\n",
    "\n",
    "\n",
    "column_names_linf, runs_data_dict_linf = load_data_from_levs(runs_to_plot,data_file_path)\n",
    "print(column_names_linf)\n",
    "print(runs_data_dict_linf.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = runs_data_dict_linf['t6115_tol8_linf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_list = []\n",
    "constraint_list = []\n",
    "component_list = []\n",
    "\n",
    "# domain_list.append(\"SphereB0\")\n",
    "domain_list.append(\"erMA0\")\n",
    "\n",
    "# constraint_list.append('1Con')\n",
    "constraint_list.append('2Con')\n",
    "# constraint_list.append('3Con')\n",
    "\n",
    "component_list.append('t')\n",
    "\n",
    "temp_list = copy.copy(column_names_linf)\n",
    "col_domains = []\n",
    "for col in temp_list:\n",
    "  for domain in domain_list:\n",
    "    if domain in col:\n",
    "      col_domains.append(col)\n",
    "\n",
    "temp_list = col_domains\n",
    "col_domains = []\n",
    "for col in temp_list:\n",
    "  for constraint in constraint_list:\n",
    "    if constraint in col:\n",
    "      col_domains.append(col)\n",
    "\n",
    "if len(component_list) > 0:\n",
    "  temp_list = col_domains\n",
    "  col_domains = []\n",
    "  for col in temp_list:\n",
    "    for component in component_list:\n",
    "      if component in col:\n",
    "        col_domains.append(col)\n",
    "\n",
    "col_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_avg_len = None\n",
    "\n",
    "x = 'time'\n",
    "# moving_avg_len = 50*3\n",
    "\n",
    "\n",
    "for col in col_domains:\n",
    "  if moving_avg_len is not None:\n",
    "    plt.semilogy(np.array(data[x])[moving_avg_len-1:],moving_average_valid(data[col],moving_avg_len),label=col)\n",
    "  else:\n",
    "    plt.semilogy(data[x],data[col],label=col)\n",
    "  # plt.plot(data[x],data[col],label=col)\n",
    "plt.xlabel(x)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(moving_average_valid(data[col],average_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot horizons.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"/groups/sxs/hchaudha/spec_runs\")\n",
    "runs_to_plot = {}\n",
    "\n",
    "# runs_to_plot[\"high_accuracy_L0\"] =  \"high_accuracy_L35/Ev/Lev0_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"high_accuracy_L1\"] =  \"high_accuracy_L35/Ev/Lev1_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"high_accuracy_L2\"] =  \"high_accuracy_L35/Ev/Lev2_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"high_accuracy_L3\"] =  \"high_accuracy_L35/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"high_accuracy_L4\"] =  \"high_accuracy_L35/Ev/Lev4_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"high_accuracy_L45\"] =  \"high_accuracy_L35/Ev/Lev45_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"high_accuracy_L5\"] =  \"high_accuracy_L35/Ev/Lev5_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"high_accuracy_L55\"] =  \"high_accuracy_L35/Ev/Lev55_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"high_accuracy_L6\"] =  \"high_accuracy_L35/Ev/Lev6_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"high_accuracy_L3_master\"] =  \"high_accuracy_L35_master/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"high_accuracy_L4_master\"] =  \"high_accuracy_L35_master/Ev/Lev4_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"high_accuracy_L5_master\"] =  \"high_accuracy_L35_master/Ev/Lev5_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"high_accuracy_L5_big_gauss\"] =  \"high_accuracy_L35_variations/Lev5_big_gaussian/Lev5_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"Lev5_big_gaussian_constra\"] =  \"high_accuracy_L35_variations/Lev5_big_gaussian_constra/Lev5_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"Lev5_big_gaussian_constra_200\"] =  \"high_accuracy_L35_variations/Lev5_big_gaussian_constra_200/Lev5_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"high_accuracy_L5_a10\"] =  \"high_accuracy_L35_variations/Lev5_big_gaussian_ah_tol10/Lev5_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"high_accuracy_L5_a100\"] =  \"high_accuracy_L35_variations/Lev5_big_gaussian_ah_tol100/Lev5_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"L3_step_bound_gauss_error\"] =  \"high_accuracy_L3_step_bound_gauss_error/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "\n",
    "# runs_to_plot[\"eq_AMR_3_tier_const\"] =  \"high_accuracy_L3_contraints/eq_AMR_3_tier_const/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"eq_AMR_3_tier_const_gamma2\"] =  \"high_accuracy_L3_contraints/eq_AMR_3_tier_const_gamma2/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"three_tier_AMR_const_L3\"] =  \"high_accuracy_L3_contraints/three_tier_AMR_const/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"normal_constraints\"]=\"high_accuracy_L3_contraints/eq_AMR_3_tier_const_variations/normal_constraints/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"normal_constraints_12_AB\"]=\"high_accuracy_L3_contraints/eq_AMR_3_tier_const_variations/normal_constraints_12_AB/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"normal_constraints_const1\"]=\"high_accuracy_L3_contraints/eq_AMR_3_tier_const_variations/normal_constraints_const1/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "\n",
    "# runs_to_plot[\"ode_impro_Lev0\"] = 'Lev01_test/new_ode_tol/high_accuracy_L35/Ev/Lev0_A?/Run/ApparentHorizons/Horizons.h5'\n",
    "# runs_to_plot[\"ode_impro_Lev1\"] = 'Lev01_test/new_ode_tol/high_accuracy_L35/Ev/Lev1_A?/Run/ApparentHorizons/Horizons.h5'\n",
    "# runs_to_plot[\"ode_impro_Lev2\"] = 'Lev01_test/new_ode_tol/high_accuracy_L35/Ev/Lev2_A?/Run/ApparentHorizons/Horizons.h5'\n",
    "# runs_to_plot[\"ode_impro_Lev3\"] = 'Lev01_test/new_ode_tol/high_accuracy_L35/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5'\n",
    "# runs_to_plot[\"ode_impro_Lev4\"] = 'Lev01_test/new_ode_tol/high_accuracy_L35/Ev/Lev4_A?/Run/ApparentHorizons/Horizons.h5'\n",
    "# runs_to_plot[\"ode_impro_Lev5\"] = 'Lev01_test/new_ode_tol/high_accuracy_L35/Ev/Lev5_A?/Run/ApparentHorizons/Horizons.h5'\n",
    "\n",
    "# runs_to_plot[\"main_Lev0\"] = 'Lev01_test/old_ode_tol/high_accuracy_L35_master/Ev/Lev0_A?/Run/ApparentHorizons/Horizons.h5'\n",
    "# runs_to_plot[\"main_Lev2\"] = 'Lev01_test/old_ode_tol/high_accuracy_L35_master/Ev/Lev2_A?/Run/ApparentHorizons/Horizons.h5'\n",
    "# runs_to_plot[\"main_Lev1\"] = 'Lev01_test/old_ode_tol/high_accuracy_L35_master/Ev/Lev1_A?/Run/ApparentHorizons/Horizons.h5'\n",
    "\n",
    "\n",
    "runs_to_plot[\"6_set1_L3s0\"] =  \"6_segs/6_set1_L3/Ev/Lev0_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "runs_to_plot[\"6_set1_L3s1\"] =  \"6_segs/6_set1_L3/Ev/Lev1_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "runs_to_plot[\"6_set1_L3s2\"] =  \"6_segs/6_set1_L3/Ev/Lev2_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "runs_to_plot[\"6_set1_L3s3\"] =  \"6_segs/6_set1_L3/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "\n",
    "runs_to_plot[\"6_set1_L6s4\"] =  \"6_segs/6_set1_L6/Ev/Lev4_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "runs_to_plot[\"6_set1_L6s5\"] =  \"6_segs/6_set1_L6/Ev/Lev5_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "runs_to_plot[\"6_set1_L6s6\"] =  \"6_segs/6_set1_L6/Ev/Lev6_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "\n",
    "# runs_to_plot[\"all_100\"] =  \"3_DH_q1_ns_d18_L3_higher_acc/all_100/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "data_dict = load_horizon_data_from_levs(base_path, runs_to_plot)\n",
    "data_dict = flatten_dict(data_dict)\n",
    "data_dict[list(data_dict.keys())[0]].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_avg_len = 0\n",
    "save_path = None\n",
    "\n",
    "x_axis = 't'\n",
    "y_axis = 'ArealMass'\n",
    "y_axis = 'ChristodoulouMass'\n",
    "# y_axis = 'CoordCenterInertial_0'\n",
    "# y_axis = 'CoordCenterInertial_1'\n",
    "# y_axis = 'CoordCenterInertial_2'\n",
    "# y_axis = 'DimensionfulInertialSpin_0'\n",
    "# y_axis = 'DimensionfulInertialSpin_1'\n",
    "# y_axis = 'DimensionfulInertialSpin_2'\n",
    "# y_axis = 'DimensionfulInertialCoordSpin_0'\n",
    "# y_axis = 'DimensionfulInertialCoordSpin_1'\n",
    "# y_axis = 'DimensionfulInertialCoordSpin_2'\n",
    "# y_axis = 'DimensionfulInertialSpinMag'\n",
    "# y_axis = 'SpinFromShape_0'\n",
    "# y_axis = 'SpinFromShape_1'\n",
    "# y_axis = 'SpinFromShape_2'\n",
    "# y_axis = 'SpinFromShape_3'\n",
    "# y_axis = 'chiInertial_0'\n",
    "# y_axis = 'chiInertial_1'\n",
    "# y_axis = 'chiInertial_2'\n",
    "# y_axis = 'chiMagInertial'\n",
    "\n",
    "\n",
    "\n",
    "# moving_avg_len=25\n",
    "minT = 0\n",
    "minT = 1200\n",
    "maxT = 25000\n",
    "# maxT = 7000\n",
    "maxT = 7500\n",
    "maxT = 4000\n",
    "\n",
    "plot_fun = lambda x,y,label : plt.plot(x,y,label=label)\n",
    "# plot_fun = lambda x,y,label : plt.semilogy(x,y,label=label)\n",
    "# plot_fun = lambda x,y,label : plt.loglog(x,y,label=label)\n",
    "# plot_fun = lambda x,y,label : plt.scatter(x,y,label=label)\n",
    "# save_path = \"/panfs/ds09/sxs/himanshu/scripts/report/not_tracked/temp2/\"\n",
    "\n",
    "filtered_dict = {}\n",
    "allowed_horizons = [\"AhA\"]\n",
    "for horizons in allowed_horizons:\n",
    "  for runs_keys in data_dict.keys():\n",
    "    if horizons in runs_keys:\n",
    "      filtered_dict[runs_keys] = data_dict[runs_keys]\n",
    " \n",
    "with plt.style.context('default'):\n",
    "  plt.rcParams[\"figure.figsize\"] = (12,10)\n",
    "  plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "  plt.rcParams[\"figure.autolayout\"] = True\n",
    "  plot_graph_for_runs(filtered_dict, x_axis, y_axis, minT, maxT, save_path=save_path, moving_avg_len=moving_avg_len, plot_fun=plot_fun)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bh = 'corrected_coord_spin2_AhB'\n",
    "y_axis1 = 'chiInertial_0'\n",
    "y_axis2 = 'CoordSpinChiInertial_0'\n",
    "\n",
    "X = data_dict[bh][x_axis]\n",
    "Y1 = data_dict[bh][y_axis1]\n",
    "Y2 = data_dict[bh][y_axis2]\n",
    "plt.plot(X,Y1,label=y_axis1)\n",
    "plt.plot(X,Y2,label=y_axis2)\n",
    "plt.xlabel(x_axis)\n",
    "# plt.ylabel(y_axis1+\" - \"+y_axis2)\n",
    "plt.legend()\n",
    "# plt.title()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = 't'\n",
    "y_axis = 'ChristodoulouMass'\n",
    "minT = 500\n",
    "maxT = 800\n",
    "run1 = filtered_dict['AccTest_q1ns_Lev5_AhA']\n",
    "# run1 = filtered_dict['AccTest_q1ns_Lev6p_AhA']\n",
    "run2 = filtered_dict['AccTest_q1ns_Lev6p_AhA']\n",
    "interp_grid_pts = run1[x_axis].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_run1 = CubicSpline(run1[x_axis].to_numpy(),run1[y_axis].to_numpy())\n",
    "interp_run2 = CubicSpline(run2[x_axis].to_numpy(),run2[y_axis].to_numpy())\n",
    "interp_grid = np.arange(minT,maxT,(maxT-minT)/interp_grid_pts)\n",
    "\n",
    "plt.plot(interp_grid, interp_run2(interp_grid) - interp_run1(interp_grid))\n",
    "plt.xlabel(x_axis)\n",
    "plt.ylabel(y_axis)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(interp_grid, interp_run2(interp_grid) - interp_run1(interp_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inertial_dist(run_name:str, data_dict):\n",
    "    ct = data_dict[f\"{run_name}_AhA\"].t\n",
    "    dx = data_dict[f\"{run_name}_AhA\"].CoordCenterInertial_0 - data_dict[f\"{run_name}_AhB\"].CoordCenterInertial_0\n",
    "    dy = data_dict[f\"{run_name}_AhA\"].CoordCenterInertial_1 - data_dict[f\"{run_name}_AhB\"].CoordCenterInertial_1\n",
    "    dz = data_dict[f\"{run_name}_AhA\"].CoordCenterInertial_2 - data_dict[f\"{run_name}_AhB\"].CoordCenterInertial_2\n",
    "\n",
    "    dx = np.sqrt(dx**2 + dy**2 + dz**2)\n",
    "\n",
    "    return ct,dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run_name in runs_to_plot.keys():\n",
    "    ct,dx = inertial_dist(run_name,data_dict)\n",
    "    plt.plot(ct,dx,label=run_name)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_dict.keys())\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(data_dict['76_ngd_master_mr1_50_3000_AhA'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all paraview files into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = Path(\"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/near_bhs_100_obs/\")\n",
    "\n",
    "Lev = 3\n",
    "file_pattern =f\"Lev{Lev}_A[A-Z]/Run/GaugeVis.pvd\"\n",
    "file_patternGrid =f\"Lev{Lev}_A[A-Z]/Run/GaugeVisGrid.pvd\"\n",
    "file_patternAll =f\"Lev{Lev}_A[A-Z]/Run/GuageVisAll.pvd\"\n",
    "\n",
    "combine_pvd_files(base_folder,file_pattern)\n",
    "combine_pvd_files(base_folder,file_patternGrid)\n",
    "combine_pvd_files(base_folder,file_patternAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Create GaugeVis\n",
    "command = f\"cd {base_folder} && mkdir ./GaugeVis\"\n",
    "status = subprocess.run(command, capture_output=True, shell=True, text=True)\n",
    "if status.returncode == 0:\n",
    "  print(f\"Succesfully created GaugeVis in {base_folder}\")\n",
    "else:\n",
    "  sys.exit(\n",
    "      f\"GaugeVis creation failed in {base_folder} with error: \\n {status.stderr}\")\n",
    "\n",
    "# Create GaugeVis subfolder\n",
    "vtu_folder_path = base_folder+\"/GaugeVis/GaugeVis\"\n",
    "command = f\"mkdir {vtu_folder_path}\"\n",
    "status = subprocess.run(command, capture_output=True, shell=True, text=True)\n",
    "if status.returncode == 0:\n",
    "  print(f\"Succesfully created {vtu_folder_path}\")\n",
    "else:\n",
    "  sys.exit(\n",
    "      f\"GaugeVis creation failed as {vtu_folder_path} with error: \\n {status.stderr}\")\n",
    "\n",
    "\n",
    "# Copy vtu files\n",
    "GaugeVisFolder=[]\n",
    "\n",
    "for paths in path_collection:\n",
    "  GaugeVisFolder.append(paths[:-4])\n",
    "\n",
    "for paths in GaugeVisFolder:\n",
    "  command = f\"cp {paths}/*.vtu {vtu_folder_path}/\"\n",
    "  status = subprocess.run(command, capture_output=True, shell=True, text=True)\n",
    "  if status.returncode == 0:\n",
    "    print(f\"Succesfully copied vtu files from {paths}\")\n",
    "  else:\n",
    "    sys.exit(\n",
    "        f\"Copying vtu files from {paths} failed with error: \\n {status.stderr}\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiler results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make report (do not run randomly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./report_new_gauge.json\") as report_data:\n",
    "  data = json.load(report_data)\n",
    "\n",
    "os.mkdir(data['report_folder'])\n",
    "\n",
    "subfolders = []\n",
    "for folders in data['runs_to_track']:\n",
    "  subfolders_path = data['report_folder'] + \"/\" + path_to_folder_name(folders) + \"/\"\n",
    "  print(subfolders_path)\n",
    "  os.mkdir(subfolders_path)\n",
    "  subfolders.append(subfolders_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_still_going_on = True\n",
    "while runs_still_going_on:\n",
    "  # time.sleep(data['report_generation_frequency'])\n",
    "\n",
    "  for i,run_folder_path in enumerate(data['runs_to_track']):\n",
    "    # if is_the_current_run_going_on(run_folder_path) or True:\n",
    "    if True:\n",
    "      plots_for_a_folder(data['things_to_plot'],subfolders[i],run_folder_path)\n",
    "    print(run_folder_path)\n",
    "\n",
    "\n",
    "  runs_still_going_on = False\n",
    "  print(\"all done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all columns and data files paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write all the cols in the dat files for reference\n",
    "lev_golb=\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/gauge_driver_kerr_target_50_50_0_16_16_01/Ev/Lev1_AA\"\n",
    "dat_files_glob=lev_golb+\"/Run/**/**.dat\"\n",
    "path_pattern = dat_files_glob\n",
    "\n",
    "path_collection = []\n",
    "for folder_name in glob.iglob(path_pattern, recursive=True):\n",
    "    if os.path.isdir(folder_name) or os.path.isfile(folder_name):\n",
    "        path_collection.append(folder_name)\n",
    "        print(folder_name.split(\"/\")[-1])\n",
    "\n",
    "\n",
    "column_data_for_dat_files = {\n",
    "  'columns_of_dat_files' : [\n",
    "  ] \n",
    "}\n",
    "\n",
    "for file_path in path_collection:\n",
    "  file_name = file_path.split(\"/\")[-1]\n",
    "  columns_list =  list(read_dat_file(file_path).columns)\n",
    "  column_data_for_dat_files['columns_of_dat_files'].append({\n",
    "    'file_name': file_name,\n",
    "    'file_path': file_path,\n",
    "    'columns': columns_list\n",
    "  })\n",
    "\n",
    "\n",
    "with open('./column_data_for_dat_files.json', 'w') as outfile:\n",
    "  json.dump(column_data_for_dat_files, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JoinH5(h5_file_list, output_path, output_file_name):\n",
    "\n",
    "  file_list_to_str = \"\"\n",
    "  for h5file in h5_file_list:\n",
    "    file_list_to_str += h5file + \" \"\n",
    "\n",
    "  command = f\"cd {output_path} && {spec_home}/Support/bin/JoinH5 -o {output_file_name} {file_list_to_str}\"\n",
    "  status = subprocess.run(command, capture_output=True, shell=True, text=True)\n",
    "  if status.returncode == 0:\n",
    "    print(f\"Succesfully ran JoinH5 in {output_path}\")\n",
    "  else:\n",
    "    sys.exit(\n",
    "        f\"JoinH5 failed in {output_path} with error: \\n {status.stderr}\")\n",
    "\n",
    "\n",
    "def ExtractFromH5(h5_file, output_path):\n",
    "\n",
    "  command = f\"cd {output_path} && {spec_home}/Support/bin/ExtractFromH5 {h5_file}\"\n",
    "  status = subprocess.run(command, capture_output=True, shell=True, text=True)\n",
    "  if status.returncode == 0:\n",
    "    print(f\"Succesfully ran ExtractFromH5 in {output_path}\")\n",
    "  else:\n",
    "    sys.exit(\n",
    "        f\"ExtractFromH5 failed in {output_path} with error: \\n {status.stderr}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_base_path= \"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/profiler_results\"\n",
    "\n",
    "\n",
    "base_folder = \"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/49_ngd_weird_gauge_mr1\"\n",
    "file_pattern = base_folder+\"/Ev/Lev1_A?/Run/Profiler.h5\"\n",
    "\n",
    "path_pattern = file_pattern\n",
    "path_collection = []\n",
    "\n",
    "# make a folder in the output directory\n",
    "save_folder = output_base_path+\"/\"+base_folder.split(\"/\")[-1]\n",
    "os.mkdir(save_folder)\n",
    "\n",
    "\n",
    "# Find all the files that match the required pattern of the file\n",
    "for folder_name in glob.iglob(path_pattern, recursive=True):\n",
    "    if os.path.isdir(folder_name) or os.path.isfile(folder_name):\n",
    "        path_collection.append(folder_name)\n",
    "        print(folder_name)\n",
    "\n",
    "JoinH5(path_collection,save_folder,\"Profiler.h5\")\n",
    "ExtractFromH5(\"Profiler.h5\",save_folder)\n",
    "\n",
    "# Save path of all the summary files in extracted data\n",
    "\n",
    "file_pattern = base_folder+\"/Ev/Lev1_A?/Run/Profiler.h5\"\n",
    "\n",
    "path_pattern = file_pattern\n",
    "path_collection = []\n",
    "\n",
    "# Find all the files that match the required pattern of the file\n",
    "for folder_name in glob.iglob(path_pattern, recursive=True):\n",
    "    if os.path.isdir(folder_name) or os.path.isfile(folder_name):\n",
    "        path_collection.append(folder_name)\n",
    "        print(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the Summary files \n",
    "summary_file_pattern = save_folder+\"/**/Summary.txt\"\n",
    "summary_file_collection = []\n",
    "\n",
    "for file_path in glob.iglob(summary_file_pattern, recursive=True):\n",
    "    if os.path.isdir(file_path) or os.path.isfile(file_path):\n",
    "        summary_file_collection.append(file_path)\n",
    "        print(file_path)\n",
    "\n",
    "summary_file_collection.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/profiler_results/49_ngd_weird_gauge_mr1/extracted-Profiler/Step10522.dir/Summary.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AmrTolerances.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lev=3\n",
    "TruncationErrorMax = 0.000216536 * 4**(-Lev)\n",
    "ProjectedConstraintsMax = 0.216536 * 4**(-Lev)\n",
    "TruncationErrorMaxA = TruncationErrorMax*1.e-4\n",
    "TruncationErrorMaxB = TruncationErrorMax*1.e-4\n",
    "\n",
    "AhMaxRes  = TruncationErrorMax\n",
    "AhMinRes  = AhMaxRes / 10.0\n",
    "\n",
    "AhMaxTrunc=TruncationErrorMax\n",
    "AhMinTrunc=AhMaxTrunc / 100.0\n",
    "\n",
    "print(f\"AhMinRes={AhMinRes};\")\n",
    "print(f\"AhMaxRes={AhMaxRes};\")\n",
    "print(f\"AhMinTrunc={AhMinTrunc};\")\n",
    "print(f\"AhMaxTrunc={AhMaxTrunc};\")\n",
    "print(f\"TruncationErrorMax={TruncationErrorMax};\")\n",
    "print(f\"TruncationErrorMaxA={TruncationErrorMaxA};\")\n",
    "print(f\"TruncationErrorMaxB={TruncationErrorMaxB};\")\n",
    "print(f\"ProjectedConstraintsMax={ProjectedConstraintsMax};\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode_tol_val(Lev):\n",
    "  TruncationErrorMax = 0.000216536 * 4**(-Lev)\n",
    "  ode_tol = TruncationErrorMax/2000\n",
    "  return ode_tol\n",
    "\n",
    "for i in [0,1,2,3,4,4.5,5,5.5,6]:\n",
    "  print(i,\"->\",ode_tol_val(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SBATCH --reservation=sxs_standing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class time_step_simulator():\n",
    "  def __init__(self,average=0.05,variation_frac=0.1,running_avg_bound_N=50, step_growth_frac=0.07,chunk_time_diff=1.5, chunk_step_modify=3,chunk_start=1.5) -> None:\n",
    "    self.average = average\n",
    "    self.variation_frac = variation_frac\n",
    "\n",
    "    self.ode_error_estimate = 1e-8\n",
    "    self.ode_error_estimate_variation_fraction = 0.1 #How much can the error change from step to step\n",
    "    self.ode_tolerance = 1e-8\n",
    "    self.ode_order = 4\n",
    "\n",
    "    # Time step  tracking\n",
    "    self.time_till_now = 0\n",
    "    self.time_steps = []\n",
    "    self.time_after_step = []\n",
    "\n",
    "    # Chunking stuff\n",
    "    self.chunk_time_diff = chunk_time_diff\n",
    "    self.chunk_step_modify = chunk_step_modify\n",
    "    self.next_chunk_time = chunk_start\n",
    "    \n",
    "    # Running average stuff\n",
    "    self.running_average_step_size = average\n",
    "    self.step_growth_frac = step_growth_frac\n",
    "    self.running_avg_bound_N = running_avg_bound_N\n",
    "    self.running_average_step_size_arr = []\n",
    "\n",
    "\n",
    "  def take_step(self):\n",
    "    curr_step = self.average + (2*np.random.rand()-1)*self.variation_frac*self.average\n",
    "\n",
    "    # Bound the step size growth\n",
    "    if curr_step > self.running_average_step_size*(1+self.step_growth_frac):\n",
    "      curr_step = self.running_average_step_size*(1+self.step_growth_frac)\n",
    "\n",
    "    steps_left = (self.next_chunk_time-self.time_till_now)/curr_step\n",
    "    if steps_left <=0:\n",
    "      raise ValueError(f\"{steps_left=} is an invalid value.\")\n",
    "\n",
    "    if steps_left < self.chunk_step_modify:\n",
    "      if (steps_left < 1+1e-14):\n",
    "        # Take exact time step to hit the chunk\n",
    "        curr_step = (self.next_chunk_time-self.time_till_now)\n",
    "        # set the next chunk time\n",
    "        self.next_chunk_time = self.next_chunk_time + self.chunk_time_diff\n",
    "      else:\n",
    "        # We need to decrease the time steps to hit the chunk\n",
    "        curr_step = (self.next_chunk_time-self.time_till_now)*1.05/(np.floor(steps_left)+1.0)\n",
    "\n",
    "    avg_frac = (self.running_avg_bound_N-1)/self.running_avg_bound_N\n",
    "    self.running_average_step_size = self.running_average_step_size*avg_frac + (1-avg_frac)*curr_step\n",
    "    self.running_average_step_size_arr.append(self.running_average_step_size)\n",
    "\n",
    "    self.time_till_now = self.time_till_now + curr_step\n",
    "    self.time_after_step.append(self.time_till_now)\n",
    "    self.time_steps.append(curr_step)\n",
    "\n",
    "  def take_steps(self,num_steps=100):\n",
    "    for i in range(num_steps):\n",
    "      self.take_step()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepper = time_step_simulator(variation_frac=0.1,chunk_step_modify=3)\n",
    "stepper.take_steps(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stepper.time_after_step, stepper.time_steps,marker=\"x\")\n",
    "plt.plot(stepper.time_after_step, stepper.running_average_step_size_arr,marker=\"x\")\n",
    "# plt.plot(stepper.time_after_step, stepper.time_steps)\n",
    "# plt.plot(stepper.time_after_step, stepper.running_average_step_size_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract h5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ev_path = Path(\"/groups/sxs/hchaudha/spec_runs/Lev01_test/new_ode_tol/high_accuracy_L35/Ev\")\n",
    "save_run_path = Path(\"/groups/sxs/hchaudha/spec_runs/del.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with save_run_path.open('w') as f:\n",
    "  path_list = list(Ev_path.glob(\"Lev*_A?/Run\"))\n",
    "  path_list.sort()\n",
    "  ringdown_path_list = list(Ev_path.glob(\"Lev*_Ringdown/Lev*_A?/Run\"))\n",
    "  ringdown_path_list.sort()\n",
    "  for i in (path_list+ringdown_path_list):\n",
    "    if not os.path.islink(i.parent): # If Lev5_AA is symlink then do not write\n",
    "      f.write(f\"{i}\\n\")\n",
    "  # for i in Ev_path.glob(\"Lev?_A?/Run/extrac*\"):\n",
    "    # f.write(f\"rm -r {i}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "folder=$1\n",
    "\n",
    "cd $1\n",
    "echo \"Running in the folder: $folder\"\n",
    "/home/hchaudha/spec/Support/bin/ExtractFromH5 -o . -strippath ./*.h5\n",
    "# cat del.txt | xargs -P 8 -I {} ./del.sh {}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.islink(Path(\"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev6_AA\")),os.path.islink(Path(\"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev6_AA/Run\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hist-GrDomain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ResizeTheseSubdomains(input_str):\n",
    "    # Remove the initial part before '=' and trailing semicolon\n",
    "    input_str = input_str.split('=', 1)[1].strip(';')\n",
    "    \n",
    "    # Use regex to find all subdomain entries\n",
    "    pattern = r'(\\w+(?:\\.\\d+)*)\\(Extents=\\((\\d+,\\d+,\\d+)\\)\\)'\n",
    "    matches = re.findall(pattern, input_str)\n",
    "    \n",
    "    # Initialize the dictionary\n",
    "    subdomains_dict = {}\n",
    "\n",
    "    for name, extents in matches:\n",
    "        # Convert the extents string into a tuple of integers\n",
    "        extents = tuple(map(int, extents.split(',')))\n",
    "        \n",
    "        # Add to the dictionary\n",
    "        subdomains_dict[name] = extents\n",
    "    \n",
    "    return subdomains_dict\n",
    "\n",
    "def find_max_extents_for_domain_type(parsed_dict, domain_type):\n",
    "  def max_vals_in_a_tuple(t1, t2):\n",
    "    return tuple(max(a, b) for a, b in zip(t1, t2))\n",
    "  max_extents = (0, 0, 0)\n",
    "  for domain_name in parsed_dict:\n",
    "    if domain_type in domain_name:\n",
    "      max_extents = max_vals_in_a_tuple(max_extents, parsed_dict[domain_name])\n",
    "  return max_extents\n",
    "\n",
    "\n",
    "def AMR_input_content_spheres(sphere_name, MinExtent0, MinL, DoNotChangeBeforeTime,TruncationErrorMax=None):\n",
    "  if TruncationErrorMax is None:\n",
    "    amr_str = f\"\"\"{sphere_name}(MinExtent0={MinExtent0};MinL={MinL};\n",
    "          DoNotChangeExtent0BeforeRadiusPlusTime = {DoNotChangeBeforeTime};\n",
    "          DoNotChangeLBeforeRadiusPlusTime = {DoNotChangeBeforeTime};\n",
    "        ),\n",
    "\"\"\"\n",
    "  else:\n",
    "    amr_str = f\"\"\"{sphere_name}(MinExtent0={MinExtent0};MinL={MinL};\n",
    "          DoNotChangeExtent0BeforeRadiusPlusTime = {DoNotChangeBeforeTime};\n",
    "          DoNotChangeLBeforeRadiusPlusTime = {DoNotChangeBeforeTime};\n",
    "          TruncationErrorMax = {TruncationErrorMax};\n",
    "          Center = 0,0,0;\n",
    "        ),\n",
    "\"\"\"\n",
    "  return amr_str\n",
    "\n",
    "def AMR_input_content_cylinder(cylinder_name, MinExtent0, MinExtent1, MinExtent2, DoNotChangeBeforeTime,TruncationErrorMax=None):\n",
    "  if TruncationErrorMax is None:\n",
    "    amr_str = f\"\"\"{cylinder_name}*(MinExtent0={MinExtent0};MinExtent1={MinExtent1};MinExtent2={MinExtent2};\n",
    "          DoNotChangeExtent0BeforeRadiusPlusTime = {DoNotChangeBeforeTime};\n",
    "          DoNotChangeExtent1BeforeRadiusPlusTime = {DoNotChangeBeforeTime};\n",
    "          DoNotChangeExtent2BeforeRadiusPlusTime = {DoNotChangeBeforeTime};\n",
    "        ),\n",
    "\"\"\"\n",
    "  else:\n",
    "    amr_str = f\"\"\"{cylinder_name}*(MinExtent0={MinExtent0};MinExtent1={MinExtent1};MinExtent2={MinExtent2};\n",
    "          DoNotChangeExtent0BeforeRadiusPlusTime = {DoNotChangeBeforeTime};\n",
    "          DoNotChangeExtent1BeforeRadiusPlusTime = {DoNotChangeBeforeTime};\n",
    "          DoNotChangeExtent2BeforeRadiusPlusTime = {DoNotChangeBeforeTime};\n",
    "          TruncationErrorMax = {TruncationErrorMax};\n",
    "          Center = 0,0,0;\n",
    "        ),\n",
    "\"\"\"\n",
    "  return amr_str\n",
    "\n",
    "def make_odd(n):\n",
    "  if n%2==0:\n",
    "    return n+1\n",
    "  else:\n",
    "    return n\n",
    "\n",
    "def AMR_input_content_spheres_min_max(sphere_name, MinExtent0, MinL, DoNotChangeBeforeTime):\n",
    "  amr_str = f\"\"\"{sphere_name}(MinExtent0={MinExtent0};MinL={MinL};\n",
    "        MaxExtent0={MinExtent0};MaxL={MinL};\n",
    "        SplitAfterMaxExtent0IsReached=False;\n",
    "        SplitAfterMaxLIsReached=False;\n",
    "        DoNotChangeExtent0BeforeRadiusPlusTime = {DoNotChangeBeforeTime};\n",
    "        DoNotChangeLBeforeRadiusPlusTime = {DoNotChangeBeforeTime};\n",
    "      ),\n",
    "\"\"\"\n",
    "  return amr_str\n",
    "\n",
    "def AMR_input_content_cylinder_min_max(cylinder_name, MinExtent0, MinExtent1, MinExtent2, DoNotChangeBeforeTime):\n",
    "  MinExtent0, MinExtent1, MinExtent2 = make_odd(MinExtent0), make_odd(MinExtent1), make_odd(MinExtent2)\n",
    "  amr_str = f\"\"\"{cylinder_name}*(MinExtent0={MinExtent0};MinExtent1={MinExtent1};MinExtent2={MinExtent2};\n",
    "          MaxExtent0={MinExtent0};MaxExtent1={MinExtent1};MaxExtent2={MinExtent2};\n",
    "          SplitAfterMaxExtent0IsReached=False;\n",
    "          SplitAfterMaxExtent1IsReached=False;\n",
    "          SplitAfterMaxExtent2IsReached=False;\n",
    "          DoNotChangeExtent0BeforeRadiusPlusTime = {DoNotChangeBeforeTime};\n",
    "          DoNotChangeExtent1BeforeRadiusPlusTime = {DoNotChangeBeforeTime};\n",
    "          DoNotChangeExtent2BeforeRadiusPlusTime = {DoNotChangeBeforeTime};\n",
    "        ),\n",
    "\"\"\"\n",
    "  return amr_str\n",
    "\n",
    "def adjust_parsed_dict_for_lev(parsed_dict, lev):\n",
    "  adjustment = int(lev - 3)\n",
    "  for domain_name in parsed_dict:\n",
    "    parsed_dict[domain_name] = tuple([int(i+adjustment) for i in parsed_dict[domain_name]])\n",
    "    if \"Sphere\" in domain_name:\n",
    "      extents = parsed_dict[domain_name]\n",
    "      # For spheres M is always 2*L\n",
    "      parsed_dict[domain_name] = (extents[0], extents[1], 2*extents[1])\n",
    "  return parsed_dict\n",
    "\n",
    "input_str_base_set3_lev3 = \"ResizeTheseSubdomains=SphereA0(Extents=(8,18,36)),SphereA1(Extents=(8,19,38)),SphereA2(Extents=(8,20,40)),SphereA3(Extents=(8,21,42)),SphereA4(Extents=(8,22,44)),SphereB0(Extents=(8,18,36)),SphereB1(Extents=(8,19,38)),SphereB2(Extents=(8,20,40)),SphereB3(Extents=(8,21,42)),SphereB4(Extents=(8,22,44)),SphereC0(Extents=(15,15,30)),SphereC1(Extents=(15,15,30)),SphereC2(Extents=(15,15,30)),SphereC3(Extents=(15,15,30)),SphereC4(Extents=(15,15,30)),SphereC5(Extents=(15,15,30)),SphereC6(Extents=(15,15,30)),SphereC7(Extents=(15,15,30)),SphereC8(Extents=(15,15,30)),SphereC9(Extents=(15,15,30)),SphereC10(Extents=(15,15,30)),SphereC11(Extents=(15,16,32)),SphereC12(Extents=(15,16,32)),SphereC13(Extents=(15,16,32)),SphereC14(Extents=(15,16,32)),SphereC15(Extents=(15,16,32)),SphereC16(Extents=(15,15,30)),SphereC17(Extents=(15,15,30)),SphereC18(Extents=(15,15,30)),SphereC19(Extents=(15,15,30)),SphereC20(Extents=(15,15,30)),SphereC21(Extents=(15,15,30)),SphereC22(Extents=(15,15,30)),SphereC23(Extents=(15,16,32)),SphereC24(Extents=(15,16,32)),SphereC25(Extents=(15,16,32)),SphereC26(Extents=(15,16,32)),SphereC27(Extents=(15,16,32)),SphereC28(Extents=(15,16,32)),SphereC29(Extents=(15,16,32)),CylinderEB0.0.0(Extents=(10,23,15)),CylinderEB1.0.0(Extents=(12,23,15)),CylinderCB0.0.0(Extents=(13,23,16)),CylinderCB1.0.0(Extents=(13,23,15)),CylinderEA0.0.0(Extents=(10,23,15)),CylinderEA1.0.0(Extents=(12,23,15)),CylinderCA0.0.0(Extents=(13,23,16)),CylinderCA1.0.0(Extents=(13,23,15)),FilledCylinderEB0(Extents=(10,9,21)),FilledCylinderEB1(Extents=(10,9,21)),FilledCylinderCB0(Extents=(11,8,21)),FilledCylinderCB1(Extents=(12,8,21)),FilledCylinderMB0(Extents=(10,9,19)),FilledCylinderMB1(Extents=(12,9,19)),CylinderSMB0.0(Extents=(10,21,11)),CylinderSMB1.0(Extents=(12,21,12)),FilledCylinderEA0(Extents=(10,9,21)),FilledCylinderEA1(Extents=(10,9,21)),FilledCylinderCA0(Extents=(11,8,21)),FilledCylinderCA1(Extents=(12,8,21)),FilledCylinderMA0(Extents=(10,9,19)),FilledCylinderMA1(Extents=(12,9,19)),CylinderSMA0.0(Extents=(10,21,11)),CylinderSMA1.0(Extents=(12,21,12));\"\n",
    "parsed_dict_set3_lev3 = parse_ResizeTheseSubdomains(input_str_base_set3_lev3)\n",
    "\n",
    "input_str_base_set1_lev3 = \"ResizeTheseSubdomains=SphereA0(Extents=(8,18,36)),SphereA1(Extents=(8,19,38)),SphereA2(Extents=(8,20,40)),SphereA3(Extents=(8,21,42)),SphereA4(Extents=(8,22,44)),SphereB0(Extents=(8,18,36)),SphereB1(Extents=(8,19,38)),SphereB2(Extents=(8,20,40)),SphereB3(Extents=(8,21,42)),SphereB4(Extents=(8,22,44)),SphereC0(Extents=(15,15,30)),SphereC1(Extents=(15,15,30)),SphereC2(Extents=(15,15,30)),SphereC3(Extents=(15,15,30)),SphereC4(Extents=(15,15,30)),SphereC5(Extents=(15,15,30)),SphereC6(Extents=(15,15,30)),SphereC7(Extents=(15,15,30)),SphereC8(Extents=(15,15,30)),SphereC9(Extents=(15,15,30)),SphereC10(Extents=(15,16,32)),SphereC11(Extents=(15,16,32)),SphereC12(Extents=(15,16,32)),SphereC13(Extents=(15,16,32)),SphereC14(Extents=(15,16,32)),SphereC15(Extents=(15,15,30)),SphereC16(Extents=(15,15,30)),SphereC17(Extents=(15,15,30)),SphereC18(Extents=(15,15,30)),SphereC19(Extents=(15,15,30)),SphereC20(Extents=(15,15,30)),SphereC21(Extents=(15,15,30)),SphereC22(Extents=(15,16,32)),SphereC23(Extents=(15,16,32)),SphereC24(Extents=(15,16,32)),SphereC25(Extents=(15,16,32)),SphereC26(Extents=(15,16,32)),SphereC27(Extents=(15,16,32)),SphereC28(Extents=(15,16,32)),SphereC29(Extents=(15,16,32)),CylinderEB0.0.0(Extents=(10,23,15)),CylinderEB1.0.0(Extents=(12,23,15)),CylinderCB0.0.0(Extents=(13,23,16)),CylinderCB1.0.0(Extents=(13,23,15)),CylinderEA0.0.0(Extents=(10,23,15)),CylinderEA1.0.0(Extents=(12,23,15)),CylinderCA0.0.0(Extents=(13,23,16)),CylinderCA1.0.0(Extents=(13,23,15)),FilledCylinderEB0(Extents=(10,9,21)),FilledCylinderEB1(Extents=(10,9,21)),FilledCylinderCB0(Extents=(11,8,21)),FilledCylinderCB1(Extents=(12,8,21)),FilledCylinderMB0(Extents=(10,9,19)),FilledCylinderMB1(Extents=(12,9,19)),CylinderSMB0.0(Extents=(10,21,11)),CylinderSMB1.0(Extents=(12,21,12)),FilledCylinderEA0(Extents=(10,9,21)),FilledCylinderEA1(Extents=(10,9,21)),FilledCylinderCA0(Extents=(11,8,21)),FilledCylinderCA1(Extents=(12,8,21)),FilledCylinderMA0(Extents=(10,9,19)),FilledCylinderMA1(Extents=(12,9,19)),CylinderSMA0.0(Extents=(10,21,11)),CylinderSMA1.0(Extents=(12,21,12));\"\n",
    "parsed_dict_set1_lev3 = parse_ResizeTheseSubdomains(input_str_base_set1_lev3)\n",
    "\n",
    "input_str_base_lev6 = \"ResizeTheseSubdomains=SphereA0(Extents=(11,23,46)),SphereA1(Extents=(11,24,48)),SphereA2(Extents=(11,25,50)),SphereA3(Extents=(11,26,52)),SphereA4(Extents=(13,27,54)),SphereB0(Extents=(11,22,44)),SphereB1(Extents=(11,23,46)),SphereB2(Extents=(11,24,48)),SphereB3(Extents=(11,25,50)),SphereB4(Extents=(13,26,52)),SphereC0(Extents=(18,18,36)),SphereC1(Extents=(18,17,34)),SphereC2(Extents=(18,18,36)),SphereC3(Extents=(18,18,36)),SphereC4(Extents=(18,18,36)),SphereC5(Extents=(18,19,38)),SphereC6(Extents=(18,19,38)),SphereC7(Extents=(18,19,38)),SphereC8(Extents=(18,19,38)),SphereC9(Extents=(18,19,38)),SphereC10(Extents=(18,19,38)),SphereC11(Extents=(18,19,38)),SphereC12(Extents=(18,19,38)),SphereC13(Extents=(18,19,38)),SphereC14(Extents=(18,19,38)),SphereC15(Extents=(18,19,38)),SphereC16(Extents=(18,20,40)),SphereC17(Extents=(18,20,40)),SphereC18(Extents=(18,20,40)),SphereC19(Extents=(18,20,40)),SphereC20(Extents=(18,20,40)),SphereC21(Extents=(18,20,40)),SphereC22(Extents=(18,21,42)),SphereC23(Extents=(18,21,42)),SphereC24(Extents=(18,21,42)),SphereC25(Extents=(18,22,44)),SphereC26(Extents=(18,22,44)),SphereC27(Extents=(18,22,44)),SphereC28(Extents=(18,22,44)),SphereC29(Extents=(18,21,42)),CylinderEB0.0.0(Extents=(14,29,19)),CylinderEB1.0.0(Extents=(16,29,21)),CylinderCB0.0.0(Extents=(17,29,20)),CylinderCB1.0.0(Extents=(17,27,19)),CylinderEA0.0.0(Extents=(14,29,19)),CylinderEA1.0.0(Extents=(16,29,21)),CylinderCA0.0.0(Extents=(17,29,20)),CylinderCA1.0.0(Extents=(17,27,19)),FilledCylinderEB0(Extents=(13,11,29)),FilledCylinderEB1(Extents=(14,11,29)),FilledCylinderCB0(Extents=(15,11,29)),FilledCylinderCB1(Extents=(16,10,27)),FilledCylinderMB0(Extents=(14,11,27)),FilledCylinderMB1(Extents=(16,11,23)),CylinderSMB0.0(Extents=(15,27,15)),CylinderSMB1.0(Extents=(16,25,15)),FilledCylinderEA0(Extents=(13,11,29)),FilledCylinderEA1(Extents=(14,11,29)),FilledCylinderCA0(Extents=(15,11,29)),FilledCylinderCA1(Extents=(16,10,27)),FilledCylinderMA0(Extents=(14,11,27)),FilledCylinderMA1(Extents=(16,11,23)),CylinderSMA0.0(Extents=(15,27,15)),CylinderSMA1.0(Extents=(16,25,15));\"\n",
    "parsed_dict_lev6 = parse_ResizeTheseSubdomains(input_str_base_lev6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"ResizeTheseSubdomains=SphereA0(Extents=(8,18,36)),SphereA1(Extents=(8,19,38)),SphereA2(Extents=(8,20,40)),SphereA3(Extents=(8,21,42)),SphereA4(Extents=(8,22,44)),SphereB0(Extents=(8,18,36)),SphereB1(Extents=(8,19,38)),SphereB2(Extents=(8,20,40)),SphereB3(Extents=(8,21,42)),SphereB4(Extents=(8,22,44)),SphereC0(Extents=(15,15,30)),SphereC1(Extents=(15,15,30)),SphereC2(Extents=(15,15,30)),SphereC3(Extents=(15,15,30)),SphereC4(Extents=(15,15,30)),SphereC5(Extents=(15,15,30)),SphereC6(Extents=(15,15,30)),SphereC7(Extents=(15,15,30)),SphereC8(Extents=(15,15,30)),SphereC9(Extents=(15,15,30)),SphereC10(Extents=(15,16,32)),SphereC11(Extents=(15,16,32)),SphereC12(Extents=(15,16,32)),SphereC13(Extents=(15,16,32)),SphereC14(Extents=(15,16,32)),SphereC15(Extents=(15,15,30)),SphereC16(Extents=(15,15,30)),SphereC17(Extents=(15,15,30)),SphereC18(Extents=(15,15,30)),SphereC19(Extents=(15,15,30)),SphereC20(Extents=(15,15,30)),SphereC21(Extents=(15,15,30)),SphereC22(Extents=(15,16,32)),SphereC23(Extents=(15,16,32)),SphereC24(Extents=(15,16,32)),SphereC25(Extents=(15,16,32)),SphereC26(Extents=(15,16,32)),SphereC27(Extents=(15,16,32)),SphereC28(Extents=(15,16,32)),SphereC29(Extents=(15,16,32)),CylinderEB0.0.0(Extents=(10,23,15)),CylinderEB1.0.0(Extents=(12,23,15)),CylinderCB0.0.0(Extents=(13,23,16)),CylinderCB1.0.0(Extents=(13,23,15)),CylinderEA0.0.0(Extents=(10,23,15)),CylinderEA1.0.0(Extents=(12,23,15)),CylinderCA0.0.0(Extents=(13,23,16)),CylinderCA1.0.0(Extents=(13,23,15)),FilledCylinderEB0(Extents=(10,9,21)),FilledCylinderEB1(Extents=(10,9,21)),FilledCylinderCB0(Extents=(11,8,21)),FilledCylinderCB1(Extents=(12,8,21)),FilledCylinderMB0(Extents=(10,9,19)),FilledCylinderMB1(Extents=(12,9,19)),CylinderSMB0.0(Extents=(10,21,11)),CylinderSMB1.0(Extents=(12,21,12)),FilledCylinderEA0(Extents=(10,9,21)),FilledCylinderEA1(Extents=(10,9,21)),FilledCylinderCA0(Extents=(11,8,21)),FilledCylinderCA1(Extents=(12,8,21)),FilledCylinderMA0(Extents=(10,9,19)),FilledCylinderMA1(Extents=(12,9,19)),CylinderSMA0.0(Extents=(10,21,11)),CylinderSMA1.0(Extents=(12,21,12));\"\n",
    "\n",
    "DoNotChangeBeforeTime = 40000\n",
    "Lev = 3\n",
    "\n",
    "parsed_dict = parse_ResizeTheseSubdomains(input_str)\n",
    "adjust_parsed_dict_for_lev(parsed_dict, Lev) \n",
    "\n",
    "max_extent_dict = {\n",
    "  # Domains that are not spheres will all have the maximum extents\n",
    "  \"FilledCylinderCA\" : find_max_extents_for_domain_type(parsed_dict,\"FilledCylinderCA\"),\n",
    "  \"CylinderCA\" : find_max_extents_for_domain_type(parsed_dict,\"CylinderCA\"),\n",
    "  \"FilledCylinderEA\" : find_max_extents_for_domain_type(parsed_dict,\"FilledCylinderEA\"),\n",
    "  \"CylinderEA\" : find_max_extents_for_domain_type(parsed_dict,\"CylinderEA\"),\n",
    "  \"CylinderSMA\" : find_max_extents_for_domain_type(parsed_dict,\"CylinderSMA\"),\n",
    "  \"FilledCylinderMA\" : find_max_extents_for_domain_type(parsed_dict,\"FilledCylinderMA\"),\n",
    "  \"FilledCylinderMB\" : find_max_extents_for_domain_type(parsed_dict,\"FilledCylinderMB\"),\n",
    "  \"CylinderSMB\" : find_max_extents_for_domain_type(parsed_dict,\"CylinderSMB\"),\n",
    "  \"CylinderEB\" : find_max_extents_for_domain_type(parsed_dict,\"CylinderEB\"),\n",
    "  \"FilledCylinderEB\" : find_max_extents_for_domain_type(parsed_dict,\"FilledCylinderEB\"),\n",
    "  \"CylinderCB\" : find_max_extents_for_domain_type(parsed_dict,\"CylinderCB\"),\n",
    "  \"FilledCylinderCB\" : find_max_extents_for_domain_type(parsed_dict,\"FilledCylinderCB\"),\n",
    "}\n",
    "\n",
    "for key,extent in parsed_dict.items():\n",
    "  if 'Sphere' in key:\n",
    "    max_extent_dict[key] = extent\n",
    "\n",
    "for key in max_extent_dict:\n",
    "  if \"Sphere\" in key:\n",
    "    if \"SphereC\" in key:\n",
    "      if \"SphereC0\" in key:\n",
    "        extents = max_extent_dict[\"SphereC0\"]\n",
    "        print(AMR_input_content_spheres(\"SphereC*\", extents[0], extents[1], DoNotChangeBeforeTime=DoNotChangeBeforeTime))\n",
    "      continue\n",
    "    print(AMR_input_content_spheres(key, max_extent_dict[key][0], max_extent_dict[key][1], DoNotChangeBeforeTime=DoNotChangeBeforeTime))\n",
    "  elif \"Cylinder\" in key:\n",
    "    print(AMR_input_content_cylinder(key, max_extent_dict[key][0], max_extent_dict[key][1], max_extent_dict[key][2], DoNotChangeBeforeTime=DoNotChangeBeforeTime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"ResizeTheseSubdomains=SphereA0(Extents=(8,18,36)),SphereA1(Extents=(8,19,38)),SphereA2(Extents=(8,20,40)),SphereA3(Extents=(8,21,42)),SphereA4(Extents=(8,22,44)),SphereB0(Extents=(8,18,36)),SphereB1(Extents=(8,19,38)),SphereB2(Extents=(8,20,40)),SphereB3(Extents=(8,21,42)),SphereB4(Extents=(8,22,44)),SphereC0(Extents=(15,15,30)),SphereC1(Extents=(15,15,30)),SphereC2(Extents=(15,15,30)),SphereC3(Extents=(15,15,30)),SphereC4(Extents=(15,15,30)),SphereC5(Extents=(15,15,30)),SphereC6(Extents=(15,15,30)),SphereC7(Extents=(15,15,30)),SphereC8(Extents=(15,15,30)),SphereC9(Extents=(15,15,30)),SphereC10(Extents=(15,16,32)),SphereC11(Extents=(15,16,32)),SphereC12(Extents=(15,16,32)),SphereC13(Extents=(15,16,32)),SphereC14(Extents=(15,16,32)),SphereC15(Extents=(15,15,30)),SphereC16(Extents=(15,15,30)),SphereC17(Extents=(15,15,30)),SphereC18(Extents=(15,15,30)),SphereC19(Extents=(15,15,30)),SphereC20(Extents=(15,15,30)),SphereC21(Extents=(15,15,30)),SphereC22(Extents=(15,16,32)),SphereC23(Extents=(15,16,32)),SphereC24(Extents=(15,16,32)),SphereC25(Extents=(15,16,32)),SphereC26(Extents=(15,16,32)),SphereC27(Extents=(15,16,32)),SphereC28(Extents=(15,16,32)),SphereC29(Extents=(15,16,32)),CylinderEB0.0.0(Extents=(10,23,15)),CylinderEB1.0.0(Extents=(12,23,15)),CylinderCB0.0.0(Extents=(13,23,16)),CylinderCB1.0.0(Extents=(13,23,15)),CylinderEA0.0.0(Extents=(10,23,15)),CylinderEA1.0.0(Extents=(12,23,15)),CylinderCA0.0.0(Extents=(13,23,16)),CylinderCA1.0.0(Extents=(13,23,15)),FilledCylinderEB0(Extents=(10,9,21)),FilledCylinderEB1(Extents=(10,9,21)),FilledCylinderCB0(Extents=(11,8,21)),FilledCylinderCB1(Extents=(12,8,21)),FilledCylinderMB0(Extents=(10,9,19)),FilledCylinderMB1(Extents=(12,9,19)),CylinderSMB0.0(Extents=(10,21,11)),CylinderSMB1.0(Extents=(12,21,12)),FilledCylinderEA0(Extents=(10,9,21)),FilledCylinderEA1(Extents=(10,9,21)),FilledCylinderCA0(Extents=(11,8,21)),FilledCylinderCA1(Extents=(12,8,21)),FilledCylinderMA0(Extents=(10,9,19)),FilledCylinderMA1(Extents=(12,9,19)),CylinderSMA0.0(Extents=(10,21,11)),CylinderSMA1.0(Extents=(12,21,12));\"\n",
    "\n",
    "DoNotChangeBeforeTime = 40000\n",
    "Lev = 6\n",
    "\n",
    "parsed_dict = parse_ResizeTheseSubdomains(input_str)\n",
    "adjust_parsed_dict_for_lev(parsed_dict, Lev) \n",
    "\n",
    "max_extent_dict = {\n",
    "  # Domains that are not spheres will all have the maximum extents\n",
    "  \"FilledCylinderCA\" : find_max_extents_for_domain_type(parsed_dict,\"FilledCylinderCA\"),\n",
    "  \"CylinderCA\" : find_max_extents_for_domain_type(parsed_dict,\"CylinderCA\"),\n",
    "  \"FilledCylinderEA\" : find_max_extents_for_domain_type(parsed_dict,\"FilledCylinderEA\"),\n",
    "  \"CylinderEA\" : find_max_extents_for_domain_type(parsed_dict,\"CylinderEA\"),\n",
    "  \"CylinderSMA\" : find_max_extents_for_domain_type(parsed_dict,\"CylinderSMA\"),\n",
    "  \"FilledCylinderMA\" : find_max_extents_for_domain_type(parsed_dict,\"FilledCylinderMA\"),\n",
    "  \"FilledCylinderMB\" : find_max_extents_for_domain_type(parsed_dict,\"FilledCylinderMB\"),\n",
    "  \"CylinderSMB\" : find_max_extents_for_domain_type(parsed_dict,\"CylinderSMB\"),\n",
    "  \"CylinderEB\" : find_max_extents_for_domain_type(parsed_dict,\"CylinderEB\"),\n",
    "  \"FilledCylinderEB\" : find_max_extents_for_domain_type(parsed_dict,\"FilledCylinderEB\"),\n",
    "  \"CylinderCB\" : find_max_extents_for_domain_type(parsed_dict,\"CylinderCB\"),\n",
    "  \"FilledCylinderCB\" : find_max_extents_for_domain_type(parsed_dict,\"FilledCylinderCB\"),\n",
    "}\n",
    "\n",
    "for key,extent in parsed_dict.items():\n",
    "  if 'Sphere' in key:\n",
    "    max_extent_dict[key] = extent\n",
    "\n",
    "for key in max_extent_dict:\n",
    "  if \"Sphere\" in key:\n",
    "    if \"SphereC\" in key:\n",
    "      if \"SphereC0\" in key:\n",
    "        extents = max_extent_dict[\"SphereC0\"]\n",
    "        print(AMR_input_content_spheres_min_max(\"SphereC*\", extents[0], extents[1], DoNotChangeBeforeTime=DoNotChangeBeforeTime))\n",
    "      continue\n",
    "    print(AMR_input_content_spheres_min_max(key, max_extent_dict[key][0], max_extent_dict[key][1], DoNotChangeBeforeTime=DoNotChangeBeforeTime))\n",
    "  elif \"Cylinder\" in key:\n",
    "    print(AMR_input_content_cylinder_min_max(key, max_extent_dict[key][0], max_extent_dict[key][1], max_extent_dict[key][2], DoNotChangeBeforeTime=DoNotChangeBeforeTime))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate DomainFile from the hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = \"MinimumGridSpacing.dat\"\n",
    "data_file_path = \"Hist-GrDomain.txt\"\n",
    "\n",
    "# data_file_path = \"Hist-Domain.txt\"\n",
    "\n",
    "\n",
    "column_names, runs_data_dict_domain = load_data_from_levs(runs_to_plot,data_file_path)\n",
    "# print(column_names)\n",
    "print(runs_data_dict_domain.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"6_set1_L6s33\"\n",
    "t_target = 4000\n",
    "\n",
    "data = runs_data_dict_domain[key]\n",
    "idx = np.argmin(np.abs(data['t(M)']-t_target))\n",
    "sd_extent_dict = data.iloc[idx].to_dict()\n",
    "\n",
    "\n",
    "extents = {\n",
    "    \"SphereA\": {\n",
    "        \"RadialExtent\": 0,\n",
    "        \"L\": 0,\n",
    "        \"M\": 0,\n",
    "    },\n",
    "    \"SphereB\": {\n",
    "        \"RadialExtent\": 0,\n",
    "        \"L\": 0,\n",
    "        \"M\": 0,\n",
    "    },\n",
    "    \"SphereC\": {\n",
    "        \"RadialExtent\": 0,\n",
    "        \"L\": 0,\n",
    "        \"M\": 0,\n",
    "    },\n",
    "    \"CylinderCA\": {\n",
    "        \"RadialExtents\": 0,\n",
    "        \"PhiExtent\": 0,\n",
    "        \"ThetaExtents\": 0,\n",
    "    },\n",
    "    \"CylinderCB\": {\n",
    "        \"RadialExtents\": 0,\n",
    "        \"PhiExtent\": 0,\n",
    "        \"ThetaExtents\": 0,\n",
    "    },\n",
    "    \"CylinderEA\": {\n",
    "        \"RadialExtents\": 0,\n",
    "        \"PhiExtent\": 0,\n",
    "        \"ThetaExtents\": 0,\n",
    "    },\n",
    "    \"CylinderEB\": {\n",
    "        \"RadialExtents\": 0,\n",
    "        \"PhiExtent\": 0,\n",
    "        \"ThetaExtents\": 0,\n",
    "    },\n",
    "    \"CylinderSMA\": {\n",
    "        \"RadialExtents\": 0,\n",
    "        \"PhiExtent\": 0,\n",
    "        \"ThetaExtents\": 0,\n",
    "    },\n",
    "    \"CylinderSMB\": {\n",
    "        \"RadialExtents\": 0,\n",
    "        \"PhiExtent\": 0,\n",
    "        \"ThetaExtents\": 0,\n",
    "    },\n",
    "    \"FilledCylinderCA\": {\n",
    "        \"RadialExtents\": 0,\n",
    "        \"PhiExtent\": 0,\n",
    "        \"ThetaExtents\": 0,\n",
    "    },\n",
    "    \"FilledCylinderCB\": {\n",
    "        \"RadialExtents\": 0,\n",
    "        \"PhiExtent\": 0,\n",
    "        \"ThetaExtents\": 0,\n",
    "    },\n",
    "    \"FilledCylinderEA\": {\n",
    "        \"RadialExtents\": 0,\n",
    "        \"PhiExtent\": 0,\n",
    "        \"ThetaExtents\": 0,\n",
    "    },\n",
    "    \"FilledCylinderEB\": {\n",
    "        \"RadialExtents\": 0,\n",
    "        \"PhiExtent\": 0,\n",
    "        \"ThetaExtents\": 0,\n",
    "    },\n",
    "    \"FilledCylinderMA\": {\n",
    "        \"RadialExtents\": 0,\n",
    "        \"PhiExtent\": 0,\n",
    "        \"ThetaExtents\": 0,\n",
    "    },\n",
    "    \"FilledCylinderMB\": {\n",
    "        \"RadialExtents\": 0,\n",
    "        \"PhiExtent\": 0,\n",
    "        \"ThetaExtents\": 0,\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sd_extent_dict_keys = list(sd_extent_dict.keys())[4:]\n",
    "\n",
    "\n",
    "def reduction_function(a,b):\n",
    "    return max(a,b)\n",
    "    # return min(a,b)\n",
    "\n",
    "\n",
    "for sd_group in extents:\n",
    "    keys = list(extents[sd_group].keys())\n",
    "\n",
    "    for sd_extent in sd_extent_dict_keys:\n",
    "        if sd_extent.startswith(sd_group):\n",
    "            if \"_R\" in sd_extent:\n",
    "                extents[sd_group][keys[0]] = reduction_function(extents[sd_group][keys[0]], sd_extent_dict[sd_extent])\n",
    "            elif \"_L\" in sd_extent:\n",
    "                extents[sd_group][keys[1]] = reduction_function(extents[sd_group][keys[1]], sd_extent_dict[sd_extent])\n",
    "            elif \"_M\" in sd_extent:\n",
    "                extents[sd_group][keys[2]] = reduction_function(extents[sd_group][keys[2]], sd_extent_dict[sd_extent])\n",
    "\n",
    "for i in extents:\n",
    "    for j in extents[i]:\n",
    "        extents[i][j] = int(extents[i][j])\n",
    "\n",
    "extents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO! Subtract 1 from L and maybe phi??\n",
    "\n",
    "for i in extents:\n",
    "    for j in extents[i]:\n",
    "        if \"Sphere\" in i:\n",
    "            if \"M\" == j:\n",
    "                continue\n",
    "        print(f\"{i}::{j} = {extents[i][j]};\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = runs_data_dict['high_accuracy_L5']\n",
    "\n",
    "# filtered_cols = [i.split(\" \")[-1] for i in data.columns if 'SphereA' in i]\n",
    "filtered_cols = ['t(M)']+[i for i in data.columns if 'e' in i]\n",
    "filtered_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data creation (same as in the previous example)\n",
    "np.random.seed(0)\n",
    "data = {\n",
    "    'time': np.arange(0, 10, 1),\n",
    "    'velocity': np.random.uniform(50, 100, 10),\n",
    "    'acceleration': np.random.uniform(-5, 5, 10),\n",
    "    'fuel_left': np.random.uniform(100, 200, 10),\n",
    "    'burning_rate': np.random.uniform(0, 10, 10)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Normalize the data to [0, 1]\n",
    "normalized_df = (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "# Remove time for visualization\n",
    "visual_data = normalized_df.drop('time', axis=1)\n",
    "\n",
    "# Plot using imshow\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(visual_data, aspect='auto', cmap='viridis', origin='lower')\n",
    "\n",
    "# Set x-ticks and labels\n",
    "xtick_positions = np.arange(len(visual_data.columns))\n",
    "plt.xticks(ticks=xtick_positions, labels=visual_data.columns, rotation=90)\n",
    "# plt.xticks(ticks=np.arange(len(visual_data.columns)), labels=visual_data.columns, rotation=90)\n",
    "plt.yticks(ticks=np.arange(len(visual_data)), labels=normalized_df['time'].astype(int))\n",
    "\n",
    "plt.colorbar(label='Normalized Value')\n",
    "# plt.xlabel('Features')\n",
    "plt.ylabel('Time')\n",
    "plt.title('Feature Intensity Over Time')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(100)/100\n",
    "y1 = np.sin(x)\n",
    "y2 = np.cos(x)\n",
    "\n",
    "styles =  plt.style.available\n",
    "\n",
    "for style in styles:\n",
    "    print(style)\n",
    "    plt.style.use(style)\n",
    "    plt.plot(x,y1,label=\"y1asfasd\")\n",
    "    plt.plot(x,y2,label=\"y3asfasd\")\n",
    "    plt.title(\"asdf\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/make_report_del/{style}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 4\n",
    "print(np.convolve(np.arange(1,10),np.ones(w),'valid')/w)\n",
    "print(np.arange(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(array,avg_len):\n",
    "    return np.convolve(array,np.ones(avg_len))/avg_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss(x,cen,scale):\n",
    "  return np.exp(-(x-cen)**2/scale**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = 18\n",
    "q = 1\n",
    "d1 = 1/(1+q)*d0\n",
    "d2 = q/(1+q)*d0\n",
    "cen1 = d1\n",
    "cen2 = -d2\n",
    "scale1 = d2\n",
    "scale2 = d1\n",
    "\n",
    "tol1 = 3.38337e-10\n",
    "tol2 = 3.38337e-10\n",
    "tol_base = 3.38337e-6\n",
    "\n",
    "d = 100\n",
    "x_min,x_max = -d/2,d/2\n",
    "X = np.linspace(x_min,x_max,1000)\n",
    "\n",
    "fac_base = np.ones_like(X)\n",
    "fac1 = gauss(X,cen1,scale1)\n",
    "fac2 = gauss(X,cen2,scale2)\n",
    "\n",
    "fac_total = fac_base+fac1+fac2\n",
    "val = fac_base*np.log10(tol_base) + fac1*np.log10(tol1)+fac2*np.log10(tol2)\n",
    "val = 10**(val/fac_total)\n",
    "print(val.min(),val.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = 18\n",
    "q = 1\n",
    "cen1 = 0\n",
    "cen2 = 0\n",
    "scale1 = 100\n",
    "scale2 = 1000\n",
    "\n",
    "tol1 = 3.38337e-10\n",
    "tol2 = 3.38337e-9\n",
    "tol_base = 3.38337e-6\n",
    "\n",
    "d = 1000\n",
    "x_min,x_max = -d/2,d/2\n",
    "X = np.linspace(x_min,x_max,1000)\n",
    "\n",
    "fac_base = np.ones_like(X)\n",
    "fac1 = gauss(X,cen1,scale1)\n",
    "fac2 = gauss(X,cen2,scale2)\n",
    "\n",
    "fac_total = fac_base+fac1+fac2\n",
    "val = fac_base*np.log10(tol_base) + fac1*np.log10(tol1)+fac2*np.log10(tol2)\n",
    "val = 10**(val/fac_total)\n",
    "print(val.min(),val.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.00000216536 * 4**(-3)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.semilogy\n",
    "f(X,val,label='tot')\n",
    "# f(X,fac1*tol1,label='tol1')\n",
    "# f(X,fac2*tol2,label='tol2')\n",
    "# f(X,fac_base*tol_base,label='tol_base')\n",
    "hor_lines = [val.min(),val[int(len(val)/2)],val.max()]\n",
    "plt.hlines(y=hor_lines, xmin=-d/2, xmax=d/2, colors=['r', 'g', 'b'], linestyles='--', linewidth=2)\n",
    "for y_value in hor_lines:\n",
    "    plt.text(0, y_value*1.1, f'{y_value:.3e}', va='center', ha='left')\n",
    "\n",
    "\n",
    "plt.ylabel('AMR tolerance')\n",
    "plt.xlabel('x_axis')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(X,gauss(X,0,50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_files_to_dataframe(file_path):\n",
    "  # Function to parse a single line and return a dictionary of values\n",
    "  def parse_line(line):\n",
    "      data = {}\n",
    "      # Find all variable=value pairs\n",
    "      pairs = re.findall(r'([^;=\\s]+)=\\s*([^;]+)', line)\n",
    "      for var, val in pairs:\n",
    "          # Hist-GrDomain.txt should be parsed a little differently\n",
    "          if 'ResizeTheseSubdomains' in var:\n",
    "              items = val.split('),')\n",
    "              items[-1] = items[-1][:-1]\n",
    "              for item in items:\n",
    "                name,_,vals = item.split(\"(\")\n",
    "                r,l,m=vals[:-1].split(',')\n",
    "                data[f\"{name}_R\"] = int(r)\n",
    "                data[f\"{name}_L\"] = int(l)\n",
    "                data[f\"{name}_M\"] = int(m)\n",
    "          else:\n",
    "              data[var] = float(val) if re.match(r'^[\\d.e+-]+$', val) else val\n",
    "      return data\n",
    "  \n",
    "  with open(file_path, 'r') as file:\n",
    "    # Parse the lines\n",
    "    data = []\n",
    "    for line in file.readlines():\n",
    "        data.append(parse_line(line.strip()))\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "  return df\n",
    "\n",
    "hist_files_to_dataframe('/groups/sxs/hchaudha/spec_runs/2_SpKS_q1_sA_0_0_9_sB_0_0_9_d15/Ev/Lev3_AB/Run/Hist-GrDomain.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_files_to_dataframe('/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_etl_tol_10/Lev3_AD/Run_old/Hist-GrDomain.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_files_to_dataframe('/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_etl_tol_10/Lev3_AD/Run_old/Hist-GrDomain.txt')\n",
    "# hist_files_to_dataframe('/groups/sxs/hchaudha/spec_runs/2_SpKS_q1_sA_0_0_9_sB_0_0_9_d15/Ev/Lev3_AB/Run/Hist-FuncLambdaFactorB.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Function to parse a single line and return a dictionary of values\n",
    "def parse_line(line):\n",
    "    data = {}\n",
    "    # Find all variable=value pairs\n",
    "    pairs = re.findall(r'([^;=]+)=\\s*([\\d.e+-]+)', line)\n",
    "    for var, val in pairs:\n",
    "        data[var] = float(val) if re.match(r'[\\d.e+-]+', val) else val\n",
    "    return data\n",
    "\n",
    "# Read the file\n",
    "with open('/groups/sxs/hchaudha/spec_runs/2_SpKS_q1_sA_0_0_9_sB_0_0_9_d15/Ev/Lev3_AB/Run/Hist-FuncLambdaFactorB.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Parse the lines\n",
    "data = []\n",
    "for line in lines:\n",
    "    data.append(parse_line(line.strip()))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# The input string\n",
    "data = \"\"\"SphereA0(Extents=(12,22,44)),SphereA1(Extents=(10,23,46)),SphereA2(Extents=(10,24,48)),SphereA3(Extents=(10,25,50)),SphereA4(Extents=(13,26,52)),SphereB0(Extents=(12,22,44)),SphereB1(Extents=(10,23,46)),SphereB2(Extents=(10,24,48)),SphereB3(Extents=(10,25,50)),SphereB4(Extents=(13,26,52)),SphereC0(Extents=(15,15,30)),SphereC1(Extents=(15,15,30)),SphereC2(Extents=(15,14,28)),SphereC3(Extents=(15,15,30)),SphereC4(Extents=(15,15,30)),SphereC5(Extents=(15,15,30)),SphereC6(Extents=(15,15,30)),SphereC7(Extents=(15,16,32)),SphereC8(Extents=(15,15,30)),SphereC9(Extents=(15,16,32)),SphereC10(Extents=(15,16,32)),SphereC11(Extents=(15,15,30)),SphereC12(Extents=(15,15,30)),SphereC13(Extents=(15,15,30)),SphereC14(Extents=(15,15,30)),SphereC15(Extents=(15,15,30)),SphereC16(Extents=(16,15,30)),SphereC17(Extents=(16,16,32)),SphereC18(Extents=(16,16,32)),SphereC19(Extents=(16,16,32)),SphereC20(Extents=(15,16,32)),SphereC21(Extents=(15,16,32)),SphereC22(Extents=(15,16,32)),SphereC23(Extents=(15,16,32)),SphereC24(Extents=(15,15,30)),SphereC25(Extents=(15,16,32)),SphereC26(Extents=(15,16,32)),SphereC27(Extents=(15,16,32)),SphereC28(Extents=(15,16,32)),SphereC29(Extents=(15,16,32)),CylinderEB0.0.0(Extents=(13,31,19)),CylinderEB1.0.0(Extents=(17,25,18)),CylinderCB0.0.0(Extents=(17,23,17)),CylinderCB1.0.0(Extents=(14,21,15)),CylinderEA0.0.0(Extents=(13,31,19)),CylinderEA1.0.0(Extents=(14,25,18)),CylinderCA0.0.0(Extents=(17,23,18)),CylinderCA1.0.0(Extents=(14,21,15)),FilledCylinderEB0(Extents=(12,11,25)),FilledCylinderEB1(Extents=(12,10,25)),FilledCylinderCB0(Extents=(12,9,21)),FilledCylinderCB1(Extents=(12,8,19)),FilledCylinderMB0(Extents=(14,11,25)),FilledCylinderMB1(Extents=(16,10,21)),CylinderSMB0.0(Extents=(14,27,15)),CylinderSMB1.0(Extents=(18,25,15)),FilledCylinderEA0(Extents=(12,11,25)),FilledCylinderEA1(Extents=(12,10,25)),FilledCylinderCA0(Extents=(12,9,21)),FilledCylinderCA1(Extents=(12,8,19)),FilledCylinderMA0(Extents=(14,11,25)),FilledCylinderMA1(Extents=(14,10,21)),CylinderSMA0.0(Extents=(14,27,15)),CylinderSMA1.0(Extents=(15,25,15))\"\"\"\n",
    "\n",
    "# Split the string into individual items\n",
    "items = data.split('),')\n",
    "\n",
    "# # Function to parse each item\n",
    "# def parse_item(item):\n",
    "#     name, values = re.match(r'(\\w+)\\(Extents=\\((.*?)\\)', item).groups()\n",
    "#     r, l, m = map(int, values.split(','))\n",
    "#     return {'Name': name, 'R': r, 'L': l, 'M': m}\n",
    "\n",
    "# # Parse all items\n",
    "# parsed_data = [parse_item(item) for item in items]\n",
    "\n",
    "# # Create DataFrame\n",
    "# df = pd.DataFrame(parsed_data)\n",
    "\n",
    "# # Set 'Name' as index\n",
    "# df.set_index('Name', inplace=True)\n",
    "\n",
    "# # Create the specific variables for SphereA2\n",
    "# SphereA2_R = df.loc['SphereA2', 'R']\n",
    "# SphereA2_L = df.loc['SphereA2', 'L']\n",
    "# SphereA2_M = df.loc['SphereA2', 'M']\n",
    "\n",
    "# print(df)\n",
    "# print(f\"\\nSphereA2_R = {SphereA2_R}\")\n",
    "# print(f\"SphereA2_L = {SphereA2_L}\")\n",
    "# print(f\"SphereA2_M = {SphereA2_M}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = data.split('),')\n",
    "name,_,vals = items[0].split(\"(\")\n",
    "r,l,m=vals[:-1].split(',')\n",
    "{\n",
    "  name+\"_R\":r,\n",
    "  name+\"_L\":l,\n",
    "  name+\"_M\":m\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[0].split(\"(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals[:-1].split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = Path(\"/groups/sxs/hchaudha/spec_runs\")\n",
    "del_path = Path(\"/groups/sxs/hchaudha/spec_runs/del.sh\")\n",
    "\n",
    "with del_path.open('w') as f:\n",
    "  for i in folder_path.iterdir():\n",
    "    if i.is_dir():\n",
    "      if \"ID\" in str(i):\n",
    "        continue\n",
    "      if \"del\" in str(i):\n",
    "        continue\n",
    "      f.writelines(str(i)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dir(folder_path:Path,del_path_opened):\n",
    "  for i in folder_path.iterdir():\n",
    "    if i.is_dir():\n",
    "      print(i)\n",
    "      if \"ID\" in str(i):\n",
    "        continue\n",
    "      if \"del\" in str(i):\n",
    "        continue\n",
    "      if \"Lev\" in str(i) and i.is_symlink():\n",
    "        continue\n",
    "      if \"Run\" == i.name:\n",
    "        del_path_opened.writelines(str(i)+\"\\n\")\n",
    "        return\n",
    "      write_dir(i,del_path_opened)\n",
    "\n",
    "folder_path = Path(\"/groups/sxs/hchaudha/spec_runs\")\n",
    "# folder_path = Path(\"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eteq_tol_8\")\n",
    "del_path = Path(\"/groups/sxs/hchaudha/spec_runs/del.sh\")\n",
    "with del_path.open('w') as f:\n",
    "  write_dir(folder_path, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path.name == \"asd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.linspace(0.02,0.07,100)\n",
    "a = np.array([0.03,0.05])\n",
    "b = np.exp(-a*10)\n",
    "plt.plot(a,b)\n",
    "plt.plot(a,b**3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(-0.03*10)**3,np.exp(-0.05*10)**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file = Path(\"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/cce_bondi/path_dict.pkl\")\n",
    "with file.open('rb') as f:\n",
    "  data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = data['Lev3_R0050'].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Lev3_R0050']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "  amr_tol = 0.000216536 * 4**(-i)\n",
    "  ode_tol = amr_tol/2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path(\"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/cce_bondi/Lev3_R0200/Lev3_R0200.h5\")\n",
    "file_path = Path(\"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/cce_bondi/Lev4_R0200/Lev4_R0200.h5\")\n",
    "file_path = Path(\"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/cce_bondi/Lev5_R0050/Lev5_R0050.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(file_path,'r') as f:\n",
    "    names = []\n",
    "    f.visit(names.append)\n",
    "    f.visit(print)\n",
    "    data = np.array(f['Beta.dat'])\n",
    "    # print(np.array(data),np.array(data).shape)\n",
    "\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 6.76675e-09\n",
    "for i in range(7):\n",
    "  print(i-2,a*4**(4-i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sympy as sp\n",
    "\n",
    "# Define symbol\n",
    "x = sp.symbols('x')\n",
    "\n",
    "# Taylor polynomial generator\n",
    "def taylor_poly(func, terms):\n",
    "    return sp.series(func, x, 0, terms).removeO()\n",
    "\n",
    "# Functions to test\n",
    "funcs = {'sin': sp.sin(x), 'cos': sp.cos(x), 'tan': sp.tan(x)}\n",
    "\n",
    "# Test parameters\n",
    "a_vals = [1/4, 1/8, 1/16, 1/32]\n",
    "terms_vals = [6, 10 ,16]\n",
    "\n",
    "# Compute max errors\n",
    "results = []\n",
    "for name, f in funcs.items():\n",
    "    for terms in terms_vals:\n",
    "        # Generate Taylor polynomial and lambdify\n",
    "        poly_expr = taylor_poly(f, terms)\n",
    "        poly_func = sp.lambdify(x, poly_expr, 'numpy')\n",
    "        for a in a_vals:\n",
    "            xs = np.linspace(-a, a, 10001)\n",
    "            true_vals = getattr(np, name)(xs)\n",
    "            approx_vals = poly_func(xs)\n",
    "            max_err = np.max(np.abs(true_vals - approx_vals))\n",
    "            results.append({\n",
    "                'Function': name,\n",
    "                'Interval a': a,\n",
    "                'Terms': terms,\n",
    "                'Max Error': max_err\n",
    "            })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Pivot tables for each function\n",
    "pivot_tables = {}\n",
    "for func in df['Function'].unique():\n",
    "    sub = df[df['Function'] == func]\n",
    "    pivot = sub.pivot(index='Interval a', columns='Terms', values='Max Error')\n",
    "    pivot_tables[func] = pivot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_tables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sxs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
