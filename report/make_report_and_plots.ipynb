{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import random\n",
    "import re\n",
    "import h5py\n",
    "import copy\n",
    "import sys\n",
    "from numba import njit\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict\n",
    "import os\n",
    "import glob\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams[\"figure.figsize\"] = (12,10)\n",
    "import json\n",
    "import time\n",
    "import matplotlib\n",
    "from pathlib import Path\n",
    "from scipy.interpolate import CubicSpline\n",
    "spec_home=\"/home/himanshu/spec/my_spec\"\n",
    "matplotlib.matplotlib_fname()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Various functions to read across levs\n",
    "### Also functions to make reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to read h5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_profiler(file_name):\n",
    "  with h5py.File(file_name,'r') as f:\n",
    "    steps = set()\n",
    "    procs = set()\n",
    "    names = []\n",
    "    f.visit(names.append)\n",
    "    for name in names:\n",
    "      step = name.split('.')[0][4:]\n",
    "      steps.add(step)\n",
    "      if 'Proc' in name:\n",
    "        procs.add(name.split('/')[-1][4:-4])\n",
    "\n",
    "    dict_list = []\n",
    "    for step in steps:\n",
    "      for proc in procs:\n",
    "        data = f[f'Step{step}.dir/Proc{proc}.txt'][0].decode()\n",
    "\n",
    "        lines = data.split(\"\\n\")\n",
    "        time = float((lines[0].split(\"=\")[-1])[:-1])\n",
    "\n",
    "        curr_dict = {\n",
    "            \"t(M)\": time,\n",
    "            \"step\": step,\n",
    "            \"proc\": proc\n",
    "        }\n",
    "        # Find where the columns end\n",
    "        a = lines[4]\n",
    "        event_end = a.find(\"Event\")+5\n",
    "        cum_end = a.find(\"cum(%)\")+6\n",
    "        exc_end = a.find(\"exc(%)\")+6\n",
    "        inc_end = a.find(\"inc(%)\")+6\n",
    "\n",
    "        for line in lines[6:-2]:\n",
    "          Event = line[:event_end].strip()\n",
    "          cum = float(line[event_end:cum_end].strip())\n",
    "          exc = float(line[cum_end:exc_end].strip())\n",
    "          inc = float(line[exc_end:inc_end].strip())\n",
    "          N = int(line[inc_end:].strip())\n",
    "          # print(a)\n",
    "          # a = line.split(\"  \")\n",
    "          # Event,cum,exc,inc,N = [i.strip() for i in a if i!= '']\n",
    "          curr_dict[f'{Event}_cum'] = cum\n",
    "          curr_dict[f'{Event}_exc'] = exc\n",
    "          curr_dict[f'{Event}_inc'] = inc\n",
    "          curr_dict[f'{Event}_N'] = N\n",
    "\n",
    "        dict_list.append(curr_dict)\n",
    "  return pd.DataFrame(dict_list)\n",
    "\n",
    "def read_profiler_multiindex(folder_path:Path):\n",
    "  dir_paths,dat_paths = list_all_dir_and_dat_files(folder_path)\n",
    "  steps = set()\n",
    "  # Get step names\n",
    "  for dir in dir_paths:\n",
    "    step = dir.name.split('.')[0][4:]\n",
    "    steps.add(step)\n",
    "\n",
    "  procs = set()\n",
    "  # Get the proc names\n",
    "  for txt in dir_paths[0].iterdir():\n",
    "    if \".txt\" in txt.name and \"Summary\" not in txt.name:\n",
    "      procs.add(txt.name[4:-4])\n",
    "\n",
    "  dict_list = []\n",
    "  col_names = set()\n",
    "  row_names = []\n",
    "  for step in steps:\n",
    "    for proc in procs:\n",
    "      txt_file_path = folder_path/f'Step{step}.dir/Proc{proc}.txt'\n",
    "\n",
    "      with txt_file_path.open(\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "      time = float((lines[0].split(\"=\")[-1])[:-2])\n",
    "\n",
    "      curr_dict = {\n",
    "          \"time\": time,\n",
    "          \"step\": step,\n",
    "          \"proc\": proc\n",
    "      }\n",
    "\n",
    "      # Find where the columns end\n",
    "      a = lines[4]\n",
    "      event_end = a.find(\"Event\")+5\n",
    "      cum_end = a.find(\"cum(%)\")+6\n",
    "      exc_end = a.find(\"exc(%)\")+6\n",
    "      inc_end = a.find(\"inc(%)\")+6\n",
    "\n",
    "      row_names.append((str(proc),str(time)))\n",
    "\n",
    "      for line in lines[6:-2]:\n",
    "        Event = line[:event_end].strip()\n",
    "        cum = float(line[event_end:cum_end].strip())\n",
    "        exc = float(line[cum_end:exc_end].strip())\n",
    "        inc = float(line[exc_end:inc_end].strip())\n",
    "        N = int(line[inc_end:].strip())\n",
    "        # print(a)\n",
    "        # a = line.split(\"  \")\n",
    "        # Event,cum,exc,inc,N = [i.strip() for i in a if i!= '']\n",
    "        col_names.add(Event)\n",
    "        curr_dict[(\"cum\",Event)] = cum\n",
    "        curr_dict[(\"exc\",Event)] = exc\n",
    "        curr_dict[(\"inc\",Event)] = inc\n",
    "        curr_dict[(\"N\",Event)] = N\n",
    "\n",
    "      dict_list.append(curr_dict)\n",
    "\n",
    "  # Multi index rows\n",
    "  index = pd.MultiIndex.from_tuples(row_names, names=[\"proc\",\"t(M)\"])\n",
    "  df = pd.DataFrame(dict_list,index=index)\n",
    "  \n",
    "  # Multi index cols\n",
    "  multi_index_columns = [(k if isinstance(k, tuple) else (k, '')) for k in df.columns]\n",
    "  df.columns = pd.MultiIndex.from_tuples(multi_index_columns)\n",
    "  df.columns.names = ['metric', 'process']\n",
    "\n",
    "  # data.xs('24', level=\"proc\")['N']\n",
    "  # data.xs('0.511442', level=\"t(M)\")['cum']\n",
    "  # data.xs(('0','0.511442'),level=('proc','t(M)'))\n",
    "  # data.xs('cum',level='metric',axis=1) = data['cum']\n",
    "  # data.xs('MPI::MPreduceAdd(MV<double>)',level='process',axis=1)\n",
    "  # data[data['time']<50]\n",
    "  # data[data['time']<50]['cum'].xs('0',level='proc')['MPI::MPreduceAdd(MV<double>)']\n",
    "  return df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to read dat and hist files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dat_file(file_name):\n",
    "  cols_names = []\n",
    "  # Read column names\n",
    "  with open(file_name,'r') as f:\n",
    "      lines = f.readlines()\n",
    "      for line in lines:\n",
    "        if \"#\" not in line:\n",
    "          # From now onwards it will be all data\n",
    "          break\n",
    "        elif \"=\" in line:\n",
    "          cols_names.append(line.split('=')[-1][1:-1].strip())\n",
    "        else:\n",
    "          continue\n",
    "\n",
    "  return pd.read_csv(file_name,sep=\"\\s+\",comment=\"#\",names=cols_names)\n",
    "\n",
    "def hist_files_to_dataframe(file_path):\n",
    "  # Function to parse a single line and return a dictionary of values\n",
    "  def parse_line(line):\n",
    "      data = {}\n",
    "      # Find all variable=value pairs\n",
    "      pairs = re.findall(r'([^;=\\s]+)=\\s*([^;]+)', line)\n",
    "      for var, val in pairs:\n",
    "          # Hist-GrDomain.txt should be parsed a little differently\n",
    "          if 'ResizeTheseSubdomains' in var:\n",
    "              items = val.split('),')\n",
    "              items[-1] = items[-1][:-1]\n",
    "              for item in items:\n",
    "                name,_,vals = item.split(\"(\")\n",
    "                r,l,m=vals[:-1].split(',')\n",
    "                data[f\"{name}_R\"] = int(r)\n",
    "                data[f\"{name}_L\"] = int(l)\n",
    "                data[f\"{name}_M\"] = int(m)\n",
    "          else:\n",
    "              data[var] = float(val) if re.match(r'^[\\d.e+-]+$', val) else val\n",
    "      return data\n",
    "  \n",
    "  with open(file_path, 'r') as file:\n",
    "    # Parse the lines\n",
    "    data = []\n",
    "    for line in file.readlines():\n",
    "        data.append(parse_line(line.strip()))\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "  return df\n",
    "\n",
    "# Files like AhACoefs.dat have unequal number of columns\n",
    "def read_dat_file_uneq_cols(file_name):\n",
    "  cols_names = []\n",
    "\n",
    "  temp_file = \"./temp.csv\"\n",
    "  col_length = 0\n",
    "  with open(file_name,'r') as f:\n",
    "    with open(temp_file,'w') as w:\n",
    "      lines = f.readlines()\n",
    "      for line in lines:\n",
    "        if(line[0] != '#'): # This is data\n",
    "          w.writelines(\" \".join(line.split()[:col_length])+\"\\n\")\n",
    "        if(line[0:3] == '# [' or line[0:4] == '#  ['): # Some dat files have comments on the top\n",
    "          cols_names.append(line.split('=')[-1][1:-1].strip())\n",
    "          col_length = col_length+1\n",
    "\n",
    "\n",
    "  return pd.read_csv(temp_file,delim_whitespace=True,names=cols_names)\n",
    "\n",
    "def read_dat_file_across_AA(file_pattern):\n",
    "\n",
    "  path_pattern = file_pattern\n",
    "  path_collection = []\n",
    "\n",
    "\n",
    "  for folder_name in glob.iglob(path_pattern, recursive=True):\n",
    "      if os.path.isdir(folder_name) or os.path.isfile(folder_name):\n",
    "          path_collection.append(folder_name)\n",
    "          print(folder_name)\n",
    "\n",
    "\n",
    "  read_data_collection = []\n",
    "  for path in path_collection:\n",
    "    # AhACoefs.dat has uneq cols\n",
    "    if \"Coefs.dat\" in path:\n",
    "        read_data_collection.append(read_dat_file_uneq_cols(path))\n",
    "    elif \"Hist-\" in path:\n",
    "        read_data_collection.append(hist_files_to_dataframe(path))\n",
    "    elif \"Profiler\" in path:\n",
    "        read_data_collection.append(read_profiler(path))\n",
    "    else:\n",
    "        read_data_collection.append(read_dat_file(path))\n",
    "\n",
    "  data = pd.concat(read_data_collection)\n",
    "  rename_dict = {\n",
    "     't':'t(M)',\n",
    "     'time':'t(M)',\n",
    "     'Time':'t(M)',\n",
    "     'time after step':'t(M)',\n",
    "  }\n",
    "  data.rename(columns=rename_dict, inplace=True)\n",
    "  # print(data.columns)\n",
    "  return data\n",
    "\n",
    "def read_AH_files(Ev_path):\n",
    "  fileA = Ev_path + \"Run/ApparentHorizons/AhA.dat\"\n",
    "  fileB = Ev_path + \"Run/ApparentHorizons/AhB.dat\"\n",
    "\n",
    "  dataA = read_dat_file_across_AA(fileA)\n",
    "  dataB = read_dat_file_across_AA(fileB)\n",
    "\n",
    "  return dataA,dataB  \n",
    "\n",
    "  \n",
    "# Combines all the pvd files into a single file and save it in the base folder\n",
    "def combine_pvd_files(base_folder:Path, file_pattern:str, output_path=None):\n",
    "  pvd_start =\"\"\"<?xml version=\"1.0\"?>\\n<VTKFile type=\"Collection\" version=\"0.1\" byte_order=\"LittleEndian\">\\n  <Collection>\\n\"\"\"\n",
    "  pvd_end =\"  </Collection>\\n</VTKFile>\"\n",
    "\n",
    "  vis_folder_name = file_pattern.split(\"/\")[-1][:-4]\n",
    "  Lev = file_pattern[0:4]\n",
    "\n",
    "  if output_path is None:\n",
    "    output_path = f\"{base_folder}/{vis_folder_name}_{Lev}.pvd\"\n",
    "\n",
    "  pvd_files = list(base_folder.glob(file_pattern))\n",
    "  pvd_folders = list(base_folder.glob(file_pattern[:-4]))\n",
    "\n",
    "\n",
    "  with open(output_path,'w') as write_file:\n",
    "    write_file.writelines(pvd_start)\n",
    "    for files in pvd_files:\n",
    "      print(files)\n",
    "      with files.open(\"r\") as f:\n",
    "        for line in f.readlines():\n",
    "          line = line.replace(vis_folder_name,str(files)[:-4])\n",
    "          if \"DataSet\" in line:\n",
    "            write_file.writelines(line)\n",
    "    write_file.writelines(pvd_end)\n",
    "  \n",
    "  print(output_path)\n",
    "\n",
    "def moving_average(array,avg_len):\n",
    "    return np.convolve(array,np.ones(avg_len))/avg_len\n",
    "    \n",
    "def moving_average_valid(array,avg_len):\n",
    "    return np.convolve(array,np.ones(avg_len),'valid')/avg_len\n",
    "\n",
    "\n",
    "def path_to_folder_name(folder_name):\n",
    "  return folder_name.replace(\"/\",\"_\")\n",
    "\n",
    "# Give a dict of {\"run_name\" = runs_path} and data_file_path to get {\"run_name\" = dat_file_data}\n",
    "def load_data_from_levs(runs_path, data_file_path):\n",
    "  data_dict = {}\n",
    "  column_list = \"\"\n",
    "  for run_name in runs_path.keys():\n",
    "    data_dict[run_name] = read_dat_file_across_AA(runs_path[run_name]+data_file_path)\n",
    "    column_list = data_dict[run_name].columns\n",
    "  return column_list, data_dict\n",
    "\n",
    "def add_diff_columns(runs_data_dict, x_axis, y_axis, diff_base):\n",
    "  if diff_base not in runs_data_dict.keys():\n",
    "    raise Exception(f\"{diff_base} not in {runs_data_dict.keys()}\")\n",
    "  \n",
    "  interpolated_data = CubicSpline(runs_data_dict[diff_base][x_axis],runs_data_dict[diff_base][y_axis],extrapolate=False)\n",
    "\n",
    "  for key in runs_data_dict:\n",
    "    if key == diff_base:\n",
    "      continue\n",
    "    df = runs_data_dict[key]\n",
    "    df['diff_'+y_axis] = df[y_axis] - interpolated_data(df[x_axis])\n",
    "\n",
    "def plot_graph_for_runs(runs_data_dict, x_axis, y_axis, minT, maxT, legend_dict=None, save_path=None, moving_avg_len=0, plot_fun = lambda x,y,label : plt.plot(x,y,label=label),sort_by=None, diff_base=None):\n",
    "  sort_run_data_dict(runs_data_dict,sort_by=sort_by)\n",
    "  current_runs_data_dict_keys = list(runs_data_dict.keys())\n",
    "\n",
    "  if diff_base is not None:\n",
    "    add_diff_columns(runs_data_dict, x_axis, y_axis, diff_base)\n",
    "    current_runs_data_dict_keys = []\n",
    "    for key in runs_data_dict:\n",
    "      if key == diff_base:\n",
    "        continue\n",
    "      else:\n",
    "        current_runs_data_dict_keys.append(key)\n",
    "    y_axis = \"diff_\"+y_axis\n",
    " \n",
    "  # Find the indices corresponding to maxT and minT\n",
    "  minT_indx_list={}\n",
    "  maxT_indx_list={}\n",
    "\n",
    "  if legend_dict is None:\n",
    "    legend_dict = {}\n",
    "    for run_name in current_runs_data_dict_keys:\n",
    "      legend_dict[run_name] = None\n",
    "  else:\n",
    "    for run_name in current_runs_data_dict_keys:\n",
    "      if run_name not in legend_dict:\n",
    "        raise ValueError(f\"{run_name} not in {legend_dict=}\")\n",
    "\n",
    "  \n",
    "  for run_name in current_runs_data_dict_keys:\n",
    "    minT_indx_list[run_name] = len(runs_data_dict[run_name][x_axis][runs_data_dict[run_name][x_axis] < minT])\n",
    "    maxT_indx_list[run_name] = len(runs_data_dict[run_name][x_axis][runs_data_dict[run_name][x_axis] < maxT])\n",
    "\n",
    "  if moving_avg_len == 0:\n",
    "\n",
    "    for run_name in current_runs_data_dict_keys:\n",
    "      x_data = runs_data_dict[run_name][x_axis][minT_indx_list[run_name]:maxT_indx_list[run_name]]\n",
    "      y_data = runs_data_dict[run_name][y_axis][minT_indx_list[run_name]:maxT_indx_list[run_name]]\n",
    "      \n",
    "\n",
    "    #   print(f\"{len(x_data)=},{len(y_data)=},{len(np.argsort(x_data))=},{type(x_data)=}\")\n",
    "\n",
    "    #   sorted_indices = x_data.argsort()\n",
    "    #   x_data = x_data.iloc[sorted_indices]\n",
    "    #   y_data = y_data.iloc[sorted_indices]\n",
    "      legend = legend_dict[run_name]\n",
    "      if legend is None:\n",
    "        legend = run_name\n",
    "      plot_fun(x_data, y_data,legend)\n",
    "\n",
    "    plt.xlabel(x_axis)\n",
    "    plt.ylabel(y_axis)\n",
    "    title = \"\\\"\" +  y_axis+\"\\\" vs \\\"\"+x_axis+\"\\\"\"\n",
    "    if diff_base is not None:\n",
    "      title = title + f\" diff_base={diff_base}\"\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "  else:\n",
    "    for run_name in current_runs_data_dict_keys:\n",
    "      x_data = np.array(runs_data_dict[run_name][x_axis][minT_indx_list[run_name] + moving_avg_len-1:maxT_indx_list[run_name]])\n",
    "      y_data = np.array(moving_average_valid(runs_data_dict[run_name][y_axis][minT_indx_list[run_name]:maxT_indx_list[run_name]], moving_avg_len))\n",
    "\n",
    "    #   sorted_indices = np.argsort(x_data)\n",
    "    #   x_data = x_data[sorted_indices]\n",
    "    #   y_data = y_data[sorted_indices]\n",
    "      legend = legend_dict[run_name]\n",
    "      if legend is None:\n",
    "        legend = run_name\n",
    "      plot_fun(x_data, y_data,legend)\n",
    "\n",
    "    plt.xlabel(x_axis)\n",
    "    plt.ylabel(y_axis)\n",
    "    title = \"\\\"\" + y_axis+ \"\\\" vs \\\"\" + x_axis + \"\\\"  \" + f\"avg_window_len={moving_avg_len}\"\n",
    "    if diff_base is not None:\n",
    "      title = title + f\" diff_base={diff_base}\"\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "  \n",
    "  if save_path is not None:\n",
    "    fig_x_label = x_axis.replace(\"/\",\"_\").replace(\".\",\"_\")\n",
    "    fig_y_label = y_axis.replace(\"/\",\"_\").replace(\".\",\"_\")\n",
    "    save_file_name = f\"{fig_y_label}_vs_{fig_x_label}_minT={minT}_maxT={maxT}\".replace(\".\",\"_\")\n",
    "    if moving_avg_len > 0:\n",
    "      save_file_name = save_file_name + f\"_moving_avg_len={moving_avg_len}\"\n",
    "    if diff_base is not None:\n",
    "      save_file_name = save_file_name + f\"_diff_base={diff_base}\"\n",
    "\n",
    "    for run_name in current_runs_data_dict_keys:\n",
    "      save_file_name = save_file_name + \"__\" + run_name.replace(\"/\",\"_\").replace(\".\",\"_\")\n",
    "\n",
    "    if len(save_file_name) >= 251: # <save_file_name>.png >=255\n",
    "      save_file_name = save_file_name[:245]+str(random.randint(10000,99999))\n",
    "      print(f\"The filename was too long!! New filename is {save_file_name}\")\n",
    "\n",
    "    plt.savefig(save_path+save_file_name)\n",
    "\n",
    "\n",
    "def find_file(pattern):\n",
    "  return glob.glob(pattern, recursive=True)[0]\n",
    "\n",
    "def plots_for_a_folder(things_to_plot,plot_folder_path,data_folder_path):\n",
    "  for plot_info in things_to_plot:\n",
    "    file_name = plot_info['file_name']\n",
    "    y_arr = plot_info['columns'][1:]\n",
    "    x_arr = [plot_info['columns'][0]]*len(y_arr)\n",
    "\n",
    "    data = read_dat_file_across_AA(data_folder_path+\"/**/\"+file_name)\n",
    "    plot_and_save(data,x_arr,y_arr,plot_folder_path,file_name)\n",
    "\n",
    "def is_the_current_run_going_on(run_folder):\n",
    "  if len(find_file(run_folder+\"/**/\"+\"TerminationReason.txt\")) > 0:\n",
    "    return False\n",
    "  else:\n",
    "    return True\n",
    "\n",
    "def plot_min_grid_spacing(runs_data_dict):\n",
    "    '''\n",
    "    runs_data_dict should have dataframes with MinimumGridSpacing.dat data.\n",
    "    The function will compute the min grid spacing along all domains and plot it.\n",
    "    '''\n",
    "    keys = runs_data_dict.keys()\n",
    "    if len(keys) == 0:\n",
    "        print(\"There are no dataframes in the dict\")\n",
    "\n",
    "    for key in keys:\n",
    "        t_step = runs_data_dict[key]['t']\n",
    "        min_val = runs_data_dict[key].drop(columns=['t']).min(axis='columns')\n",
    "        plt.plot(t_step,min_val,label=key)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"Min Grid Spacing\")\n",
    "    plt.title(\"Min grid spacing in all domains\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_GrAdjustSubChunksToDampingTimes(runs_data_dict):\n",
    "    keys = runs_data_dict.keys()\n",
    "    if len(keys) > 1:\n",
    "        print(\"To plot the Tdamp for various quantities only put one dataframe in the runs_data_dict\")\n",
    "\n",
    "    data:pd.DataFrame = runs_data_dict[list(keys)[0]]\n",
    "    tdamp_keys = []\n",
    "    for key in data.keys():\n",
    "        if 'Tdamp' in key:\n",
    "            tdamp_keys.append(key)\n",
    "\n",
    "    # Get a colormap\n",
    "    cmap = plt.get_cmap('tab10')\n",
    "    colors = cmap(np.linspace(0, 1, len(tdamp_keys)))\n",
    "\n",
    "    t_vals = data['time']\n",
    "    for i, color, key in zip(range(len(tdamp_keys)),colors, tdamp_keys):\n",
    "        if i%2==0:\n",
    "            plt.plot(t_vals,data[key],label=key,color=color)\n",
    "        else:\n",
    "            plt.plot(t_vals,data[key],label=key,color=color,linestyle=\"--\")\n",
    "\n",
    "\n",
    "    min_tdamp = data[tdamp_keys].min(axis='columns')\n",
    "    plt.plot(t_vals,min_tdamp,label=\"min_tdamp\",linewidth=3,linestyle=\"dotted\",color=\"red\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"time\")\n",
    "    plt.title(list(keys)[0])\n",
    "    plt.show()\n",
    "\n",
    "def add_max_and_min_val(runs_data_dict):\n",
    "    # If we load a file with 5 columns with first being time, then find max and min values for all the other columns, at all times and add it to the dataframe.\n",
    "    # Useful when you want to find like Linf across all domains at all times\n",
    "    for run_name in runs_data_dict.keys():\n",
    "        data_frame = runs_data_dict[run_name]\n",
    "        t = data_frame.iloc[:,0]\n",
    "        max_val = np.zeros_like(t)\n",
    "        min_val = np.zeros_like(t)\n",
    "        for i in range(len(t)):\n",
    "            max_val[i] = data_frame.iloc[i,1:].max()\n",
    "            min_val[i] = data_frame.iloc[i,1:].max()\n",
    "\n",
    "        # Add the values to the dataframe\n",
    "        data_frame['max_val'] = max_val\n",
    "        data_frame['min_val'] = min_val\n",
    "\n",
    "def sort_run_data_dict(runs_data_dict:dict,sort_by=None):\n",
    "    for run_name in runs_data_dict.keys():\n",
    "        run_df = runs_data_dict[run_name]\n",
    "        if sort_by is None:\n",
    "            sort_by = run_df.keys()[0]\n",
    "        runs_data_dict[run_name] = run_df.sort_values(by=sort_by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to read horizon files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_Bh_pandas(h5_dir):\n",
    "    # Empty dataframe\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # List of all the vars in the h5 file\n",
    "    var_list = []\n",
    "    h5_dir.visit(var_list.append)\n",
    "    \n",
    "    \n",
    "    for var in var_list:\n",
    "        # This means there is no time column\n",
    "        # print(f\"{var} : {h5_dir[var].shape}\")\n",
    "        if df.shape == (0,0):\n",
    "            # data[:,0] is time and then we have the data\n",
    "            data = h5_dir[var]\n",
    "            \n",
    "            # vars[:-4] to remove the .dat at the end\n",
    "            col_names = make_col_names(var[:-4],data.shape[1]-1)\n",
    "            col_names.append('t')\n",
    "            # Reverse the list so that we get [\"t\",\"var_name\"]\n",
    "            col_names.reverse()            \n",
    "            append_to_df(data[:],col_names,df)\n",
    "            \n",
    "        else:\n",
    "            data = h5_dir[var]\n",
    "            col_names = make_col_names(var[:-4],data.shape[1]-1)         \n",
    "            append_to_df(data[:,1:],col_names,df)\n",
    "            \n",
    "    return df\n",
    "\n",
    "def append_to_df(data,col_names,df):\n",
    "    for i,col_name in enumerate(col_names):\n",
    "        df[col_name] = data[:,i]\n",
    "        \n",
    "def make_col_names(val_name:str,val_size:int):\n",
    "    col_names = []\n",
    "    if val_size == 1:\n",
    "        col_names.append(val_name)\n",
    "    else:\n",
    "        for i in range(val_size):\n",
    "            col_names.append(val_name+f\"_{i}\")\n",
    "    return col_names\n",
    "\n",
    "\n",
    "def horizon_to_pandas(horizon_path:Path):\n",
    "    assert(horizon_path.exists())\n",
    "    df_dict = {}\n",
    "    with h5py.File(horizon_path,'r') as hf:\n",
    "        # Not all horizon files may have AhC\n",
    "        for key in hf.keys():\n",
    "            if key == 'VersionHist.ver':\n",
    "                # Newer runs have this\n",
    "                continue\n",
    "            df_dict[key[:-4]] = make_Bh_pandas(hf[key])\n",
    "\n",
    "    return df_dict\n",
    "\n",
    "def read_horizon_across_Levs(path_list:List[Path]):\n",
    "    df_listAB = []\n",
    "    df_listC = []\n",
    "    final_dict = {}\n",
    "    for path in path_list:\n",
    "        df_lev = horizon_to_pandas(path)\n",
    "        # Either [AhA,AhB] or [AhA,AhB,AhC]\n",
    "        if len(df_lev.keys()) > 1:\n",
    "            df_listAB.append(df_lev)\n",
    "        # Either [AhC] or [AhA,AhB,AhC]\n",
    "        if (len(df_lev.keys()) == 1) or (len(df_lev.keys()) ==3):\n",
    "            df_listC.append(df_lev)\n",
    "    if len(df_listAB)==1:\n",
    "        # There was only one lev\n",
    "        final_dict = df_listAB[0]\n",
    "    else:\n",
    "        final_dict[\"AhA\"] = pd.concat([df[\"AhA\"] for df in df_listAB])\n",
    "        final_dict[\"AhB\"] = pd.concat([df[\"AhB\"] for df in df_listAB])\n",
    "        if len(df_listC) > 0:\n",
    "            final_dict[\"AhC\"] = pd.concat([df[\"AhC\"] for df in df_listC])       \n",
    "    \n",
    "    return final_dict\n",
    "\n",
    "def load_horizon_data_from_levs(base_path:Path, runs_path:Dict[str,Path]):\n",
    "  data_dict = {}\n",
    "  for run_name in runs_path.keys():\n",
    "    path_list = list(base_path.glob(runs_path[run_name]))\n",
    "    print(path_list)\n",
    "    data_dict[run_name] = read_horizon_across_Levs(path_list)\n",
    "  return data_dict\n",
    "\n",
    "def flatten_dict(horizon_data_dict:Dict[str,pd.DataFrame]) -> Dict[str,pd.DataFrame] :\n",
    "  flattened_data = {}\n",
    "  for run_name in horizon_data_dict.keys():\n",
    "      for horizons in horizon_data_dict[run_name]:\n",
    "          flattened_data[run_name+\"_\"+horizons] = horizon_data_dict[run_name][horizons]\n",
    "          # print(run_name+\"_\"+horizons)\n",
    "  return flattened_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot dat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_to_plot = {}\n",
    "# runs_to_plot[\"boost_ID_test_wrong\"] =  \"/groups/sxs/hchaudha/spec_runs/boost_ID_test_wrong/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"boost_ID_test_correct\"] =  \"/groups/sxs/hchaudha/spec_runs/boost_ID_test_correct/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"corrected_coord_spin1\"] =  \"/groups/sxs/hchaudha/spec_runs/corrected_coord_spin1/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"corrected_coord_spin2\"] =  \"/groups/sxs/hchaudha/spec_runs/corrected_coord_spin2/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"2_SpKS_q1_sA_0_0_0_sB_0_0_0_d15\"] =  \"/groups/sxs/hchaudha/spec_runs/2_SpKS_q1_sA_0_0_0_sB_0_0_0_d15/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"2_SpKS_q1_sA_0_0_0_sB_0_0_99_d15\"] =  \"/groups/sxs/hchaudha/spec_runs/2_SpKS_q1_sA_0_0_0_sB_0_0_99_d15/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"2_SpKS_q1_sA_0_0_0_sB_0_0_9_d15\"] =  \"/groups/sxs/hchaudha/spec_runs/2_SpKS_q1_sA_0_0_0_sB_0_0_9_d15/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"2_SpKS_q1_sA_0_0_9_sB_0_0_9_d15\"] =  \"/groups/sxs/hchaudha/spec_runs/2_SpKS_q1_sA_0_0_9_sB_0_0_9_d15/Ev/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"3_DH_q1_ns_d18_L3\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_tol8_eq\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol8_eq/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_tol9\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol9/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"L3_tol10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_tol10_hi\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol10_hi/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_tol11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol11/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"L3_all_100_tol10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/L6_tol10/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"L3_all_1000_tol11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/L6_all_10_tol11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"local_100_tol5_11\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/local_100_tol5_11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"local_10_tol5_10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/local_10_tol5_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_local_10_tol5_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/local_10_tol5_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_local_100_tol5_11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/local_100_tol5_11/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"high_accuracy_L3\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L4\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L5\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev5_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3/Ev/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_tol8_eq_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol8_eq/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_tol9_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol9/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_tol10_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol10/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_tol10_hi_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol10_hi/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_tol11_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol11/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_all_100_tol10_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/L6_tol10/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_all_1000_tol11_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/L6_all_10_tol11/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3/Ev/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L6\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L6/Ev/Lev6_A?/Run/\"\n",
    "# runs_to_plot[\"L6_1.1\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L6_higher_acc/L6_1.1/Lev6_A?/Run/\"\n",
    "# runs_to_plot[\"L6_1.1_dp8_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L6_higher_acc/L6_1.1_dp8_tol_10/Lev6_A?/Run/\"\n",
    "# runs_to_plot[\"L6_1.1_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L6_higher_acc/L6_1.1_tol_10/Lev6_A?/Run/\"\n",
    "# runs_to_plot[\"L6_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L6_higher_acc/L6_tol_10/Lev6_A?/Run/\"\n",
    "# runs_to_plot[\"all_10\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"near_bhs_10\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/near_bhs_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"near_bhs_100\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/near_bhs_100/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"same_obs\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/same_obs/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_10_obs\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_10_obs/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_obs\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_obs/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_10_obs_tol_10\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_10_obs_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_obs_tol_10\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_obs_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_1.1_b0\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_1.1_b0/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_1.1_b0_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_1.1_b0_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_1.1\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_1.1/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_1.1_dp8_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_1.1_dp8_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_1.1_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_1.1_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_2\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_2/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_2\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_2/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_3\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_3/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dp8\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dp8/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dp8_tol10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dp8_tol10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dp8_tol11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dp8_tol11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dt\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dt_0.02\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.02/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dt_0.03\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.03/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dt_0.04\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.04/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dt_0.025\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.025/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dt_0.021\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.021/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dt_0.022\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.022/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dt_0.023\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.023/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dt_0.024\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.024/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dt_0.0225\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.0225/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dt005\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt0.005/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_tol_11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_tol_11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_tol_9\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_tol_9/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_tol_1.128e-11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_tol_1.128e-11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_tol_1.692e-11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_tol_1.692e-11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_tol_3.383e-11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_tol_3.383e-11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"near_bhs_10_obs\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/near_bhs_10_obs/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"near_bhs_100_obs\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/near_bhs_100_obs/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L6_AA\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L6_AA/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"4_SphKS_q1_15_SSphKS_ID\"] =  \"/groups/sxs/hchaudha/spec_runs/4_SphKS_q1_15_SSphKS_ID/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"4_SphKS_q1_15_SKS_ID\"] =  \"/groups/sxs/hchaudha/spec_runs/4_SphKS_q1_15_SKS_ID/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"5_gd_SphKS_gauge_ID\"] =  \"/groups/sxs/hchaudha/spec_runs/5_gd_SphKS_gauge_ID/Ev/Lev2_A[A-S]/Run/\"\n",
    "# runs_to_plot[\"5_ngd_SphKS_ID\"] =  \"/groups/sxs/hchaudha/spec_runs/5_ngd_SphKS_ID/Ev/Lev2_A?/Run/\"\n",
    "# runs_to_plot[\"5_ngd_KS_ID\"] =  \"/groups/sxs/hchaudha/spec_runs/5_ngd_KS_ID/Ev/Lev2_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"all_100_t2690_eteq_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eteq_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eteq_tol_11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eteq_tol_11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eteq_tol_12\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eteq_tol_12/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eteq_tol_8\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eteq_tol_8/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eteq_tol_9\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eteq_tol_9/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eteq_tol_eq\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eteq_tol_eq/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eth_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eth_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eth_tol_11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eth_tol_11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eth_tol_12\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eth_tol_12/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eth_tol_8\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eth_tol_8/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eth_tol_9\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eth_tol_9/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eth_tol_eq\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eth_tol_eq/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_etl_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_etl_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_etl_tol_11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_etl_tol_11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_etl_tol_8\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_etl_tol_8/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_etl_tol_9\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_etl_tol_9/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_etl_tol_eq\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_etl_tol_eq/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"t6115_tol11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol9\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol9/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol8\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol8/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol7\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol7/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol11_AMR\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol11_AMR/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol10_AMR\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol10_AMR/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol9_AMR\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol9_AMR/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol8_AMR\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol8_AMR/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol7_AMR\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol7_AMR/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol8_linf\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol8_linf/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.02\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.02/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.03\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.03/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.041\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.041/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.042\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.042/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.043\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.043/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.044\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.044/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.045\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.045/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.046\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.046/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.047\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.047/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.048\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.048/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.049\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.049/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.050\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.050/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.052\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.052/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.054\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.054/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.056\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.056/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_2.368e-07\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_2.368e-07/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_1.692e-07\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_1.692e-07/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_1.015e-07\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_1.015e-07/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_6.767e-08\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_6.767e-08/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_5.075e-08\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_5.075e-08/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_3.383e-08\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_3.383e-08/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_2.256e-08\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_2.256e-08/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_1.692e-08\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_1.692e-08/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_1.128e-08\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_1.128e-08/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_6.767e-09\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_6.767e-09/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_4.833e-09\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_4.833e-09/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_3.383e-09\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_3.383e-09/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_1.692e-09\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_1.692e-09/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_1.128e-09\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_1.128e-09/Lev3_A?/Run/\"\n",
    "\n",
    "\n",
    "# runs_to_plot[\"all_100_t2710_0.021_0.021\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_0.21/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_0.022\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_0.22/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_0.0225\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_0.225/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_0.023\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_0.23/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_0.024\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_0.24/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_max_tol8\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_max_tol8/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_max_tol9\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_max_tol9/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_max_tol10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_max_tol10/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_max_tol10.5\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_max_tol10.5/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_max_tol11\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_max_tol11/Lev3_AE/Run/\"\n",
    "\n",
    "# runs_to_plot[\"eq_t4000_tol10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/eq_t4000_tol10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"eq_t4000_tol5_10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/eq_t4000_tol5_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"eq_t4000_tol5_11\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/eq_t4000_tol5_11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"eq_t4000_tol9\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/eq_t4000_tol9/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t4000_tol10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/t4000_tol10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t4000_tol5_10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/t4000_tol5_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t4000_tol5_11\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/t4000_tol5_11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t4000_tol8\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/t4000_tol8/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t4000_tol9\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/t4000_tol9/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"Lev3_AA_tol10_all_10\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol10_all_10/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol11_all_10\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol11_all_10/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol12_all_10\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol12_all_10/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol5_10_all_10\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol5_10_all_10/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol5_11_all_10\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol5_11_all_10/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol10\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol10/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol11\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol11/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol12\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol12/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol5_10\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol5_10/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol5_11\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol5_11/Lev3_A?_/Run/\"\n",
    "\n",
    "runs_to_plot[\"3_100\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/changing_spectral_grid/3_100/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"3_10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/changing_spectral_grid/3_10/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"3_1\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/changing_spectral_grid/3_1/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"2_100\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/changing_spectral_grid/2_100/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"2_10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/changing_spectral_grid/2_10/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"2_1\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/changing_spectral_grid/2_1/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"1_100\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/changing_spectral_grid/1_100/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"1_10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/changing_spectral_grid/1_10/Lev3_A?/Run/\"\n",
    "\n",
    "# data_file_path = \"ConstraintNorms/GhCe.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCeExt.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCeExt_L2.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCeExt_Norms.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCe_L2.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCe_Linf.dat\"\n",
    "data_file_path = \"ConstraintNorms/GhCe_Norms.dat\"\n",
    "# data_file_path = \"ConstraintNorms/GhCe_VolL2.dat\"\n",
    "# data_file_path = \"ConstraintNorms/NormalizedGhCe_Linf.dat\"\n",
    "# data_file_path = \"ConstraintNorms/NormalizedGhCe_Norms.dat\"\n",
    "# data_file_path=\"CharSpeedNorms/CharSpeeds_Min_SliceLFF.SphereA0.dat\"\n",
    "# data_file_path=\"MinimumGridSpacing.dat\"\n",
    "# data_file_path=\"GrAdjustMaxTstepToDampingTimes.dat\"\n",
    "# data_file_path=\"GrAdjustSubChunksToDampingTimes.dat\"\n",
    "# data_file_path=\"DiagAhSpeedA.dat\"\n",
    "# data_file_path=\"ApparentHorizons/AhA.dat\"\n",
    "# data_file_path=\"ApparentHorizons/AhB.dat\" \n",
    "# data_file_path=\"ApparentHorizons/MinCharSpeedAhA.dat\"\n",
    "# data_file_path=\"ApparentHorizons/RescaledRadAhA.dat\"\n",
    "# data_file_path=\"ApparentHorizons/AhACoefs.dat\"\n",
    "# data_file_path=\"ApparentHorizons/AhBCoefs.dat\"\n",
    "# data_file_path=\"ApparentHorizons/Trajectory_AhB.dat\"\n",
    "# data_file_path=\"ApparentHorizons/HorizonSepMeasures.dat\"\n",
    "# data_file_path=\"TStepperDiag.dat\"\n",
    "# data_file_path=\"TimeInfo.dat\"\n",
    "# data_file_path = \"Hist-FuncSkewAngle.txt\"\n",
    "# data_file_path = \"Hist-FuncCutX.txt\"\n",
    "# data_file_path = \"Hist-FuncExpansionFactor.txt\"\n",
    "# data_file_path = \"Hist-FuncLambdaFactorA0.txt\"\n",
    "# data_file_path = \"Hist-FuncLambdaFactorA.txt\"\n",
    "# data_file_path = \"Hist-FuncLambdaFactorB0.txt\"\n",
    "# data_file_path = \"Hist-FuncLambdaFactorB.txt\"\n",
    "# data_file_path = \"Hist-FuncQuatRotMatrix.txt\"\n",
    "# data_file_path = \"Hist-FuncSkewAngle.txt\"\n",
    "# data_file_path = \"Hist-FuncSmoothCoordSep.txt\"\n",
    "# data_file_path = \"Hist-FuncSmoothMinDeltaRNoLam00AhA.txt\"\n",
    "# data_file_path = \"Hist-FuncSmoothMinDeltaRNoLam00AhB.txt\"\n",
    "# data_file_path = \"Hist-FuncSmoothRAhA.txt\"\n",
    "# data_file_path = \"Hist-FuncSmoothRAhB.txt\"\n",
    "# data_file_path = \"Hist-FuncTrans.txt\"\n",
    "# data_file_path = \"Hist-GrDomain.txt\"\n",
    "# data_file_path = \"Profiler.h5\"\n",
    "column_names, runs_data_dict = load_data_from_levs(runs_to_plot,data_file_path)\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_avg_len=0\n",
    "save_path = None\n",
    "diff_base = None\n",
    "x_axis = 't(M)'\n",
    "\n",
    "# diff_base = 'Lev3_AA_tol12_all_10'\n",
    "# add_max_and_min_val(runs_data_dict)\n",
    "# y_axis = 'max_val'\n",
    "# y_axis = 'min_val'\n",
    "\n",
    "y_axis = 'Linf(GhCe)'\n",
    "# y_axis = 'Tx'\n",
    "# y_axis = 'Linf(GhCeExt)'\n",
    "# y_axis = 'L2(GhCe)'\n",
    "# y_axis = 'Linf(NormalizedGhCe) on SphereA0'\n",
    "# y_axis = 'Linf(GhCe) on SphereA0'\n",
    "# y_axis = 'Linf(GhCe) on SphereB0'\n",
    "# y_axis = 'Linf(GhCe) on SphereC0'\n",
    "# y_axis = 'Linf(GhCe) on SphereE0'\n",
    "# y_axis = 'L2(GhCe) on SphereB0'\n",
    "# y_axis = 'Linf(GhCe) on SphereC1'\n",
    "# y_axis = 'Linf(GhCe) on CylinderSMB1.0'\n",
    "# y_axis = 'Linf(GhCe) on CylinderSMB1.0'\n",
    "# y_axis = 'Linf(GhCe) on FilledCylinderMB0'\n",
    "# y_axis = 'Linf(GhCe) on FilledCylinderMA0'\n",
    "\n",
    "# y_axis = 'MPI::MPreduceAdd(MV<double>)_exc'\n",
    "# x_axis = 't'\n",
    "# y_axis = 'T [hours]'\n",
    "# y_axis = 'dt/dT'\n",
    "\n",
    "# x_axis = 't'\n",
    "# y_axis = 'CoordSepHorizons'\n",
    "# y_axis = 'ProperSepHorizons'\n",
    "# y_axis = 'MinCharSpeedAhA[9]'\n",
    "# y_axis = 'MinimumGridSpacing[SphereB0]'\n",
    "# y_axis = 'MinimumGridSpacing[SphereA0]'\n",
    "# y_axis = 'MinimumGridSpacing[CylinderSMB0.0]'\n",
    "# y_axis = 'dt/dT'\n",
    "\n",
    "\n",
    "# x_axis = 'time'\n",
    "# y_axis = 'ROverRAh(surf 9)'\n",
    "# y_axis = 'L_surface'\n",
    "# y_axis = 'L_mesh'\n",
    "# y_axis = 'L_max'\n",
    "# y_axis = 'NormalizedResidual'\n",
    "# y_axis = 'Shape_TruncationError'\n",
    "# y_axis = 'NumIterations'\n",
    "# y_axis = 'max(R_ij)'\n",
    "# y_axis = 'Coef(8,8)'\n",
    "# y_axis = 'Coef(1,-1)'\n",
    "# y_axis = 'Center-y'\n",
    "# y_axis = 'Coef(0,0)'\n",
    "# y_axis = 'Shape_NumberOfPiledUpModes'\n",
    "# y_axis = 'max(r)'\n",
    "# y_axis = 'min(r)'\n",
    "# y_axis = 'max(|r^i-c^i|)'\n",
    "\n",
    "\n",
    "# y_axis = 'courant factor'\n",
    "# y_axis = 'error/1e-08'\n",
    "# y_axis = 'NfailedSteps'\n",
    "# y_axis = 'NumRhsEvaluations in this segment'\n",
    "# y_axis = 'dt'\n",
    "\n",
    "minT = 0\n",
    "# minT = 1450\n",
    "# minT = 2100\n",
    "# minT = 2692\n",
    "# minT = 2710\n",
    "# minT = 3500\n",
    "# minT = 4000\n",
    "# minT = 6115+5\n",
    "minT = 9272\n",
    "# minT = 9100\n",
    "# minT = 9372\n",
    "\n",
    "# maxT = 2710\n",
    "# maxT = minT+0.5\n",
    "# maxT = minT+2\n",
    "# maxT = minT+100\n",
    "# maxT = 4500\n",
    "# maxT = 4000\n",
    "# maxT = 9300\n",
    "# maxT = 9375\n",
    "maxT = 40000\n",
    "# moving_avg_len = 50\n",
    "# moving_avg_len = 10\n",
    "\n",
    "plot_fun = lambda x,y,label : plt.plot(x,y,label=label)\n",
    "# plot_fun = lambda x,y,label : plt.plot(x,y,label=label,marker='x')\n",
    "plot_fun = lambda x,y,label : plt.semilogy(x,y,label=label)\n",
    "# plot_fun = lambda x,y,label : plt.semilogy(x,y,label=label,marker='x')\n",
    "# plot_fun = lambda x,y,label : plt.loglog(x,y,label=label) \n",
    "# plot_fun = lambda x,y,label : plt.scatter(x,y,label=label,s=10,marker=\"x\")\n",
    "# save_path = \"/groups/sxs/hchaudha/rough/high_acc_plots/\"\n",
    "# save_path = \"/groups/sxs/hchaudha/rough/plots/\"\n",
    "# save_path = \"/home/hchaudha/notes/spec_accuracy/figures/\"\n",
    "legend_dict = {}\n",
    "for key in runs_data_dict.keys():\n",
    "  legend_dict[key] = None\n",
    "\n",
    "# legend_dict = { '3_DH_q1_ns_d18_L3': \"Lev3\",\n",
    "#                 '3_DH_q1_ns_d18_L6': \"Lev6\",\n",
    "#                 'all_10': \"Lev3_all_tols_10\",\n",
    "#                 'all_100': \"Lev3_all_tols_100\",\n",
    "#                 'near_bhs_10': \"Lev3_sphere_AB_tols_10\",\n",
    "#                 'near_bhs_100': \"Lev3_sphere_AB_tols_100\"}\n",
    "# legend_dict = {\n",
    "#  '3_DH_q1_ns_d18_L3':\"Lev3_ode_tol_1e-8\",\n",
    "#  '3_DH_q1_ns_d18_L3_tol9':\"Lev3_ode_tol_1e-9\",\n",
    "#  '3_DH_q1_ns_d18_L3_tol10':\"Lev3_ode_tol_1e-10\",\n",
    "#  '3_DH_q1_ns_d18_L3_tol11':\"Lev3_ode_tol_1e-11\",\n",
    "#  '3_DH_q1_ns_d18_L3_all_100_tol10':\"Lev3_AMR_tol_100_ode_tol_1e-11\",\n",
    "#  }\n",
    "\n",
    "with plt.style.context('default'):\n",
    "  plt.rcParams[\"figure.figsize\"] = (12,10)\n",
    "  plt.rcParams[\"figure.autolayout\"] = True\n",
    "  plot_graph_for_runs(runs_data_dict, x_axis, y_axis, minT, maxT, legend_dict=legend_dict, save_path=save_path, moving_avg_len=moving_avg_len, plot_fun=plot_fun, diff_base=diff_base)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_avg_len=0\n",
    "save_path = \"/home/hchaudha/notes/spec_accuracy/ghce_domains/\"\n",
    "diff_base = None\n",
    "\n",
    "plot_fun = lambda x,y,label : plt.semilogy(x,y,label=label,marker='x')\n",
    "plot_fun = lambda x,y,label : plt.semilogy(x,y,label=label)\n",
    "\n",
    "legend_dict = {}\n",
    "for key in runs_data_dict.keys():\n",
    "  legend_dict[key] = None\n",
    "\n",
    "minT = 9300\n",
    "maxT = 10000\n",
    "\n",
    "x_axis = 't(M)'\n",
    "\n",
    "for y_axis in column_names:\n",
    "  with plt.style.context('default'):\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,10)\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "    plot_graph_for_runs(runs_data_dict, x_axis, y_axis, minT, maxT, legend_dict=legend_dict, save_path=save_path, moving_avg_len=moving_avg_len, plot_fun=plot_fun, diff_base=diff_base)\n",
    "    plt.close()\n",
    "  print(f\"{y_axis} done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in column_names[1:]:\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = runs_data_dict['all_100_t2690_obs']\n",
    "minT = 2690\n",
    "minT = 2700\n",
    "maxT = 2710\n",
    "maxT = minT+2\n",
    "df = df[df['time']>minT]\n",
    "df = df[df['time']<maxT]\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# plt.plot(df['time'],df['Linf(GhCe) on SphereA0'],marker='x')\n",
    "# plt.plot(df['time'],df['Linf(GhCe) on SphereA4'],marker='x')\n",
    "plt.plot(df['time'],df['Linf(GhCe) on SphereA3'],marker='x')\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = runs_data_dict['all_100_t2690_obs_grid_tol_10']\n",
    "data = data[data['time after step']>2690]\n",
    "dt_arr = np.array(data.dt)\n",
    "averaged_dt = np.zeros_like(dt_arr)\n",
    "averaged_dt[0] =  dt_arr.mean() \n",
    "N = 100\n",
    "for i in range(len(dt_arr)-1):\n",
    "  averaged_dt[i+1] = averaged_dt[i]*(N-1)/(N)+dt_arr[i+1]*1/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['time after step'],averaged_dt)\n",
    "plt.plot(data['time after step'],dt_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_file_path=\"ConstraintNorms/GhCe_L2.dat\"\n",
    "data_file_path=\"ConstraintNorms/GhCe_Linf.dat\"\n",
    "# data_file_path=\"ConstraintNorms/NormalizedGhCe_Linf.dat\"\n",
    "# data_file_path=\"ConstraintNorms/GhCe_VolL2.dat\"\n",
    "column_names_linf2, runs_data_dict_linf2 = load_data_from_levs(runs_to_plot,data_file_path)\n",
    "# print(column_names_linf2)\n",
    "print(runs_data_dict_linf2.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_name = list(runs_data_dict_linf2.keys())[4]\n",
    "run_name = 'high_accuracy_L5'\n",
    "run_name = '3_DH_q1_ns_d18_L6'\n",
    "# run_name = '3_DH_q1_ns_d18_L3_all_100_tol10_rd'\n",
    "# run_name = 'all_100_t2690_eteq_tol_11'\n",
    "# run_name = 'all_100_obs_tol_10'\n",
    "# run_name = 'all_10_obs'\n",
    "# run_name = 'all_10_obs_tol_10'\n",
    "# run_name = 'all_100_t2690_obs_grid_dt'\n",
    "df = runs_data_dict_linf2[run_name].copy()\n",
    "df = df.sort_values(by=df.columns[0])\n",
    "\n",
    "tmin= 1050\n",
    "# tmin=9300\n",
    "# tmin=9372\n",
    "# tmin=2691\n",
    "tmax=2000\n",
    "# tmax=tmin+4\n",
    "# tmax=6150\n",
    "# tmax=50000\n",
    "\n",
    "df = df[df['t(M)']>=tmin]\n",
    "df = df[df['t(M)']<tmax]\n",
    "t_name = df.columns[0]\n",
    "y_axis = df.columns[1].split(\" \")[0]\n",
    "all_cols_but_t = df.columns[1:]\n",
    "# Find the maximum value across columns B, C, D, and F for each row\n",
    "df['max_val'] = df[all_cols_but_t].max(axis=1)\n",
    "\n",
    "# Determine which column had the maximum value\n",
    "df['max_source'] = df[all_cols_but_t].idxmax(axis=1)\n",
    "\n",
    "# List all columns that have at least one max value\n",
    "columns_with_max = df['max_source'].unique()\n",
    "\n",
    "# Generate a colormap for the columns with at least one max value\n",
    "num_colors = len(columns_with_max)\n",
    "colors = plt.get_cmap('tab20', num_colors)  # Using 'tab20' colormap\n",
    "color_map = {column: colors(i) for i, column in enumerate(columns_with_max)}\n",
    "\n",
    "# Plot max_BCD vs t with different colors for different sources\n",
    "plt.figure(figsize=(18, 10))\n",
    "for i,source in enumerate(columns_with_max):\n",
    "    subset = df[df['max_source'] == source]\n",
    "    if i%4 == 0:\n",
    "        plt.scatter(subset[t_name], subset['max_val'], color=color_map[source], label=source, s=10, marker=\"^\")\n",
    "    if i%4 == 1:\n",
    "        plt.scatter(subset[t_name], subset['max_val'], color=color_map[source], label=source, s=10, marker=\"v\")\n",
    "    if i%4 == 2:\n",
    "        plt.scatter(subset[t_name], subset['max_val'], color=color_map[source], label=source, s=10, marker=\">\")\n",
    "    if i%4 == 3:\n",
    "        plt.scatter(subset[t_name], subset['max_val'], color=color_map[source], label=source, s=10, marker=\"<\")\n",
    "\n",
    "plt.xlabel(t_name)\n",
    "plt.ylabel(y_axis)\n",
    "plt.yscale('log')\n",
    "plt.title(f'{y_axis} vs {t_name} for {run_name}')\n",
    "plt.legend()\n",
    "plt.grid(True)  \n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"/groups/sxs/hchaudha/rough/{run_name}_{y_axis}_vs_{t_name}_{tmin}_{tmax}.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_file_path=\"MinimumGridSpacing.dat\"\n",
    "column_names, runs_data_dict = load_data_from_levs(runs_to_plot,data_file_path)\n",
    "sort_run_data_dict(runs_data_dict)\n",
    "plot_min_grid_spacing(runs_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path=\"GrAdjustSubChunksToDampingTimes.dat\"\n",
    "column_names, runs_data_dict = load_data_from_levs(runs_to_plot,data_file_path)\n",
    "sort_run_data_dict(runs_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(runs_data_dict.keys())\n",
    "idx = 0\n",
    "runs_data_dict_temp = {f\"{list(runs_data_dict.keys())[idx]}\":runs_data_dict[list(runs_data_dict.keys())[idx]]}\n",
    "plot_GrAdjustSubChunksToDampingTimes(runs_data_dict_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plots for h5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_to_plot = {}\n",
    "# runs_to_plot[\"boost_ID_test_wrong\"] =  \"/groups/sxs/hchaudha/spec_runs/boost_ID_test_wrong/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"boost_ID_test_correct\"] =  \"/groups/sxs/hchaudha/spec_runs/boost_ID_test_correct/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"corrected_coord_spin1\"] =  \"/groups/sxs/hchaudha/spec_runs/corrected_coord_spin1/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"corrected_coord_spin2\"] =  \"/groups/sxs/hchaudha/spec_runs/corrected_coord_spin2/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"2_SpKS_q1_sA_0_0_0_sB_0_0_0_d15\"] =  \"/groups/sxs/hchaudha/spec_runs/2_SpKS_q1_sA_0_0_0_sB_0_0_0_d15/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"2_SpKS_q1_sA_0_0_0_sB_0_0_99_d15\"] =  \"/groups/sxs/hchaudha/spec_runs/2_SpKS_q1_sA_0_0_0_sB_0_0_99_d15/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"2_SpKS_q1_sA_0_0_0_sB_0_0_9_d15\"] =  \"/groups/sxs/hchaudha/spec_runs/2_SpKS_q1_sA_0_0_0_sB_0_0_9_d15/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"2_SpKS_q1_sA_0_0_9_sB_0_0_9_d15\"] =  \"/groups/sxs/hchaudha/spec_runs/2_SpKS_q1_sA_0_0_9_sB_0_0_9_d15/Ev/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"3_DH_q1_ns_d18_L3\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_tol8_eq\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol8_eq/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_tol9\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol9/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_tol10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_tol10_hi\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol10_hi/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_tol11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_all_100_tol10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/L6_tol10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_all_1000_tol11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/L6_all_10_tol11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"local_100_tol5_11\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/local_100_tol5_11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"local_10_tol5_10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/local_10_tol5_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_local_10_tol5_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/local_10_tol5_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"L3_local_100_tol5_11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/local_100_tol5_11/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"high_accuracy_L3\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L4\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev4_A?/Run/\"\n",
    "# runs_to_plot[\"high_accuracy_L5\"] =  \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev5_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3/Ev/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_tol8_eq_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol8_eq/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_tol9_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol9/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_tol10_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol10/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_tol10_hi_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol10_hi/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_tol11_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/tol11/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_all_100_tol10_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/L6_tol10/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_all_1000_tol11_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/L6_all_10_tol11/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L3_rd\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3/Ev/Lev3_Ringdown/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"3_DH_q1_ns_d18_L6\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L6/Ev/Lev6_A?/Run/\"\n",
    "# runs_to_plot[\"L6_1.1\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L6_higher_acc/L6_1.1/Lev6_A?/Run/\"\n",
    "# runs_to_plot[\"L6_1.1_dp8_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L6_higher_acc/L6_1.1_dp8_tol_10/Lev6_A?/Run/\"\n",
    "# runs_to_plot[\"L6_1.1_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L6_higher_acc/L6_1.1_tol_10/Lev6_A?/Run/\"\n",
    "# runs_to_plot[\"L6_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L6_higher_acc/L6_tol_10/Lev6_A?/Run/\"\n",
    "# runs_to_plot[\"all_10\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"near_bhs_10\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/near_bhs_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"near_bhs_100\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/near_bhs_100/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"same_obs\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/same_obs/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"all_10_obs\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_10_obs/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"all_100_obs\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_obs/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"all_10_obs_tol_10\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_10_obs_tol_10/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"all_100_obs_tol_10\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_obs_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_1.1_b0\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_1.1_b0/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_1.1_b0_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_1.1_b0_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_1.1\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_1.1/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_1.1_dp8_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_1.1_dp8_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_1.1_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_1.1_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_2\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_2/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_2\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_2/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_3\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_3/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dp8\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dp8/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dp8_tol10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dp8_tol10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dp8_tol11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dp8_tol11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dt\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dt_0.02\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.02/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dt_0.03\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.03/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dt_0.04\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.04/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dt_0.025\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.025/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dt_0.021\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.021/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dt_0.022\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.022/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dt_0.023\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.023/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dt_0.024\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.024/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dt_0.0225\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt_0.0225/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_dt005\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_dt0.005/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_tol_11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_tol_11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_obs_grid_tol_9\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_tol_9/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_tol_1.128e-11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_tol_1.128e-11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_tol_1.692e-11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_tol_1.692e-11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_tol_3.383e-11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_tol_3.383e-11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"near_bhs_10_obs\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/near_bhs_10_obs/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"near_bhs_100_obs\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/near_bhs_100_obs/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L6_AA\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L6_AA/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"4_SphKS_q1_15_SSphKS_ID\"] =  \"/groups/sxs/hchaudha/spec_runs/4_SphKS_q1_15_SSphKS_ID/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"4_SphKS_q1_15_SKS_ID\"] =  \"/groups/sxs/hchaudha/spec_runs/4_SphKS_q1_15_SKS_ID/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"5_gd_SphKS_gauge_ID\"] =  \"/groups/sxs/hchaudha/spec_runs/5_gd_SphKS_gauge_ID/Ev/Lev2_A[A-S]/Run/\"\n",
    "# runs_to_plot[\"5_ngd_SphKS_ID\"] =  \"/groups/sxs/hchaudha/spec_runs/5_ngd_SphKS_ID/Ev/Lev2_A?/Run/\"\n",
    "# runs_to_plot[\"5_ngd_KS_ID\"] =  \"/groups/sxs/hchaudha/spec_runs/5_ngd_KS_ID/Ev/Lev2_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"all_100_t2690_eteq_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eteq_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eteq_tol_11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eteq_tol_11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eteq_tol_12\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eteq_tol_12/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eteq_tol_8\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eteq_tol_8/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eteq_tol_9\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eteq_tol_9/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eteq_tol_eq\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eteq_tol_eq/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eth_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eth_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eth_tol_11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eth_tol_11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eth_tol_12\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eth_tol_12/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eth_tol_8\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eth_tol_8/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eth_tol_9\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eth_tol_9/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_eth_tol_eq\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eth_tol_eq/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_etl_tol_10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_etl_tol_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_etl_tol_11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_etl_tol_11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_etl_tol_8\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_etl_tol_8/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_etl_tol_9\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_etl_tol_9/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"all_100_t2690_etl_tol_eq\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_etl_tol_eq/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"t6115_tol11\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol10\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol9\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol9/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol8\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol8/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol7\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol7/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol11_AMR\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol11_AMR/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol10_AMR\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol10_AMR/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol9_AMR\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol9_AMR/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol8_AMR\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol8_AMR/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol7_AMR\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol7_AMR/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_tol8_linf\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol8_linf/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.02\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.02/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.03\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.03/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.041\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.041/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.042\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.042/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.043\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.043/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.044\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.044/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.045\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.045/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.046\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.046/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.047\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.047/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.048\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.048/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.049\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.049/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.050\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.050/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.052\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.052/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.054\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.054/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_dt0.056\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_dt0.056/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_2.368e-07\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_2.368e-07/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_1.692e-07\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_1.692e-07/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_1.015e-07\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_1.015e-07/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_6.767e-08\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_6.767e-08/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_5.075e-08\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_5.075e-08/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_3.383e-08\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_3.383e-08/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_2.256e-08\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_2.256e-08/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_1.692e-08\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_1.692e-08/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_1.128e-08\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_1.128e-08/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_6.767e-09\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_6.767e-09/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_4.833e-09\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_4.833e-09/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_3.383e-09\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_3.383e-09/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_1.692e-09\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_1.692e-09/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t6115_linf_tol_1.128e-09\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_linf_tol_1.128e-09/Lev3_A?/Run/\"\n",
    "\n",
    "\n",
    "# runs_to_plot[\"all_100_t2710_0.021_0.021\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_0.21/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_0.022\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_0.22/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_0.0225\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_0.225/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_0.023\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_0.23/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_0.024\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_0.24/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_max_tol8\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_max_tol8/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_max_tol9\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_max_tol9/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_max_tol10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_max_tol10/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_max_tol10.5\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_max_tol10.5/Lev3_AE/Run/\"\n",
    "# runs_to_plot[\"all_100_t2710_0.021_max_tol11\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100_t2710_0.021_max_tol11/Lev3_AE/Run/\"\n",
    "\n",
    "# runs_to_plot[\"eq_t4000_tol10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/eq_t4000_tol10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"eq_t4000_tol5_10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/eq_t4000_tol5_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"eq_t4000_tol5_11\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/eq_t4000_tol5_11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"eq_t4000_tol9\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/eq_t4000_tol9/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t4000_tol10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/t4000_tol10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t4000_tol5_10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/t4000_tol5_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t4000_tol5_11\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/t4000_tol5_11/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t4000_tol8\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/t4000_tol8/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"t4000_tol9\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_all_10/t4000_tol9/Lev3_A?/Run/\"\n",
    "\n",
    "# runs_to_plot[\"Lev3_AA_tol10_all_10\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol10_all_10/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol11_all_10\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol11_all_10/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol12_all_10\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol12_all_10/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol5_10_all_10\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol5_10_all_10/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol5_11_all_10\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol5_11_all_10/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol10\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol10/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol11\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol11/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol12\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol12/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol5_10\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol5_10/Lev3_A?_/Run/\"\n",
    "# runs_to_plot[\"Lev3_AA_tol5_11\"]  = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/rd_all_10_tol11/Lev3_AA_tol5_11/Lev3_A?_/Run/\"\n",
    "\n",
    "# runs_to_plot[\"3_100\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/changing_spectral_grid/3_100/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/changing_spectral_grid/3_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_1\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/changing_spectral_grid/3_1/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"2_100\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/changing_spectral_grid/2_100/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"2_10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/changing_spectral_grid/2_10/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"2_1\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/changing_spectral_grid/2_1/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"1_100\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/changing_spectral_grid/1_100/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"1_10\"] = \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_ringdown_tol/changing_spectral_grid/1_10/Lev3_A?/Run/\"\n",
    "\n",
    "data_file_path = \"extracted-FilterDiagnostics/BoundaryFilters.dir/ExpChebCoef.dir/SliceLFF.SphereA0.dat\"\n",
    "data_file_path = \"extracted-PowerDiagnostics/SphereA0.dir/Bf0I1_NumberOfPiledUpModes.dat\"\n",
    "data_file_path = \"extracted-PowerDiagnostics/SphereA0.dir/Bf0I1_PowerInHighestUnfilteredModes.dat\"\n",
    "data_file_path = \"extracted-PowerDiagnostics/SphereA0.dir/Bf0I1_TruncationError.dat\"\n",
    "data_file_path = \"extracted-PowerDiagnostics/SphereA0.dir/Bf1S2_TruncationError.dat\"\n",
    "# data_file_path = \"extracted-RhsExpense/CostPerProc.dir/Proc00.dat\"\n",
    "# data_file_path = \"extracted-RhsExpense/CostPerSubdomain.dir/SphereB2.dat\"\n",
    "# data_file_path = \"extracted-OrbitDiagnostics/OrbitalPhase.dat\"\n",
    "# data_file_path = \"extracted-AdjustGridExtents/SphereA0.dir/Extents.dat\"\n",
    "# data_file_path = \"extracted-ControlNthDeriv/ExpansionFactor.dir/a.dat\"\n",
    "# data_file_path = \"extracted-ControlNthDeriv/Trans.dir/Tx.dat\"\n",
    "# data_file_path = \"extracted-FilterDiagnostics/SubdomainFilters.dir/ExpChebCoef.dir/SphereB0.dat\"\n",
    "\n",
    "column_names, runs_data_dict = load_data_from_levs(runs_to_plot,data_file_path)\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_avg_len=0\n",
    "save_path = None\n",
    "diff_base = None\n",
    "x_axis = 't(M)'\n",
    "\n",
    "# diff_base = 'Lev3_AA_tol12_all_10'\n",
    "# add_max_and_min_val(runs_data_dict)\n",
    "# y_axis = 'max_val'\n",
    "# y_axis = 'min_val'\n",
    "\n",
    "y_axis = 'GridDiagPowerpsi'\n",
    "y_axis = 'GridDiagPowerkappa'\n",
    "# y_axis = 'CostPerPoint'\n",
    "# y_axis = 'Cost'\n",
    "# y_axis = 'NumberOfPoints'\n",
    "# y_axis = 'Q'\n",
    "# y_axis = 'lambda'\n",
    "# y_axis = 'Bf0I1'\n",
    "\n",
    "minT = 0\n",
    "minT = 2100\n",
    "# minT = 2710\n",
    "\n",
    "maxT = 4000\n",
    "# maxT = 9300\n",
    "# maxT = 9375\n",
    "# maxT = 40000\n",
    "# moving_avg_len = 50\n",
    "# moving_avg_len = 10\n",
    "\n",
    "plot_fun = lambda x,y,label : plt.plot(x,y,label=label)\n",
    "# plot_fun = lambda x,y,label : plt.plot(x,y,label=label,marker='x')\n",
    "# plot_fun = lambda x,y,label : plt.semilogy(x,y,label=label)\n",
    "# plot_fun = lambda x,y,label : plt.semilogy(x,y,label=label,marker='x')\n",
    "# plot_fun = lambda x,y,label : plt.loglog(x,y,label=label) \n",
    "# plot_fun = lambda x,y,label : plt.scatter(x,y,label=label,s=10,marker=\"x\")\n",
    "# save_path = \"/groups/sxs/hchaudha/rough/high_acc_plots/\"\n",
    "# save_path = \"/groups/sxs/hchaudha/rough/plots/\"\n",
    "# save_path = \"/home/hchaudha/notes/spec_accuracy/figures/\"\n",
    "legend_dict = {}\n",
    "for key in runs_data_dict.keys():\n",
    "  legend_dict[key] = None\n",
    "\n",
    "# legend_dict = { '3_DH_q1_ns_d18_L3': \"Lev3\",\n",
    "#                 '3_DH_q1_ns_d18_L6': \"Lev6\",\n",
    "#                 'all_10': \"Lev3_all_tols_10\",\n",
    "#                 'all_100': \"Lev3_all_tols_100\",\n",
    "#                 'near_bhs_10': \"Lev3_sphere_AB_tols_10\",\n",
    "#                 'near_bhs_100': \"Lev3_sphere_AB_tols_100\"}\n",
    "# legend_dict = {\n",
    "#  '3_DH_q1_ns_d18_L3':\"Lev3_ode_tol_1e-8\",\n",
    "#  '3_DH_q1_ns_d18_L3_tol9':\"Lev3_ode_tol_1e-9\",\n",
    "#  '3_DH_q1_ns_d18_L3_tol10':\"Lev3_ode_tol_1e-10\",\n",
    "#  '3_DH_q1_ns_d18_L3_tol11':\"Lev3_ode_tol_1e-11\",\n",
    "#  '3_DH_q1_ns_d18_L3_all_100_tol10':\"Lev3_AMR_tol_100_ode_tol_1e-11\",\n",
    "#  }\n",
    "\n",
    "with plt.style.context('default'):\n",
    "  plt.rcParams[\"figure.figsize\"] = (12,10)\n",
    "  plt.rcParams[\"figure.autolayout\"] = True\n",
    "  plot_graph_for_runs(runs_data_dict, x_axis, y_axis, minT, maxT, legend_dict=legend_dict, save_path=save_path, moving_avg_len=moving_avg_len, plot_fun=plot_fun, diff_base=diff_base)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots for one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = Path(\"/groups/sxs/hchaudha/spec_runs/\")\n",
    "run_path = main_path/Path(\"3_DH_q1_ns_d18_L3/Ev/Lev3_A?/Run/\")\n",
    "# run_path = main_path/Path(\"3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs/Lev3_AD/Run/\")\n",
    "# run_path = main_path/Path(\"3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs/Lev3_AD/Run/\")\n",
    "# run_path = main_path/Path(\"3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_tol_9/Lev3_AD/Run/\")\n",
    "# run_path = main_path/Path(\"3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_tol_10/Lev3_AD/Run/\")\n",
    "# run_path = main_path/Path(\"3_DH_q1_ns_d18_L3_higher_acc/all_100_t2690_obs_grid_tol_11/Lev3_AD/Run/\")\n",
    "\n",
    "data_files = {}\n",
    "\n",
    "# data_files[\"GrAdjustSubChunksToDampingTimes\"] = {\"path\":run_path/\"GrAdjustSubChunksToDampingTimes.dat\", \"prefix\":None}\n",
    "# data_files[\"MemoryInfo\"] = {\"path\":run_path/\"MemoryInfo.dat\", \"prefix\":None}\n",
    "# data_files[\"MinimumGridSpacing\"] = {\"path\":run_path/\"MinimumGridSpacing.dat\", \"prefix\":None}\n",
    "# data_files[\"DiagInclinationAngle\"] = {\"path\":run_path/\"DiagInclinationAngle.dat\", \"prefix\":None}\n",
    "# data_files[\"ApparentHorizons/Trajectory_AhA\"] = {\"path\":run_path/\"ApparentHorizons/Trajectory_AhA.dat\", \"prefix\":\"AhA\"}\n",
    "# data_files[\"ApparentHorizons/MinCharSpeedAhA\"] = {\"path\":run_path/\"ApparentHorizons/MinCharSpeedAhA.dat\", \"prefix\":\"AhA\"}\n",
    "# data_files[\"ApparentHorizons/Trajectory_AhB\"] = {\"path\":run_path/\"ApparentHorizons/Trajectory_AhB.dat\", \"prefix\":\"AhB\"}\n",
    "# data_files[\"ApparentHorizons/SmoothCoordSepHorizon\"] = {\"path\":run_path/\"ApparentHorizons/SmoothCoordSepHorizon.dat\", \"prefix\":None}\n",
    "# data_files[\"ApparentHorizons/MinCharSpeedAhB\"] = {\"path\":run_path/\"ApparentHorizons/MinCharSpeedAhB.dat\", \"prefix\":\"AhB\"}\n",
    "# data_files[\"ApparentHorizons/RescaledRadAhB\"] = {\"path\":run_path/\"ApparentHorizons/RescaledRadAhB.dat\", \"prefix\":\"AhB\"}\n",
    "# data_files[\"ApparentHorizons/AhACoefs\"] = {\"path\":run_path/\"ApparentHorizons/AhACoefs.dat\", \"prefix\":\"AhA\"}\n",
    "# data_files[\"ApparentHorizons/AhB\"] = {\"path\":run_path/\"ApparentHorizons/AhB.dat\", \"prefix\":\"AhB\"}\n",
    "# data_files[\"ApparentHorizons/HorizonSepMeasures\"] = {\"path\":run_path/\"ApparentHorizons/HorizonSepMeasures.dat\", \"prefix\":None}\n",
    "# data_files[\"ApparentHorizons/AhA\"] = {\"path\":run_path/\"ApparentHorizons/AhA.dat\", \"prefix\":\"AhA\"}\n",
    "# data_files[\"ApparentHorizons/RescaledRadAhA\"] = {\"path\":run_path/\"ApparentHorizons/RescaledRadAhA.dat\", \"prefix\":\"AhA\"}\n",
    "# data_files[\"ApparentHorizons/AhBCoefs\"] = {\"path\":run_path/\"ApparentHorizons/AhBCoefs.dat\", \"prefix\":\"AhB\"}\n",
    "# data_files[\"TimeInfo\"] = {\"path\":run_path/\"TimeInfo.dat\", \"prefix\":None}\n",
    "# data_files[\"GrAdjustMaxTstepToDampingTimes\"] = {\"path\":run_path/\"GrAdjustMaxTstepToDampingTimes.dat\", \"prefix\":None}\n",
    "# data_files[\"FailedTStepperDiag\"] = {\"path\":run_path/\"FailedTStepperDiag.dat\", \"prefix\":None}\n",
    "# data_files[\"DiagAhSpeedA\"] = {\"path\":run_path/\"DiagAhSpeedA.dat\", \"prefix\":\"AhA\"}\n",
    "# data_files[\"DiagAhSpeedB\"] = {\"path\":run_path/\"DiagAhSpeedB.dat\", \"prefix\":\"AhB\"}\n",
    "# data_files[\"CharSpeedNorms/CharSpeeds_Max_SliceLFF.SphereA0\"] = {\"path\":run_path/\"CharSpeedNorms/CharSpeeds_Max_SliceLFF.SphereA0.dat\", \"prefix\":\"Max_A0\"}\n",
    "# data_files[\"CharSpeedNorms/CharSpeeds_Min_SliceLFF.SphereA0\"] = {\"path\":run_path/\"CharSpeedNorms/CharSpeeds_Min_SliceLFF.SphereA0.dat\", \"prefix\":\"Min_A0\"}\n",
    "# data_files[\"CharSpeedNorms/CharSpeeds_Min_SliceLFF.SphereB0\"] = {\"path\":run_path/\"CharSpeedNorms/CharSpeeds_Min_SliceLFF.SphereB0.dat\", \"prefix\":\"Max_B0\"}\n",
    "# data_files[\"CharSpeedNorms/CharSpeeds_Max_SliceLFF.SphereB0\"] = {\"path\":run_path/\"CharSpeedNorms/CharSpeeds_Max_SliceLFF.SphereB0.dat\", \"prefix\":\"Min_B0\"}\n",
    "# data_files[\"CharSpeedNorms/CharSpeeds_Max_SliceUFF.SphereC29\"] = {\"path\":run_path/\"CharSpeedNorms/CharSpeeds_Max_SliceUFF.SphereC29.dat\", \"prefix\":\"Max_C29\"}\n",
    "# data_files[\"CharSpeedNorms/CharSpeeds_Min_SliceUFF.SphereC29\"] = {\"path\":run_path/\"CharSpeedNorms/CharSpeeds_Min_SliceUFF.SphereC29.dat\", \"prefix\":\"Min_C29\"}\n",
    "# data_files[\"ConstraintNorms/NormalizedGhCe_Norms\"] = {\"path\":run_path/\"ConstraintNorms/NormalizedGhCe_Norms.dat\", \"prefix\":None}\n",
    "# data_files[\"ConstraintNorms/GhCeExt_Norms\"] = {\"path\":run_path/\"ConstraintNorms/GhCeExt_Norms.dat\", \"prefix\":None}\n",
    "# data_files[\"ConstraintNorms/GhCe_L2\"] = {\"path\":run_path/\"ConstraintNorms/GhCe_L2.dat\", \"prefix\":None}\n",
    "# data_files[\"ConstraintNorms/GhCeExt_L2\"] = {\"path\":run_path/\"ConstraintNorms/GhCeExt_L2.dat\", \"prefix\":None}\n",
    "# data_files[\"ConstraintNorms/GhCeExt\"] = {\"path\":run_path/\"ConstraintNorms/GhCeExt.dat\", \"prefix\":None}\n",
    "# data_files[\"ConstraintNorms/GhCe\"] = {\"path\":run_path/\"ConstraintNorms/GhCe.dat\", \"prefix\":None}\n",
    "# data_files[\"ConstraintNorms/GhCe_VolL2\"] = {\"path\":run_path/\"ConstraintNorms/GhCe_VolL2.dat\", \"prefix\":None}\n",
    "data_files[\"ConstraintNorms/GhCe_Norms\"] = {\"path\":run_path/\"ConstraintNorms/GhCe_Norms.dat\", \"prefix\":None}\n",
    "# data_files[\"ConstraintNorms/NormalizedGhCe_Linf\"] = {\"path\":run_path/\"ConstraintNorms/NormalizedGhCe_Linf.dat\", \"prefix\":None}\n",
    "# data_files[\"ConstraintNorms/GhCe_Linf\"] = {\"path\":run_path/\"ConstraintNorms/GhCe_Linf.dat\", \"prefix\":None}\n",
    "data_files[\"TStepperDiag\"] = {\"path\":run_path/\"TStepperDiag.dat\", \"prefix\":None}\n",
    "# data_files[\"DiagCutXCorrection\"] = {\"path\":run_path/\"DiagCutXCorrection.dat\", \"prefix\":None}\n",
    "\n",
    "\n",
    "# data = read_dat_file_across_AA(str(data_files['TimeInfo']['path']))\n",
    "for key in data_files:\n",
    "  data_files[key][\"dataframe\"] = read_dat_file_across_AA(str(data_files[key]['path']))\n",
    "  cols = list(data_files[key][\"dataframe\"].columns)\n",
    "  # Make new cols names s.t. the first cols is 't' and add prefix as required\n",
    "  new_cols = []\n",
    "  new_cols.append('t')\n",
    "  if data_files[key]['prefix'] is None:\n",
    "    [new_cols.append(name) for name in cols[1:]]\n",
    "  else:\n",
    "    [new_cols.append(name+\"_\"+data_files[key]['prefix']) for name in cols[1:]]\n",
    "  data_files[key][\"dataframe\"].columns = new_cols\n",
    "\n",
    "  # Set 't' to be a index and copy it into a column called 'time'\n",
    "  # data_files[key][\"dataframe\"][\"time\"] = data_files[key][\"dataframe\"][\"t\"]\n",
    "  # data_files[key][\"dataframe\"].set_index('t', inplace=True)\n",
    "\n",
    "combined = None\n",
    "for key in data_files:\n",
    "  if combined is None:\n",
    "    combined = data_files[key][\"dataframe\"]\n",
    "    continue\n",
    "  else:\n",
    "    combined  = pd.merge(combined,data_files[key][\"dataframe\"],on='t',how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.sort_values(by='t')\n",
    "for i in combined.columns:\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(combined['t'],combined['SuggestedDampingTime'])\n",
    "\n",
    "x_val = 't'\n",
    "plot_list=[\n",
    "  # ('Linf(GhCe) on SphereA0','semilogy',None,None,'x'),\n",
    "  ('Linf(GhCe)','semilogy',None,None,'.'),\n",
    "  ('dt','plot',1e-3,None,None),\n",
    "]\n",
    "\n",
    "for i in plot_list:\n",
    "  y_val, plot_type, mul_factor, add_factor, marker = i\n",
    "\n",
    "  label = y_val\n",
    "  if mul_factor is not None:\n",
    "    label = f\"{label}*{mul_factor}\"\n",
    "  else:\n",
    "    mul_factor = 1\n",
    "  if add_factor is not None:\n",
    "    label = f\"{label}+{add_factor}\"\n",
    "  else:\n",
    "    add_factor = 0\n",
    "\n",
    "\n",
    "  match plot_type:\n",
    "    case 'semilogy':\n",
    "      plt.semilogy(combined[x_val],combined[y_val]*mul_factor+add_factor,marker=marker,label=label)\n",
    "    case 'plot':\n",
    "      plt.plot(combined[x_val],combined[y_val]*mul_factor+add_factor,marker=marker,label=label)\n",
    "\n",
    "title = str(run_path).split('/')[-4]\n",
    "save_name = str(run_path).split('/')[-4]\n",
    "for i in str(run_path).split('/')[-3:-1]:\n",
    "  title = title +\"/\" +i\n",
    "  save_name = save_name+\"&\"+i\n",
    "\n",
    "\n",
    "plt.title(title)\n",
    "plt.xlabel(x_val)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"/groups/sxs/hchaudha/rough/plots/{save_name}.png\",dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def check_duplicate_rows(df, subset_column):\n",
    "    \"\"\"\n",
    "    Function to find and print duplicate rows in a DataFrame based on a specified column.\n",
    "    It checks if all duplicate rows are identical or if they differ in some columns.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to check for duplicates.\n",
    "    subset_column (str): The column to check for duplicate values.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Get rows with duplicate values in the subset column\n",
    "    duplicate_mask = df.duplicated(subset=[subset_column], keep=False)\n",
    "    duplicate_rows = df[duplicate_mask]\n",
    "    \n",
    "    # Sort the duplicate rows by the subset column to group them together\n",
    "    duplicate_rows_sorted = duplicate_rows.sort_values(by=subset_column)\n",
    "    \n",
    "    # Iterate through groups of rows with the same value in the subset column\n",
    "    for _, group in duplicate_rows_sorted.groupby(subset_column):\n",
    "        if len(group) > 1:\n",
    "            print(f\"\\nRows with '{subset_column}' value: {group[subset_column].iloc[0]}\")\n",
    "            \n",
    "            # Check if all rows in the group are identical\n",
    "            identical = all(group.iloc[0].equals(row) for _, row in group.iterrows())\n",
    "            if identical:\n",
    "                print(\"All rows are identical:\")\n",
    "                print(group)\n",
    "            else:\n",
    "                print(\"Rows differ in some columns:\")\n",
    "                print(group)\n",
    "                \n",
    "                # Optionally, show which columns differ\n",
    "                differing_columns = group.columns[group.nunique() > 1].tolist()\n",
    "                print(f\"Columns with differences: {differing_columns}\")\n",
    "\n",
    "\n",
    "\n",
    "# Usage example\n",
    "# combined = pd.read_csv('your_data.csv')  # Load your data\n",
    "# check_duplicate_rows(combined, 't')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicate_rows(combined,'t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def check_duplicate_rows(df, subset_column):\n",
    "    # Get rows with duplicate values in the subset column\n",
    "    duplicate_mask = df.duplicated(subset=[subset_column], keep=False)\n",
    "    duplicate_rows = df[duplicate_mask]\n",
    "    \n",
    "    # Sort the duplicate rows by the subset column to group them together\n",
    "    duplicate_rows_sorted = duplicate_rows.sort_values(by=subset_column)\n",
    "    \n",
    "    # Iterate through groups of rows with the same value in the subset column\n",
    "    for _, group in duplicate_rows_sorted.groupby(subset_column):\n",
    "        if len(group) > 1:\n",
    "            print(f\"\\nRows with '{subset_column}' value: {group[subset_column].iloc[0]}\")\n",
    "            \n",
    "            # Check if all rows in the group are identical\n",
    "            if group.drop_duplicates().shape[0] == 1:\n",
    "                print(\"All rows are identical:\")\n",
    "                print(group)\n",
    "            else:\n",
    "                print(\"Rows differ in some columns:\")\n",
    "                print(group)\n",
    "                \n",
    "                # Show which columns differ\n",
    "                differing_columns = group.columns[group.nunique() > 1].tolist()\n",
    "                print(f\"Columns with differences: {differing_columns}\")\n",
    "                \n",
    "                # Show the differences\n",
    "                for col in differing_columns:\n",
    "                    if col != subset_column:\n",
    "                        print(f\"\\nDifferences in column '{col}':\")\n",
    "                        print(group[['t', col]])\n",
    "\n",
    "# Usage example\n",
    "# combined = pd.read_csv('your_data.csv')  # Load your data\n",
    "check_duplicate_rows(combined, 't')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.sort_values(by='t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['MinCharSpeedAhA[7]_AhA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = []\n",
    "for i in data:\n",
    "  print(i.columns.is_unique)\n",
    "  col_names = col_names+list(i.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(col_names), len(set(col_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(data,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linf plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_to_plot = {}\n",
    "runs_to_plot[\"t6115_tol8_linf\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_high_tol/t6115_tol8_linf/Lev3_A?/Run/\"\n",
    "\n",
    "data_file_path=\"ConstraintNorms/Linf.dat\"\n",
    "\n",
    "\n",
    "column_names_linf, runs_data_dict_linf = load_data_from_levs(runs_to_plot,data_file_path)\n",
    "print(column_names_linf)\n",
    "print(runs_data_dict_linf.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = runs_data_dict_linf['t6115_tol8_linf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_list = []\n",
    "constraint_list = []\n",
    "component_list = []\n",
    "\n",
    "# domain_list.append(\"SphereB0\")\n",
    "domain_list.append(\"erMA0\")\n",
    "\n",
    "# constraint_list.append('1Con')\n",
    "constraint_list.append('2Con')\n",
    "# constraint_list.append('3Con')\n",
    "\n",
    "component_list.append('t')\n",
    "\n",
    "temp_list = copy.copy(column_names_linf)\n",
    "col_domains = []\n",
    "for col in temp_list:\n",
    "  for domain in domain_list:\n",
    "    if domain in col:\n",
    "      col_domains.append(col)\n",
    "\n",
    "temp_list = col_domains\n",
    "col_domains = []\n",
    "for col in temp_list:\n",
    "  for constraint in constraint_list:\n",
    "    if constraint in col:\n",
    "      col_domains.append(col)\n",
    "\n",
    "if len(component_list) > 0:\n",
    "  temp_list = col_domains\n",
    "  col_domains = []\n",
    "  for col in temp_list:\n",
    "    for component in component_list:\n",
    "      if component in col:\n",
    "        col_domains.append(col)\n",
    "\n",
    "col_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_avg_len = None\n",
    "\n",
    "x = 'time'\n",
    "# moving_avg_len = 50*3\n",
    "\n",
    "\n",
    "for col in col_domains:\n",
    "  if moving_avg_len is not None:\n",
    "    plt.semilogy(np.array(data[x])[moving_avg_len-1:],moving_average_valid(data[col],moving_avg_len),label=col)\n",
    "  else:\n",
    "    plt.semilogy(data[x],data[col],label=col)\n",
    "  # plt.plot(data[x],data[col],label=col)\n",
    "plt.xlabel(x)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(moving_average_valid(data[col],average_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot horizons.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "if \"hchaudha\" in current_dir:\n",
    "    base_path = Path(\"/groups/sxs/hchaudha/spec_runs\")\n",
    "else:\n",
    "    base_path = Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_to_plot = {}\n",
    "runs_to_plot[\"all_100\"] =  \"3_DH_q1_ns_d18_L3_higher_acc/all_100/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "data_dict = load_horizon_data_from_levs(base_path, runs_to_plot)\n",
    "data_dict = flatten_dict(data_dict)\n",
    "data_dict[list(data_dict.keys())[0]].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_avg_len = 0\n",
    "save_path = None\n",
    "\n",
    "x_axis = 't'\n",
    "# y_axis = 'ArealMass'\n",
    "# y_axis = 'ChristodoulouMass'\n",
    "# y_axis = 'CoordCenterInertial_0'\n",
    "# y_axis = 'CoordCenterInertial_1'\n",
    "# y_axis = 'CoordCenterInertial_2'\n",
    "# y_axis = 'DimensionfulInertialSpin_0'\n",
    "# y_axis = 'DimensionfulInertialSpin_1'\n",
    "# y_axis = 'DimensionfulInertialSpin_2'\n",
    "# y_axis = 'DimensionfulInertialCoordSpin_0'\n",
    "# y_axis = 'DimensionfulInertialCoordSpin_1'\n",
    "# y_axis = 'DimensionfulInertialCoordSpin_2'\n",
    "# y_axis = 'DimensionfulInertialSpinMag'\n",
    "# y_axis = 'SpinFromShape_0'\n",
    "# y_axis = 'SpinFromShape_1'\n",
    "# y_axis = 'SpinFromShape_2'\n",
    "# y_axis = 'SpinFromShape_3'\n",
    "# y_axis = 'chiInertial_0'\n",
    "# y_axis = 'chiInertial_1'\n",
    "# y_axis = 'chiInertial_2'\n",
    "# y_axis = 'chiMagInertial'\n",
    "\n",
    "\n",
    "\n",
    "# moving_avg_len=25\n",
    "minT = 2271\n",
    "maxT = 8000\n",
    "\n",
    "plot_fun = lambda x,y,label : plt.plot(x,y,label=label)\n",
    "# plot_fun = lambda x,y,label : plt.semilogy(x,y,label=label)\n",
    "# plot_fun = lambda x,y,label : plt.loglog(x,y,label=label)\n",
    "# plot_fun = lambda x,y,label : plt.scatter(x,y,label=label)\n",
    "# save_path = \"/panfs/ds09/sxs/himanshu/scripts/report/not_tracked/temp2/\"\n",
    "\n",
    "filtered_dict = {}\n",
    "allowed_horizons = [\"AhB\"]\n",
    "for horizons in allowed_horizons:\n",
    "  for runs_keys in data_dict.keys():\n",
    "    if horizons in runs_keys:\n",
    "      filtered_dict[runs_keys] = data_dict[runs_keys]\n",
    " \n",
    "with plt.style.context('default'):\n",
    "  plt.rcParams[\"figure.figsize\"] = (12,10)\n",
    "  plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "  plt.rcParams[\"figure.autolayout\"] = True\n",
    "  plot_graph_for_runs(filtered_dict, x_axis, y_axis, minT, maxT, save_path=save_path, moving_avg_len=moving_avg_len, plot_fun=plot_fun)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bh = 'corrected_coord_spin2_AhB'\n",
    "y_axis1 = 'chiInertial_0'\n",
    "y_axis2 = 'CoordSpinChiInertial_0'\n",
    "\n",
    "X = data_dict[bh][x_axis]\n",
    "Y1 = data_dict[bh][y_axis1]\n",
    "Y2 = data_dict[bh][y_axis2]\n",
    "plt.plot(X,Y1,label=y_axis1)\n",
    "plt.plot(X,Y2,label=y_axis2)\n",
    "plt.xlabel(x_axis)\n",
    "# plt.ylabel(y_axis1+\" - \"+y_axis2)\n",
    "plt.legend()\n",
    "# plt.title()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = 't'\n",
    "y_axis = 'ChristodoulouMass'\n",
    "minT = 500\n",
    "maxT = 800\n",
    "run1 = filtered_dict['AccTest_q1ns_Lev5_AhA']\n",
    "# run1 = filtered_dict['AccTest_q1ns_Lev6p_AhA']\n",
    "run2 = filtered_dict['AccTest_q1ns_Lev6p_AhA']\n",
    "interp_grid_pts = run1[x_axis].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_run1 = CubicSpline(run1[x_axis].to_numpy(),run1[y_axis].to_numpy())\n",
    "interp_run2 = CubicSpline(run2[x_axis].to_numpy(),run2[y_axis].to_numpy())\n",
    "interp_grid = np.arange(minT,maxT,(maxT-minT)/interp_grid_pts)\n",
    "\n",
    "plt.plot(interp_grid, interp_run2(interp_grid) - interp_run1(interp_grid))\n",
    "plt.xlabel(x_axis)\n",
    "plt.ylabel(y_axis)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(interp_grid, interp_run2(interp_grid) - interp_run1(interp_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inertial_dist(run_name:str, data_dict):\n",
    "    ct = data_dict[f\"{run_name}_AhA\"].t\n",
    "    dx = data_dict[f\"{run_name}_AhA\"].CoordCenterInertial_0 - data_dict[f\"{run_name}_AhB\"].CoordCenterInertial_0\n",
    "    dy = data_dict[f\"{run_name}_AhA\"].CoordCenterInertial_1 - data_dict[f\"{run_name}_AhB\"].CoordCenterInertial_1\n",
    "    dz = data_dict[f\"{run_name}_AhA\"].CoordCenterInertial_2 - data_dict[f\"{run_name}_AhB\"].CoordCenterInertial_2\n",
    "\n",
    "    dx = np.sqrt(dx**2 + dy**2 + dz**2)\n",
    "\n",
    "    return ct,dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run_name in runs_to_plot.keys():\n",
    "    ct,dx = inertial_dist(run_name,data_dict)\n",
    "    plt.plot(ct,dx,label=run_name)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_dict.keys())\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(data_dict['76_ngd_master_mr1_50_3000_AhA'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all paraview files into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = Path(\"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/near_bhs_100_obs/\")\n",
    "\n",
    "Lev = 3\n",
    "file_pattern =f\"Lev{Lev}_A[A-Z]/Run/GaugeVis.pvd\"\n",
    "file_patternGrid =f\"Lev{Lev}_A[A-Z]/Run/GaugeVisGrid.pvd\"\n",
    "file_patternAll =f\"Lev{Lev}_A[A-Z]/Run/GuageVisAll.pvd\"\n",
    "\n",
    "combine_pvd_files(base_folder,file_pattern)\n",
    "combine_pvd_files(base_folder,file_patternGrid)\n",
    "combine_pvd_files(base_folder,file_patternAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Create GaugeVis\n",
    "command = f\"cd {base_folder} && mkdir ./GaugeVis\"\n",
    "status = subprocess.run(command, capture_output=True, shell=True, text=True)\n",
    "if status.returncode == 0:\n",
    "  print(f\"Succesfully created GaugeVis in {base_folder}\")\n",
    "else:\n",
    "  sys.exit(\n",
    "      f\"GaugeVis creation failed in {base_folder} with error: \\n {status.stderr}\")\n",
    "\n",
    "# Create GaugeVis subfolder\n",
    "vtu_folder_path = base_folder+\"/GaugeVis/GaugeVis\"\n",
    "command = f\"mkdir {vtu_folder_path}\"\n",
    "status = subprocess.run(command, capture_output=True, shell=True, text=True)\n",
    "if status.returncode == 0:\n",
    "  print(f\"Succesfully created {vtu_folder_path}\")\n",
    "else:\n",
    "  sys.exit(\n",
    "      f\"GaugeVis creation failed as {vtu_folder_path} with error: \\n {status.stderr}\")\n",
    "\n",
    "\n",
    "# Copy vtu files\n",
    "GaugeVisFolder=[]\n",
    "\n",
    "for paths in path_collection:\n",
    "  GaugeVisFolder.append(paths[:-4])\n",
    "\n",
    "for paths in GaugeVisFolder:\n",
    "  command = f\"cp {paths}/*.vtu {vtu_folder_path}/\"\n",
    "  status = subprocess.run(command, capture_output=True, shell=True, text=True)\n",
    "  if status.returncode == 0:\n",
    "    print(f\"Succesfully copied vtu files from {paths}\")\n",
    "  else:\n",
    "    sys.exit(\n",
    "        f\"Copying vtu files from {paths} failed with error: \\n {status.stderr}\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiler results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make report (do not run randomly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./report_new_gauge.json\") as report_data:\n",
    "  data = json.load(report_data)\n",
    "\n",
    "os.mkdir(data['report_folder'])\n",
    "\n",
    "subfolders = []\n",
    "for folders in data['runs_to_track']:\n",
    "  subfolders_path = data['report_folder'] + \"/\" + path_to_folder_name(folders) + \"/\"\n",
    "  print(subfolders_path)\n",
    "  os.mkdir(subfolders_path)\n",
    "  subfolders.append(subfolders_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_still_going_on = True\n",
    "while runs_still_going_on:\n",
    "  # time.sleep(data['report_generation_frequency'])\n",
    "\n",
    "  for i,run_folder_path in enumerate(data['runs_to_track']):\n",
    "    # if is_the_current_run_going_on(run_folder_path) or True:\n",
    "    if True:\n",
    "      plots_for_a_folder(data['things_to_plot'],subfolders[i],run_folder_path)\n",
    "    print(run_folder_path)\n",
    "\n",
    "\n",
    "  runs_still_going_on = False\n",
    "  print(\"all done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all columns and data files paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write all the cols in the dat files for reference\n",
    "lev_golb=\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/gauge_driver_kerr_target_50_50_0_16_16_01/Ev/Lev1_AA\"\n",
    "dat_files_glob=lev_golb+\"/Run/**/**.dat\"\n",
    "path_pattern = dat_files_glob\n",
    "\n",
    "path_collection = []\n",
    "for folder_name in glob.iglob(path_pattern, recursive=True):\n",
    "    if os.path.isdir(folder_name) or os.path.isfile(folder_name):\n",
    "        path_collection.append(folder_name)\n",
    "        print(folder_name.split(\"/\")[-1])\n",
    "\n",
    "\n",
    "column_data_for_dat_files = {\n",
    "  'columns_of_dat_files' : [\n",
    "  ] \n",
    "}\n",
    "\n",
    "for file_path in path_collection:\n",
    "  file_name = file_path.split(\"/\")[-1]\n",
    "  columns_list =  list(read_dat_file(file_path).columns)\n",
    "  column_data_for_dat_files['columns_of_dat_files'].append({\n",
    "    'file_name': file_name,\n",
    "    'file_path': file_path,\n",
    "    'columns': columns_list\n",
    "  })\n",
    "\n",
    "\n",
    "with open('./column_data_for_dat_files.json', 'w') as outfile:\n",
    "  json.dump(column_data_for_dat_files, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JoinH5(h5_file_list, output_path, output_file_name):\n",
    "\n",
    "  file_list_to_str = \"\"\n",
    "  for h5file in h5_file_list:\n",
    "    file_list_to_str += h5file + \" \"\n",
    "\n",
    "  command = f\"cd {output_path} && {spec_home}/Support/bin/JoinH5 -o {output_file_name} {file_list_to_str}\"\n",
    "  status = subprocess.run(command, capture_output=True, shell=True, text=True)\n",
    "  if status.returncode == 0:\n",
    "    print(f\"Succesfully ran JoinH5 in {output_path}\")\n",
    "  else:\n",
    "    sys.exit(\n",
    "        f\"JoinH5 failed in {output_path} with error: \\n {status.stderr}\")\n",
    "\n",
    "\n",
    "def ExtractFromH5(h5_file, output_path):\n",
    "\n",
    "  command = f\"cd {output_path} && {spec_home}/Support/bin/ExtractFromH5 {h5_file}\"\n",
    "  status = subprocess.run(command, capture_output=True, shell=True, text=True)\n",
    "  if status.returncode == 0:\n",
    "    print(f\"Succesfully ran ExtractFromH5 in {output_path}\")\n",
    "  else:\n",
    "    sys.exit(\n",
    "        f\"ExtractFromH5 failed in {output_path} with error: \\n {status.stderr}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_base_path= \"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/profiler_results\"\n",
    "\n",
    "\n",
    "base_folder = \"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/49_ngd_weird_gauge_mr1\"\n",
    "file_pattern = base_folder+\"/Ev/Lev1_A?/Run/Profiler.h5\"\n",
    "\n",
    "path_pattern = file_pattern\n",
    "path_collection = []\n",
    "\n",
    "# make a folder in the output directory\n",
    "save_folder = output_base_path+\"/\"+base_folder.split(\"/\")[-1]\n",
    "os.mkdir(save_folder)\n",
    "\n",
    "\n",
    "# Find all the files that match the required pattern of the file\n",
    "for folder_name in glob.iglob(path_pattern, recursive=True):\n",
    "    if os.path.isdir(folder_name) or os.path.isfile(folder_name):\n",
    "        path_collection.append(folder_name)\n",
    "        print(folder_name)\n",
    "\n",
    "JoinH5(path_collection,save_folder,\"Profiler.h5\")\n",
    "ExtractFromH5(\"Profiler.h5\",save_folder)\n",
    "\n",
    "# Save path of all the summary files in extracted data\n",
    "\n",
    "file_pattern = base_folder+\"/Ev/Lev1_A?/Run/Profiler.h5\"\n",
    "\n",
    "path_pattern = file_pattern\n",
    "path_collection = []\n",
    "\n",
    "# Find all the files that match the required pattern of the file\n",
    "for folder_name in glob.iglob(path_pattern, recursive=True):\n",
    "    if os.path.isdir(folder_name) or os.path.isfile(folder_name):\n",
    "        path_collection.append(folder_name)\n",
    "        print(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the Summary files \n",
    "summary_file_pattern = save_folder+\"/**/Summary.txt\"\n",
    "summary_file_collection = []\n",
    "\n",
    "for file_path in glob.iglob(summary_file_pattern, recursive=True):\n",
    "    if os.path.isdir(file_path) or os.path.isfile(file_path):\n",
    "        summary_file_collection.append(file_path)\n",
    "        print(file_path)\n",
    "\n",
    "summary_file_collection.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/profiler_results/49_ngd_weird_gauge_mr1/extracted-Profiler/Step10522.dir/Summary.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AmrTolerances.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lev=8\n",
    "TruncationErrorMax = 0.000216536 * 4**(-Lev)\n",
    "ProjectedConstraintsMax = 0.216536 * 4**(-Lev)\n",
    "TruncationErrorMaxA = TruncationErrorMax*1.e-4\n",
    "TruncationErrorMaxB = TruncationErrorMax*1.e-4\n",
    "\n",
    "AhMaxRes  = TruncationErrorMax\n",
    "AhMinRes  = AhMaxRes / 10.0\n",
    "\n",
    "AhMaxTrunc=TruncationErrorMax\n",
    "AhMinTrunc=AhMaxTrunc / 100.0\n",
    "\n",
    "print(f\"AhMinRes={AhMinRes};\")\n",
    "print(f\"AhMaxRes={AhMaxRes};\")\n",
    "print(f\"AhMinTrunc={AhMinTrunc};\")\n",
    "print(f\"AhMaxTrunc={AhMaxTrunc};\")\n",
    "print(f\"TruncationErrorMax={TruncationErrorMax};\")\n",
    "print(f\"TruncationErrorMaxA={TruncationErrorMaxA};\")\n",
    "print(f\"TruncationErrorMaxB={TruncationErrorMaxB};\")\n",
    "print(f\"ProjectedConstraintsMax={ProjectedConstraintsMax};\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(100)/100\n",
    "y1 = np.sin(x)\n",
    "y2 = np.cos(x)\n",
    "\n",
    "styles =  plt.style.available\n",
    "\n",
    "for style in styles:\n",
    "    print(style)\n",
    "    plt.style.use(style)\n",
    "    plt.plot(x,y1,label=\"y1asfasd\")\n",
    "    plt.plot(x,y2,label=\"y3asfasd\")\n",
    "    plt.title(\"asdf\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/make_report_del/{style}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 4\n",
    "print(np.convolve(np.arange(1,10),np.ones(w),'valid')/w)\n",
    "print(np.arange(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(array,avg_len):\n",
    "    return np.convolve(array,np.ones(avg_len))/avg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss(x,cen,scale):\n",
    "  return np.exp(-(x-cen)**2/scale**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = 18\n",
    "q = 1\n",
    "d1 = 1/(1+q)*d0\n",
    "d2 = q/(1+q)*d0\n",
    "cen1 = d1\n",
    "cen2 = -d2\n",
    "scale1 = d2\n",
    "scale2 = d1\n",
    "\n",
    "tol1 = 3.38337e-10\n",
    "tol2 = 3.38337e-10\n",
    "tol_base = 3.38337e-6\n",
    "\n",
    "d = 100\n",
    "x_min,x_max = -d/2,d/2\n",
    "X = np.linspace(x_min,x_max,1000)\n",
    "\n",
    "fac_base = np.ones_like(X)\n",
    "fac1 = gauss(X,cen1,scale1)\n",
    "fac2 = gauss(X,cen2,scale2)\n",
    "\n",
    "fac_total = fac_base+fac1+fac2\n",
    "val = fac_base*np.log10(tol_base) + fac1*np.log10(tol1)+fac2*np.log10(tol2)\n",
    "val = 10**(val/fac_total)\n",
    "print(val.min(),val.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.semilogy\n",
    "f(X,val,label='tot')\n",
    "# f(X,fac1*tol1,label='tol1')\n",
    "# f(X,fac2*tol2,label='tol2')\n",
    "# f(X,fac_base*tol_base,label='tol_base')\n",
    "hor_lines = [val.min(),val[int(len(val)/2)],val.max()]\n",
    "plt.hlines(y=hor_lines, xmin=-d/2, xmax=d/2, colors=['r', 'g', 'b'], linestyles='--', linewidth=2)\n",
    "for y_value in hor_lines:\n",
    "    plt.text(0, y_value*1.1, f'{y_value:.3e}', va='center', ha='left')\n",
    "\n",
    "\n",
    "plt.ylabel('AMR tolerance')\n",
    "plt.xlabel('x_axis')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X,gauss(X,0,50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_files_to_dataframe(file_path):\n",
    "  # Function to parse a single line and return a dictionary of values\n",
    "  def parse_line(line):\n",
    "      data = {}\n",
    "      # Find all variable=value pairs\n",
    "      pairs = re.findall(r'([^;=\\s]+)=\\s*([^;]+)', line)\n",
    "      for var, val in pairs:\n",
    "          # Hist-GrDomain.txt should be parsed a little differently\n",
    "          if 'ResizeTheseSubdomains' in var:\n",
    "              items = val.split('),')\n",
    "              items[-1] = items[-1][:-1]\n",
    "              for item in items:\n",
    "                name,_,vals = item.split(\"(\")\n",
    "                r,l,m=vals[:-1].split(',')\n",
    "                data[f\"{name}_R\"] = int(r)\n",
    "                data[f\"{name}_L\"] = int(l)\n",
    "                data[f\"{name}_M\"] = int(m)\n",
    "          else:\n",
    "              data[var] = float(val) if re.match(r'^[\\d.e+-]+$', val) else val\n",
    "      return data\n",
    "  \n",
    "  with open(file_path, 'r') as file:\n",
    "    # Parse the lines\n",
    "    data = []\n",
    "    for line in file.readlines():\n",
    "        data.append(parse_line(line.strip()))\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "  return df\n",
    "\n",
    "hist_files_to_dataframe('/groups/sxs/hchaudha/spec_runs/2_SpKS_q1_sA_0_0_9_sB_0_0_9_d15/Ev/Lev3_AB/Run/Hist-GrDomain.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_files_to_dataframe('/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_etl_tol_10/Lev3_AD/Run_old/Hist-GrDomain.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_files_to_dataframe('/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_etl_tol_10/Lev3_AD/Run_old/Hist-GrDomain.txt')\n",
    "# hist_files_to_dataframe('/groups/sxs/hchaudha/spec_runs/2_SpKS_q1_sA_0_0_9_sB_0_0_9_d15/Ev/Lev3_AB/Run/Hist-FuncLambdaFactorB.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to parse a single line and return a dictionary of values\n",
    "def parse_line(line):\n",
    "    data = {}\n",
    "    # Find all variable=value pairs\n",
    "    pairs = re.findall(r'([^;=]+)=\\s*([\\d.e+-]+)', line)\n",
    "    for var, val in pairs:\n",
    "        data[var] = float(val) if re.match(r'[\\d.e+-]+', val) else val\n",
    "    return data\n",
    "\n",
    "# Read the file\n",
    "with open('/groups/sxs/hchaudha/spec_runs/2_SpKS_q1_sA_0_0_9_sB_0_0_9_d15/Ev/Lev3_AB/Run/Hist-FuncLambdaFactorB.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Parse the lines\n",
    "data = []\n",
    "for line in lines:\n",
    "    data.append(parse_line(line.strip()))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# The input string\n",
    "data = \"\"\"SphereA0(Extents=(12,22,44)),SphereA1(Extents=(10,23,46)),SphereA2(Extents=(10,24,48)),SphereA3(Extents=(10,25,50)),SphereA4(Extents=(13,26,52)),SphereB0(Extents=(12,22,44)),SphereB1(Extents=(10,23,46)),SphereB2(Extents=(10,24,48)),SphereB3(Extents=(10,25,50)),SphereB4(Extents=(13,26,52)),SphereC0(Extents=(15,15,30)),SphereC1(Extents=(15,15,30)),SphereC2(Extents=(15,14,28)),SphereC3(Extents=(15,15,30)),SphereC4(Extents=(15,15,30)),SphereC5(Extents=(15,15,30)),SphereC6(Extents=(15,15,30)),SphereC7(Extents=(15,16,32)),SphereC8(Extents=(15,15,30)),SphereC9(Extents=(15,16,32)),SphereC10(Extents=(15,16,32)),SphereC11(Extents=(15,15,30)),SphereC12(Extents=(15,15,30)),SphereC13(Extents=(15,15,30)),SphereC14(Extents=(15,15,30)),SphereC15(Extents=(15,15,30)),SphereC16(Extents=(16,15,30)),SphereC17(Extents=(16,16,32)),SphereC18(Extents=(16,16,32)),SphereC19(Extents=(16,16,32)),SphereC20(Extents=(15,16,32)),SphereC21(Extents=(15,16,32)),SphereC22(Extents=(15,16,32)),SphereC23(Extents=(15,16,32)),SphereC24(Extents=(15,15,30)),SphereC25(Extents=(15,16,32)),SphereC26(Extents=(15,16,32)),SphereC27(Extents=(15,16,32)),SphereC28(Extents=(15,16,32)),SphereC29(Extents=(15,16,32)),CylinderEB0.0.0(Extents=(13,31,19)),CylinderEB1.0.0(Extents=(17,25,18)),CylinderCB0.0.0(Extents=(17,23,17)),CylinderCB1.0.0(Extents=(14,21,15)),CylinderEA0.0.0(Extents=(13,31,19)),CylinderEA1.0.0(Extents=(14,25,18)),CylinderCA0.0.0(Extents=(17,23,18)),CylinderCA1.0.0(Extents=(14,21,15)),FilledCylinderEB0(Extents=(12,11,25)),FilledCylinderEB1(Extents=(12,10,25)),FilledCylinderCB0(Extents=(12,9,21)),FilledCylinderCB1(Extents=(12,8,19)),FilledCylinderMB0(Extents=(14,11,25)),FilledCylinderMB1(Extents=(16,10,21)),CylinderSMB0.0(Extents=(14,27,15)),CylinderSMB1.0(Extents=(18,25,15)),FilledCylinderEA0(Extents=(12,11,25)),FilledCylinderEA1(Extents=(12,10,25)),FilledCylinderCA0(Extents=(12,9,21)),FilledCylinderCA1(Extents=(12,8,19)),FilledCylinderMA0(Extents=(14,11,25)),FilledCylinderMA1(Extents=(14,10,21)),CylinderSMA0.0(Extents=(14,27,15)),CylinderSMA1.0(Extents=(15,25,15))\"\"\"\n",
    "\n",
    "# Split the string into individual items\n",
    "items = data.split('),')\n",
    "\n",
    "# # Function to parse each item\n",
    "# def parse_item(item):\n",
    "#     name, values = re.match(r'(\\w+)\\(Extents=\\((.*?)\\)', item).groups()\n",
    "#     r, l, m = map(int, values.split(','))\n",
    "#     return {'Name': name, 'R': r, 'L': l, 'M': m}\n",
    "\n",
    "# # Parse all items\n",
    "# parsed_data = [parse_item(item) for item in items]\n",
    "\n",
    "# # Create DataFrame\n",
    "# df = pd.DataFrame(parsed_data)\n",
    "\n",
    "# # Set 'Name' as index\n",
    "# df.set_index('Name', inplace=True)\n",
    "\n",
    "# # Create the specific variables for SphereA2\n",
    "# SphereA2_R = df.loc['SphereA2', 'R']\n",
    "# SphereA2_L = df.loc['SphereA2', 'L']\n",
    "# SphereA2_M = df.loc['SphereA2', 'M']\n",
    "\n",
    "# print(df)\n",
    "# print(f\"\\nSphereA2_R = {SphereA2_R}\")\n",
    "# print(f\"SphereA2_L = {SphereA2_L}\")\n",
    "# print(f\"SphereA2_M = {SphereA2_M}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = data.split('),')\n",
    "name,_,vals = items[0].split(\"(\")\n",
    "r,l,m=vals[:-1].split(',')\n",
    "{\n",
    "  name+\"_R\":r,\n",
    "  name+\"_L\":l,\n",
    "  name+\"_M\":m\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[0].split(\"(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals[:-1].split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = Path(\"/groups/sxs/hchaudha/spec_runs\")\n",
    "del_path = Path(\"/groups/sxs/hchaudha/spec_runs/del.sh\")\n",
    "\n",
    "with del_path.open('w') as f:\n",
    "  for i in folder_path.iterdir():\n",
    "    if i.is_dir():\n",
    "      if \"ID\" in str(i):\n",
    "        continue\n",
    "      if \"del\" in str(i):\n",
    "        continue\n",
    "      f.writelines(str(i)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dir(folder_path:Path,del_path_opened):\n",
    "  for i in folder_path.iterdir():\n",
    "    if i.is_dir():\n",
    "      print(i)\n",
    "      if \"ID\" in str(i):\n",
    "        continue\n",
    "      if \"del\" in str(i):\n",
    "        continue\n",
    "      if \"Lev\" in str(i) and i.is_symlink():\n",
    "        continue\n",
    "      if \"Run\" == i.name:\n",
    "        del_path_opened.writelines(str(i)+\"\\n\")\n",
    "        return\n",
    "      write_dir(i,del_path_opened)\n",
    "\n",
    "folder_path = Path(\"/groups/sxs/hchaudha/spec_runs\")\n",
    "# folder_path = Path(\"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_all_100_t2690/all_100_t2690_eteq_tol_8\")\n",
    "del_path = Path(\"/groups/sxs/hchaudha/spec_runs/del.sh\")\n",
    "with del_path.open('w') as f:\n",
    "  write_dir(folder_path, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path.name == \"asd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true,
  "vscode": {
   "interpreter": {
    "hash": "009adc1c8ee1f76b2251d0bb13ed6e10d4fef5bd0a6f7d195d9f2892e5880fe6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
