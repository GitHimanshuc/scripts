{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import h5py\n",
    "import sys\n",
    "from numba import njit\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict\n",
    "import os\n",
    "import glob\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams[\"figure.figsize\"] = (12,10)\n",
    "import json\n",
    "import time\n",
    "import matplotlib\n",
    "from pathlib import Path\n",
    "from scipy.interpolate import CubicSpline\n",
    "spec_home=\"/home/himanshu/spec/my_spec\"\n",
    "matplotlib.matplotlib_fname()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Various functions to read across levs\n",
    "### Also functions to make reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dat_file(file_name):\n",
    "  cols_names = []\n",
    "\n",
    "  temp_file = \"./temp.csv\"\n",
    "  with open(file_name,'r') as f:\n",
    "    with open(temp_file,'w') as w:\n",
    "      lines = f.readlines()\n",
    "      for line in lines:\n",
    "        if(line[0] != '#'): # This is data\n",
    "          w.writelines(line)\n",
    "        if(line[0:3] == '# [' or line[0:4] == '#  ['): # Some dat files have comments on the top\n",
    "          cols_names.append(line.split('=')[-1][1:-1].strip())\n",
    "\n",
    "\n",
    "  return pd.read_csv(temp_file,delim_whitespace=True,names=cols_names)\n",
    "\n",
    "# Files like AhACoefs.dat have unequal number of columns\n",
    "def read_dat_file_uneq_cols(file_name):\n",
    "  cols_names = []\n",
    "\n",
    "  temp_file = \"./temp.csv\"\n",
    "  col_length = 0\n",
    "  with open(file_name,'r') as f:\n",
    "    with open(temp_file,'w') as w:\n",
    "      lines = f.readlines()\n",
    "      for line in lines:\n",
    "        if(line[0] != '#'): # This is data\n",
    "          w.writelines(\" \".join(line.split()[:col_length])+\"\\n\")\n",
    "        if(line[0:3] == '# [' or line[0:4] == '#  ['): # Some dat files have comments on the top\n",
    "          cols_names.append(line.split('=')[-1][1:-1].strip())\n",
    "          col_length = col_length+1\n",
    "\n",
    "\n",
    "  return pd.read_csv(temp_file,delim_whitespace=True,names=cols_names)\n",
    "\n",
    "def plot_and_save(data,x_arr,y_arr,save_folder,file_name):\n",
    "  for x_axis,y_axis in zip(x_arr,y_arr):\n",
    "    plt.plot(data[x_axis],data[y_axis])\n",
    "    plt.xlabel(x_axis)\n",
    "    plt.ylabel(y_axis)\n",
    "    title = file_name[:-4]+\" \\\"\"+y_axis+\"\\\" vs \\\"\"+x_axis+\"\\\"\"\n",
    "    plt.title(title)\n",
    "    save_path = save_folder+title.replace(\"/\",\"_\")\n",
    "    print(f\"Plotted: {save_path}\" )\n",
    "    print(os.getcwd())\n",
    "\n",
    "    plt.savefig(save_path)\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "\n",
    "def read_dat_file_across_AA(file_pattern):\n",
    "\n",
    "  path_pattern = file_pattern\n",
    "  path_collection = []\n",
    "\n",
    "\n",
    "  for folder_name in glob.iglob(path_pattern, recursive=True):\n",
    "      if os.path.isdir(folder_name) or os.path.isfile(folder_name):\n",
    "          path_collection.append(folder_name)\n",
    "          print(folder_name)\n",
    "\n",
    "\n",
    "  read_data_collection = []\n",
    "  for path in path_collection:\n",
    "    # AhACoefs.dat has uneq cols\n",
    "    if \"Coefs.dat\" in path:\n",
    "        read_data_collection.append(read_dat_file_uneq_cols(path))\n",
    "    else:\n",
    "        read_data_collection.append(read_dat_file(path))\n",
    "\n",
    "  data = pd.concat(read_data_collection)\n",
    "  # print(data.columns)\n",
    "  return data\n",
    "\n",
    "def read_AH_files(Ev_path):\n",
    "  fileA = Ev_path + \"Run/ApparentHorizons/AhA.dat\"\n",
    "  fileB = Ev_path + \"Run/ApparentHorizons/AhB.dat\"\n",
    "\n",
    "  dataA = read_dat_file_across_AA(fileA)\n",
    "  dataB = read_dat_file_across_AA(fileB)\n",
    "\n",
    "  return dataA,dataB  \n",
    "\n",
    "  \n",
    "# Combines all the pvd files into a single file and save it in the base folder\n",
    "def combine_pvd_files(base_folder:Path, file_pattern:str, output_path=None):\n",
    "  pvd_start =\"\"\"<?xml version=\"1.0\"?>\\n<VTKFile type=\"Collection\" version=\"0.1\" byte_order=\"LittleEndian\">\\n  <Collection>\\n\"\"\"\n",
    "  pvd_end =\"  </Collection>\\n</VTKFile>\"\n",
    "\n",
    "  vis_folder_name = file_pattern.split(\"/\")[-1][:-4]\n",
    "  Lev = file_pattern[0:4]\n",
    "\n",
    "  if output_path is None:\n",
    "    output_path = f\"{base_folder}/{vis_folder_name}_{Lev}.pvd\"\n",
    "\n",
    "  pvd_files = list(base_folder.glob(file_pattern))\n",
    "  pvd_folders = list(base_folder.glob(file_pattern[:-4]))\n",
    "\n",
    "\n",
    "  with open(output_path,'w') as write_file:\n",
    "    write_file.writelines(pvd_start)\n",
    "    for files in pvd_files:\n",
    "      print(files)\n",
    "      with files.open(\"r\") as f:\n",
    "        for line in f.readlines():\n",
    "          line = line.replace(vis_folder_name,str(files)[:-4])\n",
    "          if \"DataSet\" in line:\n",
    "            write_file.writelines(line)\n",
    "    write_file.writelines(pvd_end)\n",
    "  \n",
    "  print(output_path)\n",
    "\n",
    "def moving_average(array,avg_len):\n",
    "    return np.convolve(array,np.ones(avg_len))/avg_len\n",
    "    \n",
    "def moving_average_valid(array,avg_len):\n",
    "    return np.convolve(array,np.ones(avg_len),'valid')/avg_len\n",
    "\n",
    "\n",
    "def path_to_folder_name(folder_name):\n",
    "  return folder_name.replace(\"/\",\"_\")\n",
    "\n",
    "# Give a dict of {\"run_name\" = runs_path} and data_file_path to get {\"run_name\" = dat_file_data}\n",
    "def load_data_from_levs(runs_path, data_file_path):\n",
    "  data_dict = {}\n",
    "  column_list = \"\"\n",
    "  for run_name in runs_path.keys():\n",
    "    data_dict[run_name] = read_dat_file_across_AA(runs_path[run_name]+data_file_path)\n",
    "    column_list = data_dict[run_name].columns\n",
    "  return column_list, data_dict\n",
    "\n",
    "\n",
    "def plot_graph_for_runs(runs_data_dict, x_axis, y_axis, minT, maxT, legend_dict=None, save_path=None, moving_avg_len=0, plot_fun = lambda x,y,label : plt.plot(x,y,label=label),sort_by=None):\n",
    "  sort_run_data_dict(runs_data_dict,sort_by=sort_by)\n",
    "  minT_indx_list={}\n",
    "  maxT_indx_list={}\n",
    "\n",
    "  if legend_dict is None:\n",
    "    legend_dict = {}\n",
    "    for run_name in runs_data_dict.keys():\n",
    "      legend_dict[run_name] = None\n",
    "  else:\n",
    "    for run_name in runs_data_dict.keys():\n",
    "      if run_name not in legend_dict:\n",
    "        raise ValueError(f\"{run_name} not in {legend_dict=}\")\n",
    "\n",
    "  \n",
    "  for run_name in runs_data_dict.keys():\n",
    "    minT_indx_list[run_name] = len(runs_data_dict[run_name][x_axis][runs_data_dict[run_name][x_axis] < minT])\n",
    "    maxT_indx_list[run_name] = len(runs_data_dict[run_name][x_axis][runs_data_dict[run_name][x_axis] < maxT])\n",
    "\n",
    "  if moving_avg_len == 0:\n",
    "\n",
    "    for run_name in runs_data_dict.keys():\n",
    "      x_data = runs_data_dict[run_name][x_axis][minT_indx_list[run_name]:maxT_indx_list[run_name]]\n",
    "      y_data = runs_data_dict[run_name][y_axis][minT_indx_list[run_name]:maxT_indx_list[run_name]]\n",
    "      \n",
    "\n",
    "    #   print(f\"{len(x_data)=},{len(y_data)=},{len(np.argsort(x_data))=},{type(x_data)=}\")\n",
    "\n",
    "    #   sorted_indices = x_data.argsort()\n",
    "    #   x_data = x_data.iloc[sorted_indices]\n",
    "    #   y_data = y_data.iloc[sorted_indices]\n",
    "      legend = legend_dict[run_name]\n",
    "      if legend is None:\n",
    "        legend = run_name\n",
    "      plot_fun(x_data, y_data,legend)\n",
    "\n",
    "    plt.xlabel(x_axis)\n",
    "    plt.ylabel(y_axis)\n",
    "    title = \"\\\"\" +  y_axis+\"\\\" vs \\\"\"+x_axis+\"\\\"\"\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "  else:\n",
    "    for run_name in runs_data_dict.keys():\n",
    "      x_data = np.array(runs_data_dict[run_name][x_axis][minT_indx_list[run_name] + moving_avg_len-1:maxT_indx_list[run_name]])\n",
    "      y_data = np.array(moving_average_valid(runs_data_dict[run_name][y_axis][minT_indx_list[run_name]:maxT_indx_list[run_name]], moving_avg_len))\n",
    "\n",
    "    #   sorted_indices = np.argsort(x_data)\n",
    "    #   x_data = x_data[sorted_indices]\n",
    "    #   y_data = y_data[sorted_indices]\n",
    "      legend = legend_dict[run_name]\n",
    "      if legend is None:\n",
    "        legend = run_name\n",
    "      plot_fun(x_data, y_data,legend)\n",
    "\n",
    "    plt.xlabel(x_axis)\n",
    "    plt.ylabel(y_axis)\n",
    "    title = \"\\\"\" + y_axis+ \"\\\" vs \\\"\" + x_axis + \"\\\"  \" + f\"avg_window_len={moving_avg_len}\"\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "  \n",
    "  if save_path is not None:\n",
    "    fig_x_label = x_axis.replace(\"/\",\"_\").replace(\".\",\"_\")\n",
    "    fig_y_label = y_axis.replace(\"/\",\"_\").replace(\".\",\"_\")\n",
    "    save_file_name = f\"{fig_y_label}_vs_{fig_x_label}_minT={minT}_maxT={maxT}_moving_avg_len={moving_avg_len}\"\n",
    "    for run_name in runs_data_dict.keys():\n",
    "      save_file_name = save_file_name + \"__\" + run_name\n",
    "\n",
    "    plt.savefig(save_path+save_file_name)\n",
    "\n",
    "\n",
    "def find_file(pattern):\n",
    "  return glob.glob(pattern, recursive=True)[0]\n",
    "\n",
    "def plots_for_a_folder(things_to_plot,plot_folder_path,data_folder_path):\n",
    "  for plot_info in things_to_plot:\n",
    "    file_name = plot_info['file_name']\n",
    "    y_arr = plot_info['columns'][1:]\n",
    "    x_arr = [plot_info['columns'][0]]*len(y_arr)\n",
    "\n",
    "    data = read_dat_file_across_AA(data_folder_path+\"/**/\"+file_name)\n",
    "    plot_and_save(data,x_arr,y_arr,plot_folder_path,file_name)\n",
    "\n",
    "def is_the_current_run_going_on(run_folder):\n",
    "  if len(find_file(run_folder+\"/**/\"+\"TerminationReason.txt\")) > 0:\n",
    "    return False\n",
    "  else:\n",
    "    return True\n",
    "\n",
    "def plot_min_grid_spacing(runs_data_dict):\n",
    "    '''\n",
    "    runs_data_dict should have dataframes with MinimumGridSpacing.dat data.\n",
    "    The function will compute the min grid spacing along all domains and plot it.\n",
    "    '''\n",
    "    keys = runs_data_dict.keys()\n",
    "    if len(keys) == 0:\n",
    "        print(\"There are no dataframes in the dict\")\n",
    "\n",
    "    for key in keys:\n",
    "        t_step = runs_data_dict[key]['t']\n",
    "        min_val = runs_data_dict[key].drop(columns=['t']).min(axis='columns')\n",
    "        plt.plot(t_step,min_val,label=key)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"Min Grid Spacing\")\n",
    "    plt.title(\"Min grid spacing in all domains\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_GrAdjustSubChunksToDampingTimes(runs_data_dict):\n",
    "    keys = runs_data_dict.keys()\n",
    "    if len(keys) > 1:\n",
    "        print(\"To plot the Tdamp for various quantities only put one dataframe in the runs_data_dict\")\n",
    "\n",
    "    data:pd.DataFrame = runs_data_dict[list(keys)[0]]\n",
    "    tdamp_keys = []\n",
    "    for key in data.keys():\n",
    "        if 'Tdamp' in key:\n",
    "            tdamp_keys.append(key)\n",
    "\n",
    "    # Get a colormap\n",
    "    cmap = plt.get_cmap('tab10')\n",
    "    colors = cmap(np.linspace(0, 1, len(tdamp_keys)))\n",
    "\n",
    "    t_vals = data['time']\n",
    "    for i, color, key in zip(range(len(tdamp_keys)),colors, tdamp_keys):\n",
    "        if i%2==0:\n",
    "            plt.plot(t_vals,data[key],label=key,color=color)\n",
    "        else:\n",
    "            plt.plot(t_vals,data[key],label=key,color=color,linestyle=\"--\")\n",
    "\n",
    "\n",
    "    min_tdamp = data[tdamp_keys].min(axis='columns')\n",
    "    plt.plot(t_vals,min_tdamp,label=\"min_tdamp\",linewidth=3,linestyle=\"dotted\",color=\"red\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"time\")\n",
    "    plt.title(list(keys)[0])\n",
    "    plt.show()\n",
    "\n",
    "def add_max_and_min_val(runs_data_dict):\n",
    "    # If we load a file with 5 columns with first being time, then find max and min values for all the other columns, at all times and add it to the dataframe.\n",
    "    # Useful when you want to find like Linf across all domains at all times\n",
    "    for run_name in runs_data_dict.keys():\n",
    "        data_frame = runs_data_dict[run_name]\n",
    "        t = data_frame.iloc[:,0]\n",
    "        max_val = np.zeros_like(t)\n",
    "        min_val = np.zeros_like(t)\n",
    "        for i in range(len(t)):\n",
    "            max_val[i] = data_frame.iloc[i,1:].max()\n",
    "            min_val[i] = data_frame.iloc[i,1:].max()\n",
    "\n",
    "        # Add the values to the dataframe\n",
    "        data_frame['max_val'] = max_val\n",
    "        data_frame['min_val'] = min_val\n",
    "\n",
    "def sort_run_data_dict(runs_data_dict:dict,sort_by=None):\n",
    "    for run_name in runs_data_dict.keys():\n",
    "        run_df = runs_data_dict[run_name]\n",
    "        if sort_by is None:\n",
    "            sort_by = run_df.keys()[0]\n",
    "        runs_data_dict[run_name] = run_df.sort_values(by=sort_by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to read horizon files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_Bh_pandas(h5_dir):\n",
    "    # Empty dataframe\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # List of all the vars in the h5 file\n",
    "    var_list = []\n",
    "    h5_dir.visit(var_list.append)\n",
    "    \n",
    "    \n",
    "    for var in var_list:\n",
    "        # This means there is no time column\n",
    "        # print(f\"{var} : {h5_dir[var].shape}\")\n",
    "        if df.shape == (0,0):\n",
    "            # data[:,0] is time and then we have the data\n",
    "            data = h5_dir[var]\n",
    "            \n",
    "            # vars[:-4] to remove the .dat at the end\n",
    "            col_names = make_col_names(var[:-4],data.shape[1]-1)\n",
    "            col_names.append('t')\n",
    "            # Reverse the list so that we get [\"t\",\"var_name\"]\n",
    "            col_names.reverse()            \n",
    "            append_to_df(data[:],col_names,df)\n",
    "            \n",
    "        else:\n",
    "            data = h5_dir[var]\n",
    "            col_names = make_col_names(var[:-4],data.shape[1]-1)         \n",
    "            append_to_df(data[:,1:],col_names,df)\n",
    "            \n",
    "    return df\n",
    "\n",
    "def append_to_df(data,col_names,df):\n",
    "    for i,col_name in enumerate(col_names):\n",
    "        df[col_name] = data[:,i]\n",
    "        \n",
    "def make_col_names(val_name:str,val_size:int):\n",
    "    col_names = []\n",
    "    if val_size == 1:\n",
    "        col_names.append(val_name)\n",
    "    else:\n",
    "        for i in range(val_size):\n",
    "            col_names.append(val_name+f\"_{i}\")\n",
    "    return col_names\n",
    "\n",
    "\n",
    "def horizon_to_pandas(horizon_path:Path):\n",
    "    assert(horizon_path.exists())\n",
    "    df_dict = {}\n",
    "    with h5py.File(horizon_path,'r') as hf:\n",
    "        # Not all horizon files may have AhC\n",
    "        for key in hf.keys():\n",
    "            if key == 'VersionHist.ver':\n",
    "                # Newer runs have this\n",
    "                continue\n",
    "            df_dict[key[:-4]] = make_Bh_pandas(hf[key])\n",
    "\n",
    "    return df_dict\n",
    "\n",
    "def read_horizon_across_Levs(path_list:List[Path]):\n",
    "    df_listAB = []\n",
    "    df_listC = []\n",
    "    final_dict = {}\n",
    "    for path in path_list:\n",
    "        df_lev = horizon_to_pandas(path)\n",
    "        # Either [AhA,AhB] or [AhA,AhB,AhC]\n",
    "        if len(df_lev.keys()) > 1:\n",
    "            df_listAB.append(df_lev)\n",
    "        # Either [AhC] or [AhA,AhB,AhC]\n",
    "        if (len(df_lev.keys()) == 1) or (len(df_lev.keys()) ==3):\n",
    "            df_listC.append(df_lev)\n",
    "    if len(df_listAB)==1:\n",
    "        # There was only one lev\n",
    "        final_dict = df_listAB[0]\n",
    "    else:\n",
    "        final_dict[\"AhA\"] = pd.concat([df[\"AhA\"] for df in df_listAB])\n",
    "        final_dict[\"AhB\"] = pd.concat([df[\"AhB\"] for df in df_listAB])\n",
    "        if len(df_listC) > 0:\n",
    "            final_dict[\"AhC\"] = pd.concat([df[\"AhC\"] for df in df_listC])       \n",
    "    \n",
    "    return final_dict\n",
    "\n",
    "def load_horizon_data_from_levs(base_path:Path, runs_path:Dict[str,Path]):\n",
    "  data_dict = {}\n",
    "  for run_name in runs_path.keys():\n",
    "    path_list = list(base_path.glob(runs_path[run_name]))\n",
    "    print(path_list)\n",
    "    data_dict[run_name] = read_horizon_across_Levs(path_list)\n",
    "  return data_dict\n",
    "\n",
    "def flatten_dict(horizon_data_dict:Dict[str,pd.DataFrame]) -> Dict[str,pd.DataFrame] :\n",
    "  flattened_data = {}\n",
    "  for run_name in horizon_data_dict.keys():\n",
    "      for horizons in horizon_data_dict[run_name]:\n",
    "          flattened_data[run_name+\"_\"+horizons] = horizon_data_dict[run_name][horizons]\n",
    "          # print(run_name+\"_\"+horizons)\n",
    "  return flattened_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot dat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_to_plot = {}\n",
    "# runs_to_plot[\"boost_ID_test_wrong\"] =  \"/groups/sxs/hchaudha/spec_runs/boost_ID_test_wrong/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"boost_ID_test_correct\"] =  \"/groups/sxs/hchaudha/spec_runs/boost_ID_test_correct/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"corrected_coord_spin1\"] =  \"/groups/sxs/hchaudha/spec_runs/corrected_coord_spin1/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"corrected_coord_spin2\"] =  \"/groups/sxs/hchaudha/spec_runs/corrected_coord_spin2/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"2_SpKS_q1_sA_0_0_0_sB_0_0_0_d15\"] =  \"/groups/sxs/hchaudha/spec_runs/2_SpKS_q1_sA_0_0_0_sB_0_0_0_d15/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"2_SpKS_q1_sA_0_0_0_sB_0_0_99_d15\"] =  \"/groups/sxs/hchaudha/spec_runs/2_SpKS_q1_sA_0_0_0_sB_0_0_99_d15/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"2_SpKS_q1_sA_0_0_0_sB_0_0_9_d15\"] =  \"/groups/sxs/hchaudha/spec_runs/2_SpKS_q1_sA_0_0_0_sB_0_0_9_d15/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"2_SpKS_q1_sA_0_0_9_sB_0_0_9_d15\"] =  \"/groups/sxs/hchaudha/spec_runs/2_SpKS_q1_sA_0_0_9_sB_0_0_9_d15/Ev/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"3_DH_q1_ns_d18_L3\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3/Ev/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"3_DH_q1_ns_d18_L6\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L6/Ev/Lev6_A?/Run/\"\n",
    "runs_to_plot[\"all_10\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_10/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"all_100\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/all_100/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"near_bhs_10\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/near_bhs_10/Lev3_A?/Run/\"\n",
    "runs_to_plot[\"near_bhs_100\"] =  \"/central/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L3_higher_acc/near_bhs_100/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"3_DH_q1_ns_d18_L6_AA\"] =  \"/groups/sxs/hchaudha/spec_runs/3_DH_q1_ns_d18_L6_AA/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"4_SphKS_q1_15_SSphKS_ID\"] =  \"/groups/sxs/hchaudha/spec_runs/4_SphKS_q1_15_SSphKS_ID/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"4_SphKS_q1_15_SKS_ID\"] =  \"/groups/sxs/hchaudha/spec_runs/4_SphKS_q1_15_SKS_ID/Ev/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"5_gd_SphKS_gauge_ID\"] =  \"/groups/sxs/hchaudha/spec_runs/5_gd_SphKS_gauge_ID/Ev/Lev2_A?/Run/\"\n",
    "# runs_to_plot[\"5_ngd_SphKS_ID\"] =  \"/groups/sxs/hchaudha/spec_runs/5_ngd_SphKS_ID/Ev/Lev2_A?/Run/\"\n",
    "# runs_to_plot[\"5_ngd_KS_ID\"] =  \"/groups/sxs/hchaudha/spec_runs/5_ngd_KS_ID/Ev/Lev2_A?/Run/\"\n",
    "\n",
    "\n",
    "\n",
    "# data_file_path=\"ConstraintNorms/GhCe_Norms.dat\"\n",
    "data_file_path=\"ConstraintNorms/GhCe.dat\"\n",
    "# data_file_path=\"ConstraintNorms/GhCe_Linf.dat\"\n",
    "# data_file_path=\"ConstraintNorms/GhCe_L2.dat\"\n",
    "# data_file_path=\"ConstraintNorms/GhCe_VolL2.dat\"\n",
    "# data_file_path=\"ConstraintNorms/NormalizedGhCe_Norms.dat\"\n",
    "# data_file_path=\"ConstraintNorms/NormalizedGhCe_Linf.dat\"\n",
    "# data_file_path=\"CharSpeedNorms/CharSpeeds_Min_SliceLFF.SphereA0.dat\"\n",
    "# data_file_path=\"MinimumGridSpacing.dat\"\n",
    "# data_file_path=\"GrAdjustMaxTstepToDampingTimes.dat\"\n",
    "# data_file_path=\"GrAdjustSubChunksToDampingTimes.dat\"\n",
    "# data_file_path=\"DiagAhSpeedA.dat\"\n",
    "# data_file_path=\"ApparentHorizons/AhA.dat\"\n",
    "# data_file_path=\"ApparentHorizons/AhB.dat\"\n",
    "# data_file_path=\"ApparentHorizons/MinCharSpeedAhA.dat\"\n",
    "# data_file_path=\"ApparentHorizons/RescaledRadAhA.dat\"\n",
    "# data_file_path=\"ApparentHorizons/AhACoefs.dat\"\n",
    "# data_file_path=\"ApparentHorizons/Trajectory_AhB.dat\"\n",
    "# data_file_path=\"ApparentHorizons/HorizonSepMeasures.dat\"\n",
    "# data_file_path=\"TStepperDiag.dat\"\n",
    "# data_file_path=\"TimeInfo.dat\"\n",
    "\n",
    "\n",
    "column_names, runs_data_dict = load_data_from_levs(runs_to_plot,data_file_path)\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_avg_len=0\n",
    "save_path = None\n",
    "# add_max_and_min_val(runs_data_dict)\n",
    "# y_axis = 'max_val'\n",
    "# y_axis = 'min_val'\n",
    "x_axis = 'time'\n",
    "# y_axis = 'Shape_NumberOfPiledUpModes'\n",
    "y_axis = 'Linf(GhCe)'\n",
    "y_axis = 'L2(GhCe)'\n",
    "y_axis = 'VolLp(GhCe)' \n",
    "# y_axis = 'VolLp(sqrt(1Con^2))'\n",
    "# y_axis = 'VolLp(GhCe) on SphereA0'\n",
    "# y_axis = 'Linf(GhCe) on SphereB0'\n",
    "# y_axis = 'Linf(GhCe) on SphereA0'\n",
    "# y_axis = 'Linf(GhCe) on CylinderSMB0.0'\n",
    "# y_axis = 'U0'\n",
    "# y_axis = 'VolLp(sqrt(1Con^2))'\n",
    "# y_axis = 'VolLp(sqrt(2Con^2))'\n",
    "# y_axis = 'Linf(NormalizedGhCe)'\n",
    "# y_axis = 'MaxAllowedTstep'\n",
    "# y_axis = 'Tdamp(SmoothMinDeltaRNoLam00AhB)'\n",
    "# y_axis = 'Tdamp(LambdaFactorA0)'\n",
    "\n",
    "# x_axis = 't'\n",
    "# y_axis = 'T [hours]'\n",
    "# y_axis = 'dt/dT'\n",
    "\n",
    "# x_axis = 't'\n",
    "# y_axis = 'CoordSepHorizons'\n",
    "# y_axis = 'ProperSepHorizons'\n",
    "# y_axis = 'MinCharSpeedAhA[1]'\n",
    "# y_axis = 'MinimumGridSpacing[SphereB0]'\n",
    "# y_axis = 'MinimumGridSpacing[CylinderSMB0.0]'\n",
    "# y_axis = 'dt/dT'\n",
    "\n",
    "\n",
    "# x_axis = 'time'\n",
    "# y_axis = 'L_surface'\n",
    "# y_axis = 'Area'\n",
    "# y_axis = 'NumIterations'\n",
    "# y_axis = 'max(R_ij)'\n",
    "# y_axis = 'Coef(1,-1)'\n",
    "# y_axis = 'Shape_NumberOfPiledUpModes'\n",
    "# y_axis = 'max(r)'\n",
    "# y_axis = 'min(r)'\n",
    "# y_axis = 'max(|r^i-c^i|)'\n",
    "\n",
    "\n",
    "# y_axis = 'courant factor'\n",
    "# y_axis = 'error/1e-08'\n",
    "# y_axis = 'NfailedSteps'\n",
    "# y_axis = 'NumRhsEvaluations in this segment'\n",
    "\n",
    "# x_axis = 'time after step'\n",
    "\n",
    "# y_axis = 'dt'\n",
    "# moving_avg_len=50\n",
    "\n",
    "minT = 0\n",
    "# minT = 2000\n",
    "maxT = 40000\n",
    "# maxT = 4000\n",
    "\n",
    "plot_fun = lambda x,y,label : plt.plot(x,y,label=label)\n",
    "plot_fun = lambda x,y,label : plt.semilogy(x,y,label=label)\n",
    "# plot_fun = lambda x,y,label : plt.loglog(x,y,label=label) \n",
    "# plot_fun = lambda x,y,label : plt.scatter(x,y,label=label,s=10,marker=\"x\")\n",
    "# save_path = \"/groups/sxs/hchaudha/rough/high_acc_plots/\"\n",
    "# save_path = \"/groups/sxs/hchaudha/rough/plots/\"\n",
    "legend_dict = {}\n",
    "for key in runs_data_dict.keys():\n",
    "  legend_dict[key] = None\n",
    "\n",
    "# legend_dict = { '3_DH_q1_ns_d18_L3': \"Lev3\",\n",
    "#                 '3_DH_q1_ns_d18_L6': \"Lev6\",\n",
    "#                 'all_10': \"Lev3_all_tols_10\",\n",
    "#                 'all_100': \"Lev3_all_tols_100\",\n",
    "#                 'near_bhs_10': \"Lev3_sphere_AB_tols_10\",\n",
    "#                 'near_bhs_100': \"Lev3_sphere_AB_tols_100\"}\n",
    "\n",
    "with plt.style.context('default'):\n",
    "  plt.rcParams[\"figure.figsize\"] = (12,10)\n",
    "  plt.rcParams[\"figure.autolayout\"] = True\n",
    "  plot_graph_for_runs(runs_data_dict, x_axis, y_axis, minT, maxT, legend_dict=legend_dict, save_path=save_path, moving_avg_len=moving_avg_len, plot_fun=plot_fun)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path=\"ConstraintNorms/GhCe_L2.dat\"\n",
    "data_file_path=\"ConstraintNorms/GhCe_Linf.dat\"\n",
    "data_file_path=\"ConstraintNorms/GhCe_VolL2.dat\"\n",
    "column_names, runs_data_dict = load_data_from_levs(runs_to_plot,data_file_path)\n",
    "# print(column_names)\n",
    "print(runs_data_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_name = list(runs_data_dict.keys())[4]\n",
    "run_name = 'near_bhs_100'\n",
    "df = runs_data_dict[run_name].copy()\n",
    "df = df.sort_values(by=df.columns[0])\n",
    "\n",
    "tmin=2050\n",
    "# tmin=0\n",
    "tmax=4000\n",
    "# tmax=50000\n",
    "\n",
    "df = df[df['time']>=tmin]\n",
    "df = df[df['time']<tmax]\n",
    "t_name = df.columns[0]\n",
    "y_axis = df.columns[1].split(\" \")[0]\n",
    "all_cols_but_t = df.columns[1:]\n",
    "# Find the maximum value across columns B, C, D, and F for each row\n",
    "df['max_val'] = df[all_cols_but_t].max(axis=1)\n",
    "\n",
    "# Determine which column had the maximum value\n",
    "df['max_source'] = df[all_cols_but_t].idxmax(axis=1)\n",
    "\n",
    "# List all columns that have at least one max value\n",
    "columns_with_max = df['max_source'].unique()\n",
    "\n",
    "# Generate a colormap for the columns with at least one max value\n",
    "num_colors = len(columns_with_max)\n",
    "colors = plt.get_cmap('tab20', num_colors)  # Using 'tab20' colormap\n",
    "color_map = {column: colors(i) for i, column in enumerate(columns_with_max)}\n",
    "\n",
    "# Plot max_BCD vs t with different colors for different sources\n",
    "plt.figure(figsize=(18, 10))\n",
    "for i,source in enumerate(columns_with_max):\n",
    "    subset = df[df['max_source'] == source]\n",
    "    if i%4 == 0:\n",
    "        plt.scatter(subset[t_name], subset['max_val'], color=color_map[source], label=source, s=10, marker=\"^\")\n",
    "    if i%4 == 1:\n",
    "        plt.scatter(subset[t_name], subset['max_val'], color=color_map[source], label=source, s=10, marker=\"v\")\n",
    "    if i%4 == 2:\n",
    "        plt.scatter(subset[t_name], subset['max_val'], color=color_map[source], label=source, s=10, marker=\">\")\n",
    "    if i%4 == 3:\n",
    "        plt.scatter(subset[t_name], subset['max_val'], color=color_map[source], label=source, s=10, marker=\"<\")\n",
    "\n",
    "plt.xlabel(t_name)\n",
    "plt.ylabel(y_axis)\n",
    "plt.yscale('log')\n",
    "plt.title(f'{y_axis} vs {t_name} for {run_name}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"/groups/sxs/hchaudha/rough/{run_name}_{y_axis}_vs_{t_name}_{tmin}_{tmax}.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_file_path=\"MinimumGridSpacing.dat\"\n",
    "column_names, runs_data_dict = load_data_from_levs(runs_to_plot,data_file_path)\n",
    "sort_run_data_dict(runs_data_dict)\n",
    "plot_min_grid_spacing(runs_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path=\"GrAdjustSubChunksToDampingTimes.dat\"\n",
    "column_names, runs_data_dict = load_data_from_levs(runs_to_plot,data_file_path)\n",
    "sort_run_data_dict(runs_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(runs_data_dict.keys())\n",
    "idx = 0\n",
    "runs_data_dict_temp = {f\"{list(runs_data_dict.keys())[idx]}\":runs_data_dict[list(runs_data_dict.keys())[idx]]}\n",
    "plot_GrAdjustSubChunksToDampingTimes(runs_data_dict_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot horizons.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "if \"hchaudha\" in current_dir:\n",
    "    base_path = Path(\"/groups/sxs/hchaudha/spec_runs\")\n",
    "else:\n",
    "    base_path = Path(\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_to_plot = {}\n",
    "# runs_to_plot[\"76_ngd_master_mr1_50_3000\"] =  \"76_ngd_master_mr1_50_3000/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"76_ngd_master_mr1_200_3000\"] =  \"76_ngd_master_mr1_200_3000/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"77_gd_Kerr_q1\"] =  \"77_gd_Kerr_q1/Ev/Lev1_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"77_gd_Kerr_q3\"] =  \"77_gd_Kerr_q3/Ev/Lev1_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"77_gd_Kerr_q1_Kerr\"] =  \"77_gd_Kerr_q1/Ev_Kerr/Lev1_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"77_gd_Kerr_q3_Kerr\"] =  \"77_gd_Kerr_q3/Ev_Kerr/Lev1_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"78_ngd_master_mr1\"] =  \"78_ngd_master_mr1/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"79_ngd_master_mr1_1000_3000\"] =  \"79_ngd_master_mr1_1000_3000/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"79_ngd_master_mr1_1000_3000_high_tol\"] =  \"79_ngd_master_mr1_1000_3000/Ev_AH_high_tol/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"79_ngd_master_mr1_200_3000\"] =  \"79_ngd_master_mr1_200_3000/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"80_ngd_master_mr1_100\"] =  \"80_ngd_master_mr1_100/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"80_ngd_master_mr1_50\"] =  \"80_ngd_master_mr1_50/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"80_ngd_master_mr1_10\"] =  \"80_ngd_master_mr1_10/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"80_ngd_master_mr1_5\"] =  \"80_ngd_master_mr1_5/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"80_ngd_master_mr1_300\"] =  \"80_ngd_master_mr1_300/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"81_gd_Kerr_q3_0_9_0__0_0_0\"] =  \"81_gd_Kerr_q3_0_9_0__0_0_0/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"81_gd_DH_q3_0_9_0__0_0_0\"] =  \"81_gd_DH_q3_0_9_0__0_0_0/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"82_ngd_master_mr1_50_3000_DH_to_DH\"] =  \"82_ngd_master_mr1_50_3000_DH_to_DH/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"83_ngd_master_mr1_200_3000_no_eps\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"83_ngd_master_mr1_200_3000_no_eps_high_tol\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev_AH_high_tol/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"83_ngd_master_mr1_200_3000_no_eps_high_tol_all\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev_high_tol_all/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"83_ngd_master_mr1_200_3000_no_eps_high_tol_all_100\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev_high_tol_all_100/Lev3_A?/Run/\"\n",
    "# runs_to_plot[\"83_ngd_master_mr1_200_3000_no_eps_high_tol\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"83_ngd_master_mr1_200_3000_no_eps_no_lsr\"] =  \"83_ngd_master_mr1_200_3000_no_eps_no_lsr/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"84_gd_KerrI_3000_200\"] =  \"84_gd_KerrI_3000_200/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"84_gd_DH_3000_200\"] =  \"84_gd_DH_3000_200/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"83_no_eps_Ev_wrong_evolution\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev_wrong_evolution/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"83_no_eps_Ev_pow2\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev_pow2/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"83_no_eps_Ev_pow6\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev_pow6/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"83_no_eps_Ev_tanh15\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev_tanh15/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"83_no_eps_Ev_tanh7\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev_tanh7/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"83_no_eps_Ev_tanh7_lsr_correct_evolution\"] =  \"83_ngd_master_mr1_200_3000_no_eps/Ev_tanh7_lsr_correct_evolution/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"110_KerrToDHTanh_1\"] =  \"110_KerrToDHTanh_1/Ev/Lev1_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"110_KerrToDHTanh_3\"] =  \"110_KerrToDHTanh_3/Ev/Lev1_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"113_KerrToDHTanh_st1_scP01\"] =  \"113_KerrToDHTanh_st1_scP01/Ev/Lev1_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"113_KerrToDHTanh_st1_scP05\"] =  \"113_KerrToDHTanh_st1_scP05/Ev/Lev1_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"113_KerrToDHTanh_st1_scP1\"] =  \"113_KerrToDHTanh_st1_scP1/Ev/Lev1_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"113_KerrToDHTanh_st1_scP5\"] =  \"113_KerrToDHTanh_st1_scP5/Ev/Lev1_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"ID_SUKS2_1_0,0,0_0,0,0_10_gd_main\"] =  \"ID_SUKS2_1_0,0,0_0,0,0_10_gd_main/Ev/Lev1_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "\n",
    "# runs_to_plot[\"67_master_mr1\"] =  \"67_master_mr1/Ev/Lev1_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"master_mr1_Lev1\"] =  \"master_mr1/Ev/Lev1_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"master_mr1_Lev3\"] =  \"master_mr1/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"AccTest_q1ns_Lev3\"] =  \"AccTest_q1ns_Lev3/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"AccTest_q1ns_Lev5\"] =  \"AccTest_q1ns_Lev5/Ev/Lev5_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"AccTest_q1ns_Lev7\"] =  \"AccTest_q1ns_Lev7/Ev/Lev7_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"AccTest_q1ns_Lev9\"] =  \"AccTest_q1ns_Lev9/Ev/Lev9_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"AccTest_q1ns_Lev6p\"] =  \"AccTest_q1ns_Lev5/Ev_Lev6/Lev5_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"AccTest_q1ns_Lev7p\"] =  \"AccTest_q1ns_Lev5/Ev_Lev7/Lev5_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"AccTest_q1ns_Lev8p\"] =  \"AccTest_q1ns_Lev5/Ev_Lev8/Lev5_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "\n",
    "# runs_to_plot[\"HQ_3_gd_SUKS_L5_5_15\"] =  \"HQ_3_gd_SUKS_L5_5_15/Ev/Lev5_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"HQ_3_gd_SKS_L5_5_15\"] =  \"HQ_3_gd_SKS_L5_5_15/Ev/Lev5_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"HQ_3_ngd_DH_L5_5_15\"] =  \"HQ_3_ngd_DH_L5_5_15/Ev/Lev5_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "\n",
    "runs_to_plot[\"2_SpKS_q1_sA_0_0_0_sB_0_0_0_d15\"] =  \"2_SpKS_q1_sA_0_0_0_sB_0_0_0_d15/Ev/Lev3_AA/Run/ApparentHorizons/Horizons.h5\"\n",
    "runs_to_plot[\"2_SpKS_q1_sA_0_0_0_sB_0_0_99_d15\"] =  \"2_SpKS_q1_sA_0_0_0_sB_0_0_99_d15/Ev/Lev3_AA/Run/ApparentHorizons/Horizons.h5\"\n",
    "runs_to_plot[\"2_SpKS_q1_sA_0_0_0_sB_0_0_9_d15\"] =  \"2_SpKS_q1_sA_0_0_0_sB_0_0_9_d15/Ev/Lev3_AA/Run/ApparentHorizons/Horizons.h5\"\n",
    "runs_to_plot[\"2_SpKS_q1_sA_0_0_9_sB_0_0_9_d15\"] =  \"2_SpKS_q1_sA_0_0_9_sB_0_0_9_d15/Ev/Lev3_AA/Run/ApparentHorizons/Horizons.h5\"\n",
    "\n",
    "# runs_to_plot[\"corrected_coord_spin2\"] =  \"corrected_coord_spin2/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"boost_ID_test_wrong\"] =  \"boost_ID_test_wrong/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"boost_ID_test_correct\"] = \"boost_ID_test_correct/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "# runs_to_plot[\"corrected_coord_spin1\"] =  \"corrected_coord_spin1/Ev/Lev3_A?/Run/ApparentHorizons/Horizons.h5\"\n",
    "\n",
    "data_dict = load_horizon_data_from_levs(base_path, runs_to_plot)\n",
    "data_dict = flatten_dict(data_dict)\n",
    "data_dict[list(data_dict.keys())[0]].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_avg_len = 0\n",
    "save_path = None\n",
    "\n",
    "# x_axis = 't'\n",
    "# y_axis = 'ArealMass'\n",
    "y_axis = 'ChristodoulouMass'\n",
    "# y_axis = 'CoordCenterInertial_0'\n",
    "# y_axis = 'CoordCenterInertial_1'\n",
    "# y_axis = 'CoordCenterInertial_2'\n",
    "# y_axis = 'DimensionfulInertialSpin_0'\n",
    "# y_axis = 'DimensionfulInertialSpin_1'\n",
    "# y_axis = 'DimensionfulInertialSpin_2'\n",
    "# y_axis = 'DimensionfulInertialCoordSpin_0'\n",
    "# y_axis = 'DimensionfulInertialCoordSpin_1'\n",
    "# y_axis = 'DimensionfulInertialCoordSpin_2'\n",
    "# y_axis = 'DimensionfulInertialSpinMag'\n",
    "# y_axis = 'SpinFromShape_0'\n",
    "# y_axis = 'SpinFromShape_1'\n",
    "# y_axis = 'SpinFromShape_2'\n",
    "# y_axis = 'SpinFromShape_3'\n",
    "# y_axis = 'chiInertial_0'\n",
    "# y_axis = 'chiInertial_1'\n",
    "# y_axis = 'chiInertial_2'\n",
    "# y_axis = 'chiMagInertial'\n",
    "\n",
    "\n",
    "\n",
    "# moving_avg_len=25\n",
    "minT = 0\n",
    "maxT = 800\n",
    "\n",
    "plot_fun = lambda x,y,label : plt.plot(x,y,label=label)\n",
    "# plot_fun = lambda x,y,label : plt.semilogy(x,y,label=label)\n",
    "# plot_fun = lambda x,y,label : plt.loglog(x,y,label=label)\n",
    "# plot_fun = lambda x,y,label : plt.scatter(x,y,label=label)\n",
    "# save_path = \"/panfs/ds09/sxs/himanshu/scripts/report/not_tracked/temp2/\"\n",
    "\n",
    "filtered_dict = {}\n",
    "allowed_horizons = [\"AhB\"]\n",
    "for horizons in allowed_horizons:\n",
    "  for runs_keys in data_dict.keys():\n",
    "    if horizons in runs_keys:\n",
    "      filtered_dict[runs_keys] = data_dict[runs_keys]\n",
    " \n",
    "with plt.style.context('default'):\n",
    "  plt.rcParams[\"figure.figsize\"] = (12,10)\n",
    "  plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "  plt.rcParams[\"figure.autolayout\"] = True\n",
    "  plot_graph_for_runs(filtered_dict, x_axis, y_axis, minT, maxT, save_path=save_path, moving_avg_len=moving_avg_len, plot_fun=plot_fun)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bh = 'corrected_coord_spin2_AhB'\n",
    "y_axis1 = 'chiInertial_0'\n",
    "y_axis2 = 'CoordSpinChiInertial_0'\n",
    "\n",
    "X = data_dict[bh][x_axis]\n",
    "Y1 = data_dict[bh][y_axis1]\n",
    "Y2 = data_dict[bh][y_axis2]\n",
    "plt.plot(X,Y1,label=y_axis1)\n",
    "plt.plot(X,Y2,label=y_axis2)\n",
    "plt.xlabel(x_axis)\n",
    "# plt.ylabel(y_axis1+\" - \"+y_axis2)\n",
    "plt.legend()\n",
    "# plt.title()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = 't'\n",
    "y_axis = 'ChristodoulouMass'\n",
    "minT = 500\n",
    "maxT = 800\n",
    "run1 = filtered_dict['AccTest_q1ns_Lev5_AhA']\n",
    "# run1 = filtered_dict['AccTest_q1ns_Lev6p_AhA']\n",
    "run2 = filtered_dict['AccTest_q1ns_Lev6p_AhA']\n",
    "interp_grid_pts = run1[x_axis].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_run1 = CubicSpline(run1[x_axis].to_numpy(),run1[y_axis].to_numpy())\n",
    "interp_run2 = CubicSpline(run2[x_axis].to_numpy(),run2[y_axis].to_numpy())\n",
    "interp_grid = np.arange(minT,maxT,(maxT-minT)/interp_grid_pts)\n",
    "\n",
    "plt.plot(interp_grid, interp_run2(interp_grid) - interp_run1(interp_grid))\n",
    "plt.xlabel(x_axis)\n",
    "plt.ylabel(y_axis)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(interp_grid, interp_run2(interp_grid) - interp_run1(interp_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inertial_dist(run_name:str, data_dict):\n",
    "    ct = data_dict[f\"{run_name}_AhA\"].t\n",
    "    dx = data_dict[f\"{run_name}_AhA\"].CoordCenterInertial_0 - data_dict[f\"{run_name}_AhB\"].CoordCenterInertial_0\n",
    "    dy = data_dict[f\"{run_name}_AhA\"].CoordCenterInertial_1 - data_dict[f\"{run_name}_AhB\"].CoordCenterInertial_1\n",
    "    dz = data_dict[f\"{run_name}_AhA\"].CoordCenterInertial_2 - data_dict[f\"{run_name}_AhB\"].CoordCenterInertial_2\n",
    "\n",
    "    dx = np.sqrt(dx**2 + dy**2 + dz**2)\n",
    "\n",
    "    return ct,dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run_name in runs_to_plot.keys():\n",
    "    ct,dx = inertial_dist(run_name,data_dict)\n",
    "    plt.plot(ct,dx,label=run_name)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_dict.keys())\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(data_dict['76_ngd_master_mr1_50_3000_AhA'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all paraview files into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = Path(\"/central/groups/sxs/hchaudha/spec_runs/5_gd_SphKS_gauge_ID/Ev/\")\n",
    "\n",
    "file_pattern =\"Lev2_A[A-Z]/Run/GaugeVis.pvd\"\n",
    "file_patternGrid =\"Lev2_A[A-Z]/Run/GaugeVisGrid.pvd\"\n",
    "file_patternAll =\"Lev2_A[A-Z]/Run/GuageVisAll.pvd\"\n",
    "\n",
    "combine_pvd_files(base_folder,file_pattern)\n",
    "combine_pvd_files(base_folder,file_patternGrid)\n",
    "combine_pvd_files(base_folder,file_patternAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Create GaugeVis\n",
    "command = f\"cd {base_folder} && mkdir ./GaugeVis\"\n",
    "status = subprocess.run(command, capture_output=True, shell=True, text=True)\n",
    "if status.returncode == 0:\n",
    "  print(f\"Succesfully created GaugeVis in {base_folder}\")\n",
    "else:\n",
    "  sys.exit(\n",
    "      f\"GaugeVis creation failed in {base_folder} with error: \\n {status.stderr}\")\n",
    "\n",
    "# Create GaugeVis subfolder\n",
    "vtu_folder_path = base_folder+\"/GaugeVis/GaugeVis\"\n",
    "command = f\"mkdir {vtu_folder_path}\"\n",
    "status = subprocess.run(command, capture_output=True, shell=True, text=True)\n",
    "if status.returncode == 0:\n",
    "  print(f\"Succesfully created {vtu_folder_path}\")\n",
    "else:\n",
    "  sys.exit(\n",
    "      f\"GaugeVis creation failed as {vtu_folder_path} with error: \\n {status.stderr}\")\n",
    "\n",
    "\n",
    "# Copy vtu files\n",
    "GaugeVisFolder=[]\n",
    "\n",
    "for paths in path_collection:\n",
    "  GaugeVisFolder.append(paths[:-4])\n",
    "\n",
    "for paths in GaugeVisFolder:\n",
    "  command = f\"cp {paths}/*.vtu {vtu_folder_path}/\"\n",
    "  status = subprocess.run(command, capture_output=True, shell=True, text=True)\n",
    "  if status.returncode == 0:\n",
    "    print(f\"Succesfully copied vtu files from {paths}\")\n",
    "  else:\n",
    "    sys.exit(\n",
    "        f\"Copying vtu files from {paths} failed with error: \\n {status.stderr}\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiler results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make report (do not run randomly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./report_new_gauge.json\") as report_data:\n",
    "  data = json.load(report_data)\n",
    "\n",
    "os.mkdir(data['report_folder'])\n",
    "\n",
    "subfolders = []\n",
    "for folders in data['runs_to_track']:\n",
    "  subfolders_path = data['report_folder'] + \"/\" + path_to_folder_name(folders) + \"/\"\n",
    "  print(subfolders_path)\n",
    "  os.mkdir(subfolders_path)\n",
    "  subfolders.append(subfolders_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_still_going_on = True\n",
    "while runs_still_going_on:\n",
    "  # time.sleep(data['report_generation_frequency'])\n",
    "\n",
    "  for i,run_folder_path in enumerate(data['runs_to_track']):\n",
    "    # if is_the_current_run_going_on(run_folder_path) or True:\n",
    "    if True:\n",
    "      plots_for_a_folder(data['things_to_plot'],subfolders[i],run_folder_path)\n",
    "    print(run_folder_path)\n",
    "\n",
    "\n",
    "  runs_still_going_on = False\n",
    "  print(\"all done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all columns and data files paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write all the cols in the dat files for reference\n",
    "lev_golb=\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/gauge_driver_kerr_target_50_50_0_16_16_01/Ev/Lev1_AA\"\n",
    "dat_files_glob=lev_golb+\"/Run/**/**.dat\"\n",
    "path_pattern = dat_files_glob\n",
    "\n",
    "path_collection = []\n",
    "for folder_name in glob.iglob(path_pattern, recursive=True):\n",
    "    if os.path.isdir(folder_name) or os.path.isfile(folder_name):\n",
    "        path_collection.append(folder_name)\n",
    "        print(folder_name.split(\"/\")[-1])\n",
    "\n",
    "\n",
    "column_data_for_dat_files = {\n",
    "  'columns_of_dat_files' : [\n",
    "  ] \n",
    "}\n",
    "\n",
    "for file_path in path_collection:\n",
    "  file_name = file_path.split(\"/\")[-1]\n",
    "  columns_list =  list(read_dat_file(file_path).columns)\n",
    "  column_data_for_dat_files['columns_of_dat_files'].append({\n",
    "    'file_name': file_name,\n",
    "    'file_path': file_path,\n",
    "    'columns': columns_list\n",
    "  })\n",
    "\n",
    "\n",
    "with open('./column_data_for_dat_files.json', 'w') as outfile:\n",
    "  json.dump(column_data_for_dat_files, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JoinH5(h5_file_list, output_path, output_file_name):\n",
    "\n",
    "  file_list_to_str = \"\"\n",
    "  for h5file in h5_file_list:\n",
    "    file_list_to_str += h5file + \" \"\n",
    "\n",
    "  command = f\"cd {output_path} && {spec_home}/Support/bin/JoinH5 -o {output_file_name} {file_list_to_str}\"\n",
    "  status = subprocess.run(command, capture_output=True, shell=True, text=True)\n",
    "  if status.returncode == 0:\n",
    "    print(f\"Succesfully ran JoinH5 in {output_path}\")\n",
    "  else:\n",
    "    sys.exit(\n",
    "        f\"JoinH5 failed in {output_path} with error: \\n {status.stderr}\")\n",
    "\n",
    "\n",
    "def ExtractFromH5(h5_file, output_path):\n",
    "\n",
    "  command = f\"cd {output_path} && {spec_home}/Support/bin/ExtractFromH5 {h5_file}\"\n",
    "  status = subprocess.run(command, capture_output=True, shell=True, text=True)\n",
    "  if status.returncode == 0:\n",
    "    print(f\"Succesfully ran ExtractFromH5 in {output_path}\")\n",
    "  else:\n",
    "    sys.exit(\n",
    "        f\"ExtractFromH5 failed in {output_path} with error: \\n {status.stderr}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_base_path= \"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/profiler_results\"\n",
    "\n",
    "\n",
    "base_folder = \"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/49_ngd_weird_gauge_mr1\"\n",
    "file_pattern = base_folder+\"/Ev/Lev1_A?/Run/Profiler.h5\"\n",
    "\n",
    "path_pattern = file_pattern\n",
    "path_collection = []\n",
    "\n",
    "# make a folder in the output directory\n",
    "save_folder = output_base_path+\"/\"+base_folder.split(\"/\")[-1]\n",
    "os.mkdir(save_folder)\n",
    "\n",
    "\n",
    "# Find all the files that match the required pattern of the file\n",
    "for folder_name in glob.iglob(path_pattern, recursive=True):\n",
    "    if os.path.isdir(folder_name) or os.path.isfile(folder_name):\n",
    "        path_collection.append(folder_name)\n",
    "        print(folder_name)\n",
    "\n",
    "JoinH5(path_collection,save_folder,\"Profiler.h5\")\n",
    "ExtractFromH5(\"Profiler.h5\",save_folder)\n",
    "\n",
    "# Save path of all the summary files in extracted data\n",
    "\n",
    "file_pattern = base_folder+\"/Ev/Lev1_A?/Run/Profiler.h5\"\n",
    "\n",
    "path_pattern = file_pattern\n",
    "path_collection = []\n",
    "\n",
    "# Find all the files that match the required pattern of the file\n",
    "for folder_name in glob.iglob(path_pattern, recursive=True):\n",
    "    if os.path.isdir(folder_name) or os.path.isfile(folder_name):\n",
    "        path_collection.append(folder_name)\n",
    "        print(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the Summary files \n",
    "summary_file_pattern = save_folder+\"/**/Summary.txt\"\n",
    "summary_file_collection = []\n",
    "\n",
    "for file_path in glob.iglob(summary_file_pattern, recursive=True):\n",
    "    if os.path.isdir(file_path) or os.path.isfile(file_path):\n",
    "        summary_file_collection.append(file_path)\n",
    "        print(file_path)\n",
    "\n",
    "summary_file_collection.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/profiler_results/49_ngd_weird_gauge_mr1/extracted-Profiler/Step10522.dir/Summary.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AmrTolerances.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lev=8\n",
    "TruncationErrorMax = 0.000216536 * 4**(-Lev)\n",
    "ProjectedConstraintsMax = 0.216536 * 4**(-Lev)\n",
    "TruncationErrorMaxA = TruncationErrorMax*1.e-4\n",
    "TruncationErrorMaxB = TruncationErrorMax*1.e-4\n",
    "\n",
    "AhMaxRes  = TruncationErrorMax\n",
    "AhMinRes  = AhMaxRes / 10.0\n",
    "\n",
    "AhMaxTrunc=TruncationErrorMax\n",
    "AhMinTrunc=AhMaxTrunc / 100.0\n",
    "\n",
    "print(f\"AhMinRes={AhMinRes};\")\n",
    "print(f\"AhMaxRes={AhMaxRes};\")\n",
    "print(f\"AhMinTrunc={AhMinTrunc};\")\n",
    "print(f\"AhMaxTrunc={AhMaxTrunc};\")\n",
    "print(f\"TruncationErrorMax={TruncationErrorMax};\")\n",
    "print(f\"TruncationErrorMaxA={TruncationErrorMaxA};\")\n",
    "print(f\"TruncationErrorMaxB={TruncationErrorMaxB};\")\n",
    "print(f\"ProjectedConstraintsMax={ProjectedConstraintsMax};\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(100)/100\n",
    "y1 = np.sin(x)\n",
    "y2 = np.cos(x)\n",
    "\n",
    "styles =  plt.style.available\n",
    "\n",
    "for style in styles:\n",
    "    print(style)\n",
    "    plt.style.use(style)\n",
    "    plt.plot(x,y1,label=\"y1asfasd\")\n",
    "    plt.plot(x,y2,label=\"y3asfasd\")\n",
    "    plt.title(\"asdf\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"/panfs/ds09/sxs/himanshu/gauge_stuff/gauge_driver_runs/runs/make_report_del/{style}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 4\n",
    "print(np.convolve(np.arange(1,10),np.ones(w),'valid')/w)\n",
    "print(np.arange(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(array,avg_len):\n",
    "    return np.convolve(array,np.ones(avg_len))/avg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_data_dict[\"AccTest_q1ns_Lev5\"].iloc[1200:,1:].max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true,
  "vscode": {
   "interpreter": {
    "hash": "009adc1c8ee1f76b2251d0bb13ed6e10d4fef5bd0a6f7d195d9f2892e5880fe6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
