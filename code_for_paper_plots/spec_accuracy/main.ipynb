{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241d8afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from pathlib import Path\n",
    "from random import choice as rc\n",
    "from typing import Dict, List\n",
    "\n",
    "import h5py\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scri\n",
    "from scipy.interpolate import CubicSpline, interp1d\n",
    "from spherical_functions import LM_index as lm\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 10)\n",
    "\n",
    "spec_home = \"/home/himanshu/spec/my_spec\"\n",
    "matplotlib.matplotlib_fname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e29893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =================================================================================================\n",
    "# =================================================================================================\n",
    "# FUNCTION DEFINITIONS\n",
    "# =================================================================================================\n",
    "# =================================================================================================\n",
    "\n",
    "# =================================================================================================\n",
    "# cce_new.ipynb\n",
    "# =================================================================================================\n",
    "\n",
    "\n",
    "def load_and_pickle(\n",
    "    data_path: Path,\n",
    "    reload_data: bool = False,\n",
    "    data_type: str = \"abd\",\n",
    "    options: dict = {},\n",
    "):\n",
    "    if not data_path.exists():\n",
    "        raise Exception(f\"{data_path} does not exist!\")\n",
    "\n",
    "    saved_data_path = data_path.parent / \"saved.pkl\"\n",
    "\n",
    "    if saved_data_path.exists() and not reload_data:\n",
    "        with open(saved_data_path, \"rb\") as f:\n",
    "            saved_data = pickle.load(f)\n",
    "            print(f\"Saved data loaded: {saved_data_path}\")\n",
    "    else:\n",
    "        saved_data = {}\n",
    "        if data_type == \"abd\":\n",
    "            saved_data[\"abd\"] = scri.create_abd_from_h5(\n",
    "                file_name=str(data_path), file_format=\"spectrecce_v1\", **options\n",
    "            )\n",
    "            with open(saved_data_path, \"wb\") as f:\n",
    "                pickle.dump(saved_data, f)\n",
    "            print(f\"Data loaded and saved at : {saved_data_path}\")\n",
    "\n",
    "    return saved_data\n",
    "\n",
    "\n",
    "def load_bondi_constraints(data_path: Path):\n",
    "    if not data_path.exists():\n",
    "        raise Exception(f\"{data_path} does not exist!\")\n",
    "    saved_data_path = data_path.parent / \"saved.pkl\"\n",
    "    if not saved_data_path.exists():\n",
    "        raise Exception(f\"{saved_data_path} does not exist\")\n",
    "    else:\n",
    "        with open(saved_data_path, \"rb\") as f:\n",
    "            saved_data = pickle.load(f)\n",
    "            if \"bondi_violation_norms\" in saved_data:\n",
    "                print(f\"bondi_violation_norms loaded for {data_path}\")\n",
    "            else:\n",
    "                print(f\"Computing bondi_violation_norms for: {data_path}\")\n",
    "                saved_data[\"bondi_violation_norms\"] = saved_data[\n",
    "                    \"abd\"\n",
    "                ].bondi_violation_norms\n",
    "                with open(saved_data_path, \"wb\") as f:\n",
    "                    pickle.dump(saved_data, f)\n",
    "\n",
    "                print(f\"Saved bondi_violation_norms for: {data_path}\")\n",
    "        return saved_data\n",
    "\n",
    "\n",
    "def add_bondi_constraints(abd_data: dict):\n",
    "    for key in abd_data:\n",
    "        abd_data[key][\"bondi_violation_norms\"] = abd_data[key][\n",
    "            \"abd\"\n",
    "        ].bondi_violation_norms\n",
    "        print(f\"bondi_violation_norms computed for {key}\")\n",
    "\n",
    "\n",
    "def create_diff_dict_cce(\n",
    "    WT_data_dict: dict, l: int, m: int, base_key: str, t_interpolate: np.ndarray\n",
    "):\n",
    "    h = WT_data_dict[base_key][\"abd\"].h.interpolate(t_interpolate)\n",
    "    diff_dict = {\"t\": h.t}\n",
    "    y_base = h.data[:, lm(l, m, h.ell_min)]\n",
    "    y_norm = np.linalg.norm(y_base)\n",
    "    for key in WT_data_dict:\n",
    "        if key == base_key:\n",
    "            continue\n",
    "        h = WT_data_dict[key][\"abd\"].h.interpolate(t_interpolate)\n",
    "        y_inter = h.data[:, lm(l, m, h.ell_min)]\n",
    "        diff_dict[key + \"_diff\"] = y_inter - y_base\n",
    "        diff_dict[key + \"_absdiff\"] = np.abs(y_inter - y_base)\n",
    "        diff_dict[key + \"_rel_diff\"] = (y_inter - y_base) / y_norm\n",
    "        diff_dict[key + \"_rel_absdiff\"] = np.abs(y_inter - y_base) / y_norm\n",
    "    return diff_dict\n",
    "\n",
    "\n",
    "def extract_radii(h5_file_path: Path):\n",
    "    radii = set()\n",
    "    with h5py.File(h5_file_path, \"r\") as f:\n",
    "        names = []\n",
    "        f.visit(names.append)\n",
    "    for name in names:\n",
    "        if \"Version\" in name:\n",
    "            continue\n",
    "        radii.add(name[1:5])\n",
    "    radii = list(radii)\n",
    "    radii.sort()\n",
    "    return radii\n",
    "\n",
    "\n",
    "def generate_columns(num_cols: int, beta_type=False):\n",
    "    if beta_type:\n",
    "        num_cols = num_cols * 2\n",
    "    L_max = int(np.sqrt((num_cols - 1) / 2)) - 1\n",
    "    # print(L_max,np.sqrt((num_cols-1)/2)-1)\n",
    "    col_names = [\"t(M)\"]\n",
    "    for l in range(0, L_max + 1):\n",
    "        for m in range(-l, l + 1):\n",
    "            if beta_type:\n",
    "                if m == 0:\n",
    "                    col_names.append(f\"Re({l},{m})\")\n",
    "                elif m < 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    col_names.append(f\"Re({l},{m})\")\n",
    "                    col_names.append(f\"Im({l},{m})\")\n",
    "            else:\n",
    "                col_names.append(f\"Re({l},{m})\")\n",
    "                col_names.append(f\"Im({l},{m})\")\n",
    "    return col_names\n",
    "\n",
    "\n",
    "def WT_to_pandas(horizon_path: Path):\n",
    "    assert horizon_path.exists()\n",
    "    df_dict = {}\n",
    "    beta_type_list = [\"Beta.dat\", \"DuR.dat\", \"R.dat\", \"W.dat\"]\n",
    "    with h5py.File(horizon_path, \"r\") as hf:\n",
    "        # Not all horizon files may have AhC\n",
    "        for key in hf.keys():\n",
    "            if key == \"VersionHist.ver\":\n",
    "                continue\n",
    "            if key in beta_type_list:\n",
    "                df_dict[key] = pd.DataFrame(\n",
    "                    hf[key], columns=generate_columns(hf[key].shape[1], beta_type=True)\n",
    "                )\n",
    "            else:\n",
    "                df_dict[key] = pd.DataFrame(\n",
    "                    hf[key], columns=generate_columns(hf[key].shape[1])\n",
    "                )\n",
    "\n",
    "    return df_dict\n",
    "\n",
    "\n",
    "def create_diff_dict(WT_data_dict: dict, mode: str, variable: str, base_key: str):\n",
    "    diff_dict = {\"t(M)\": WT_data_dict[base_key][variable][\"t(M)\"]}\n",
    "    y_base = WT_data_dict[base_key][variable][mode]\n",
    "    y_norm = np.linalg.norm(y_base)\n",
    "    for key in WT_data_dict:\n",
    "        if key == base_key:\n",
    "            continue\n",
    "        y = WT_data_dict[key][variable][mode]\n",
    "        t = WT_data_dict[key][variable][\"t(M)\"]\n",
    "        y_interpolator = interp1d(t, y, kind=\"cubic\", fill_value=\"extrapolate\")\n",
    "        y_inter = y_interpolator(diff_dict[\"t(M)\"])\n",
    "        diff_dict[key + \"_diff\"] = y_inter - y_base\n",
    "        diff_dict[key + \"_absdiff\"] = np.abs(y_inter - y_base)\n",
    "        diff_dict[key + \"_rel_diff\"] = (y_inter - y_base) / y_norm\n",
    "        diff_dict[key + \"_rel_absdiff\"] = np.abs(y_inter - y_base) / y_norm\n",
    "    return diff_dict\n",
    "\n",
    "\n",
    "def filter_by_regex(regex, col_list, exclude=False):\n",
    "    filtered_set = set()\n",
    "    if type(regex) is list:\n",
    "        for reg in regex:\n",
    "            for i in col_list:\n",
    "                if re.search(reg, i):\n",
    "                    filtered_set.add(i)\n",
    "    else:\n",
    "        for i in col_list:\n",
    "            if re.search(regex, i):\n",
    "                filtered_set.add(i)\n",
    "\n",
    "    filtered_list = list(filtered_set)\n",
    "    if exclude:\n",
    "        col_list_copy = list(col_list.copy())\n",
    "        for i in filtered_list:\n",
    "            if i in col_list_copy:\n",
    "                col_list_copy.remove(i)\n",
    "        filtered_list = col_list_copy\n",
    "\n",
    "    # Restore the original order\n",
    "    filtered_original_ordered_list = []\n",
    "    for i in list(col_list):\n",
    "        if i in filtered_list:\n",
    "            filtered_original_ordered_list.append(i)\n",
    "    return filtered_original_ordered_list\n",
    "\n",
    "\n",
    "def abs_mean_value_upto_l(pd_series, L_max: int):\n",
    "    idx = pd_series.index\n",
    "    abs_cum_sum = 0\n",
    "    num = 0\n",
    "    for i in idx:\n",
    "        L = int(i.split(\",\")[0][3:])\n",
    "        if L > L_max:\n",
    "            continue\n",
    "        else:\n",
    "            abs_cum_sum = abs_cum_sum + abs(pd_series[i])\n",
    "            num = num + 1\n",
    "    return abs_cum_sum / num\n",
    "\n",
    "\n",
    "def get_mode(name):\n",
    "    return int(name.split(\"(\")[-1].split(\")\")[0])\n",
    "\n",
    "\n",
    "def get_radii(name):\n",
    "    if name[-5] == \"R\":\n",
    "        # R0257 -> 0257load_and_pickle\n",
    "        return int(name.split(\"_\")[-1][1:])\n",
    "    else:\n",
    "        return int(name.split(\"_\")[-1])\n",
    "\n",
    "\n",
    "def sort_by_power_modes(col_names):\n",
    "    col_name_copy = list(col_names).copy()\n",
    "    return sorted(col_name_copy, key=lambda x: int(get_mode(x)))\n",
    "\n",
    "\n",
    "def add_L_mode_power(df: pd.DataFrame, L: int, ReOrIm: str):\n",
    "    column_names = df.columns\n",
    "    n = 0\n",
    "    power = 0\n",
    "    for m in range(-L, L + 1):\n",
    "        col_name = f\"{ReOrIm}({L},{m})\"\n",
    "        # print(col_name)\n",
    "        if col_name in column_names:\n",
    "            power = power + df[col_name] * df[col_name]\n",
    "            n = n + 1\n",
    "    if n != 0:\n",
    "        power = power / n\n",
    "        df[f\"pow_{ReOrIm}({L})\"] = power\n",
    "    return power\n",
    "\n",
    "\n",
    "def add_all_L_mode_power(df: pd.DataFrame, L_max: int):\n",
    "    local_df = df.copy()\n",
    "    total_power_Re = 0\n",
    "    total_power_Im = 0\n",
    "    for l in range(0, L_max + 1):\n",
    "        total_power_Re = total_power_Re + add_L_mode_power(local_df, l, \"Re\")\n",
    "        total_power_Im = total_power_Im + add_L_mode_power(local_df, l, \"Im\")\n",
    "        local_df[f\"pow_cum_Re({l})\"] = total_power_Re\n",
    "        local_df[f\"pow_cum_Im({l})\"] = total_power_Im\n",
    "    return local_df\n",
    "\n",
    "\n",
    "def create_power_diff_dict(\n",
    "    power_dict: dict, pow_mode: str, variable: str, base_key: str\n",
    "):\n",
    "    diff_dict = {\"t(M)\": power_dict[base_key][\"t(M)\"]}\n",
    "    y_base = power_dict[base_key][variable][pow_mode]\n",
    "    y_norm = np.linalg.norm(y_base)\n",
    "    for key in power_dict:\n",
    "        if key == base_key:\n",
    "            continue\n",
    "        y = power_dict[key][variable][pow_mode]\n",
    "        t = power_dict[key][\"t(M)\"]\n",
    "        y_interpolator = interp1d(t, y, kind=\"cubic\", fill_value=\"extrapolate\")\n",
    "        y_inter = y_interpolator(diff_dict[\"t(M)\"])\n",
    "        diff_dict[key + \"_diff\"] = y_inter - y_base\n",
    "        diff_dict[key + \"_absdiff\"] = np.abs(y_inter - y_base)\n",
    "        diff_dict[key + \"_rel_diff\"] = (y_inter - y_base) / y_norm\n",
    "        diff_dict[key + \"_rel_absdiff\"] = np.abs(y_inter - y_base) / y_norm\n",
    "    return diff_dict\n",
    "\n",
    "\n",
    "# =================================================================================================\n",
    "# make_report_and_plots.ipynb\n",
    "# =================================================================================================\n",
    "\n",
    "\n",
    "def make_Bh_pandas(h5_dir):\n",
    "    # Empty dataframe\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # List of all the vars in the h5 file\n",
    "    var_list = []\n",
    "    h5_dir.visit(var_list.append)\n",
    "\n",
    "    for var in var_list:\n",
    "        # This means there is no time column\n",
    "        # print(f\"{var} : {h5_dir[var].shape}\")\n",
    "        if df.shape == (0, 0):\n",
    "            # data[:,0] is time and then we have the data\n",
    "            data = h5_dir[var]\n",
    "\n",
    "            # vars[:-4] to remove the .dat at the end\n",
    "            col_names = make_col_names(var[:-4], data.shape[1] - 1)\n",
    "            col_names.append(\"t\")\n",
    "            # Reverse the list so that we get [\"t\",\"var_name\"]\n",
    "            col_names.reverse()\n",
    "            append_to_df(data[:], col_names, df)\n",
    "\n",
    "        else:\n",
    "            data = h5_dir[var]\n",
    "            col_names = make_col_names(var[:-4], data.shape[1] - 1)\n",
    "            append_to_df(data[:, 1:], col_names, df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def append_to_df(data, col_names, df):\n",
    "    for i, col_name in enumerate(col_names):\n",
    "        df[col_name] = data[:, i]\n",
    "\n",
    "\n",
    "def make_col_names(val_name: str, val_size: int):\n",
    "    col_names = []\n",
    "    if val_size == 1:\n",
    "        col_names.append(val_name)\n",
    "    else:\n",
    "        for i in range(val_size):\n",
    "            col_names.append(val_name + f\"_{i}\")\n",
    "    return col_names\n",
    "\n",
    "\n",
    "def horizon_to_pandas(horizon_path: Path):\n",
    "    assert horizon_path.exists()\n",
    "    df_dict = {}\n",
    "    with h5py.File(horizon_path, \"r\") as hf:\n",
    "        # Not all horizon files may have AhC\n",
    "        for key in hf.keys():\n",
    "            if key == \"VersionHist.ver\":\n",
    "                # Newer runs have this\n",
    "                continue\n",
    "            df_dict[key[:-4]] = make_Bh_pandas(hf[key])\n",
    "\n",
    "    return df_dict\n",
    "\n",
    "\n",
    "def read_horizon_across_Levs(path_list: List[Path]):\n",
    "    df_listAB = []\n",
    "    df_listC = []\n",
    "    final_dict = {}\n",
    "    for path in path_list:\n",
    "        df_lev = horizon_to_pandas(path)\n",
    "        # Either [AhA,AhB] or [AhA,AhB,AhC]\n",
    "        if len(df_lev.keys()) > 1:\n",
    "            df_listAB.append(df_lev)\n",
    "        # Either [AhC] or [AhA,AhB,AhC]\n",
    "        if (len(df_lev.keys()) == 1) or (len(df_lev.keys()) == 3):\n",
    "            df_listC.append(df_lev)\n",
    "    if len(df_listAB) == 1:\n",
    "        # There was only one lev\n",
    "        final_dict = df_listAB[0]\n",
    "    else:\n",
    "        final_dict[\"AhA\"] = pd.concat([df[\"AhA\"] for df in df_listAB])\n",
    "        final_dict[\"AhB\"] = pd.concat([df[\"AhB\"] for df in df_listAB])\n",
    "        if len(df_listC) > 0:\n",
    "            final_dict[\"AhC\"] = pd.concat([df[\"AhC\"] for df in df_listC])\n",
    "\n",
    "    return final_dict\n",
    "\n",
    "\n",
    "def load_horizon_data_from_levs(base_path: Path, runs_path: Dict[str, Path]):\n",
    "    data_dict = {}\n",
    "    for run_name in runs_path.keys():\n",
    "        path_list = list(base_path.glob(runs_path[run_name]))\n",
    "        print(path_list)\n",
    "        data_dict[run_name] = read_horizon_across_Levs(path_list)\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def flatten_dict(horizon_data_dict: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
    "    flattened_data = {}\n",
    "    for run_name in horizon_data_dict.keys():\n",
    "        for horizons in horizon_data_dict[run_name]:\n",
    "            flattened_data[run_name + \"_\" + horizons] = horizon_data_dict[run_name][\n",
    "                horizons\n",
    "            ]\n",
    "            # print(run_name+\"_\"+horizons)\n",
    "    return flattened_data\n",
    "\n",
    "\n",
    "def read_profiler(file_name):\n",
    "    with h5py.File(file_name, \"r\") as f:\n",
    "        steps = set()\n",
    "        procs = set()\n",
    "        names = []\n",
    "        f.visit(names.append)\n",
    "        for name in names:\n",
    "            step = name.split(\".\")[0][4:]\n",
    "            steps.add(step)\n",
    "            if \"Proc\" in name:\n",
    "                procs.add(name.split(\"/\")[-1][4:-4])\n",
    "\n",
    "        dict_list = []\n",
    "        for step in steps:\n",
    "            for proc in procs:\n",
    "                data = f[f\"Step{step}.dir/Proc{proc}.txt\"][0].decode()\n",
    "\n",
    "                lines = data.split(\"\\n\")\n",
    "                time = float((lines[0].split(\"=\")[-1])[:-1])\n",
    "\n",
    "                curr_dict = {\"t(M)\": time, \"step\": step, \"proc\": proc}\n",
    "                # Find where the columns end\n",
    "                a = lines[4]\n",
    "                event_end = a.find(\"Event\") + 5\n",
    "                cum_end = a.find(\"cum(%)\") + 6\n",
    "                exc_end = a.find(\"exc(%)\") + 6\n",
    "                inc_end = a.find(\"inc(%)\") + 6\n",
    "\n",
    "                for line in lines[6:-2]:\n",
    "                    Event = line[:event_end].strip()\n",
    "                    cum = float(line[event_end:cum_end].strip())\n",
    "                    exc = float(line[cum_end:exc_end].strip())\n",
    "                    inc = float(line[exc_end:inc_end].strip())\n",
    "                    N = int(line[inc_end:].strip())\n",
    "                    # print(a)\n",
    "                    # a = line.split(\"  \")\n",
    "                    # Event,cum,exc,inc,N = [i.strip() for i in a if i!= '']\n",
    "                    curr_dict[f\"{Event}_cum\"] = cum\n",
    "                    curr_dict[f\"{Event}_exc\"] = exc\n",
    "                    curr_dict[f\"{Event}_inc\"] = inc\n",
    "                    curr_dict[f\"{Event}_N\"] = N\n",
    "\n",
    "                dict_list.append(curr_dict)\n",
    "    return pd.DataFrame(dict_list)\n",
    "\n",
    "\n",
    "def read_dat_file(file_name):\n",
    "    cols_names = []\n",
    "    # Read column names\n",
    "    with open(file_name, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if \"#\" not in line:\n",
    "                # From now onwards it will be all data\n",
    "                break\n",
    "            elif \"=\" in line:\n",
    "                if (\"[\" not in line) and (\"]\" not in line):\n",
    "                    continue\n",
    "                cols_names.append(line.split(\"=\")[-1][1:-1].strip())\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    return pd.read_csv(file_name, sep=\"\\s+\", comment=\"#\", names=cols_names)\n",
    "\n",
    "\n",
    "def hist_files_to_dataframe(file_path):\n",
    "    # Function to parse a single line and return a dictionary of values\n",
    "    def parse_line(line):\n",
    "        data = {}\n",
    "        # Find all variable=value pairs\n",
    "        pairs = re.findall(r\"([^;=\\s]+)=\\s*([^;]+)\", line)\n",
    "        for var, val in pairs:\n",
    "            # Hist-GrDomain.txt should be parsed a little differently\n",
    "            if \"ResizeTheseSubdomains\" in var:\n",
    "                items = val.split(\"),\")\n",
    "                items[-1] = items[-1][:-1]\n",
    "                for item in items:\n",
    "                    name, _, vals = item.split(\"(\")\n",
    "                    r, l, m = vals[:-1].split(\",\")\n",
    "                    data[f\"{name}_R\"] = int(r)\n",
    "                    data[f\"{name}_L\"] = int(l)\n",
    "                    data[f\"{name}_M\"] = int(m)\n",
    "            else:\n",
    "                data[var] = float(val) if re.match(r\"^[\\d.e+-]+$\", val) else val\n",
    "        return data\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        # Parse the lines\n",
    "        data = []\n",
    "        for line in file.readlines():\n",
    "            data.append(parse_line(line.strip()))\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Files like AhACoefs.dat have unequal number of columns\n",
    "def read_dat_file_uneq_cols(file_name):\n",
    "    cols_names = []\n",
    "\n",
    "    temp_file = \"./temp.csv\"\n",
    "    col_length = 0\n",
    "    with open(file_name, \"r\") as f:\n",
    "        with open(temp_file, \"w\") as w:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                if line[0] != \"#\":  # This is data\n",
    "                    w.writelines(\" \".join(line.split()[:col_length]) + \"\\n\")\n",
    "                if (\n",
    "                    line[0:3] == \"# [\" or line[0:4] == \"#  [\"\n",
    "                ):  # Some dat files have comments on the top\n",
    "                    cols_names.append(line.split(\"=\")[-1][1:-1].strip())\n",
    "                    col_length = col_length + 1\n",
    "\n",
    "    return pd.read_csv(temp_file, delim_whitespace=True, names=cols_names)\n",
    "\n",
    "\n",
    "def read_dat_file_across_AA(file_pattern):\n",
    "    # ApparentHorizons/Horizons.h5@AhA\n",
    "    if \"Horizons.h5@\" in file_pattern:\n",
    "        file_pattern, h5_key = file_pattern.split(\"@\")\n",
    "\n",
    "    path_pattern = file_pattern\n",
    "    path_collection = []\n",
    "\n",
    "    for folder_name in glob.iglob(path_pattern, recursive=True):\n",
    "        if os.path.isdir(folder_name) or os.path.isfile(folder_name):\n",
    "            path_collection.append(folder_name)\n",
    "    path_collection.sort()\n",
    "\n",
    "    read_data_collection = []\n",
    "    for path in path_collection:\n",
    "        print(path)\n",
    "        # AhACoefs.dat has uneq cols\n",
    "        if \"Coefs.dat\" in path:\n",
    "            read_data_collection.append(read_dat_file_uneq_cols(path))\n",
    "        elif \"Hist-\" in path:\n",
    "            read_data_collection.append(hist_files_to_dataframe(path))\n",
    "        elif \"Profiler\" in path:\n",
    "            read_data_collection.append(read_profiler(path))\n",
    "        elif \"Horizons.h5\" in path:\n",
    "            returned_data = read_horizonh5(path, h5_key)\n",
    "            if returned_data is not None:\n",
    "                read_data_collection.append(returned_data)\n",
    "        else:\n",
    "            read_data_collection.append(read_dat_file(path))\n",
    "\n",
    "    data = pd.concat(read_data_collection)\n",
    "    rename_dict = {\n",
    "        \"t\": \"t(M)\",\n",
    "        \"time\": \"t(M)\",\n",
    "        \"Time\": \"t(M)\",\n",
    "        \"time after step\": \"t(M)\",\n",
    "    }\n",
    "    data.rename(columns=rename_dict, inplace=True)\n",
    "    # print(data.columns)\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_horizonh5(horizonh5_path, h5_key):\n",
    "    with h5py.File(horizonh5_path, \"r\") as hf:\n",
    "        # h5_key = ['AhA','AhB','AhC']\n",
    "        # Horizons.h5 has keys 'AhA.dir'\n",
    "        key = h5_key + \".dir\"\n",
    "        # 'AhC' will not be all the horizons.h5\n",
    "        if key in hf.keys():\n",
    "            return make_Bh_pandas(hf[key])\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "def read_AH_files(Ev_path):\n",
    "    fileA = Ev_path + \"Run/ApparentHorizons/AhA.dat\"\n",
    "    fileB = Ev_path + \"Run/ApparentHorizons/AhB.dat\"\n",
    "\n",
    "    dataA = read_dat_file_across_AA(fileA)\n",
    "    dataB = read_dat_file_across_AA(fileB)\n",
    "\n",
    "    return dataA, dataB\n",
    "\n",
    "\n",
    "# Combines all the pvd files into a single file and save it in the base folder\n",
    "def combine_pvd_files(base_folder: Path, file_pattern: str, output_path=None):\n",
    "    pvd_start = \"\"\"<?xml version=\"1.0\"?>\\n<VTKFile type=\"Collection\" version=\"0.1\" byte_order=\"LittleEndian\">\\n  <Collection>\\n\"\"\"\n",
    "    pvd_end = \"  </Collection>\\n</VTKFile>\"\n",
    "\n",
    "    vis_folder_name = file_pattern.split(\"/\")[-1][:-4]\n",
    "    Lev = file_pattern[0:4]\n",
    "\n",
    "    if output_path is None:\n",
    "        output_path = f\"{base_folder}/{vis_folder_name}_{Lev}.pvd\"\n",
    "\n",
    "    pvd_files = list(base_folder.glob(file_pattern))\n",
    "    pvd_folders = list(base_folder.glob(file_pattern[:-4]))\n",
    "\n",
    "    with open(output_path, \"w\") as write_file:\n",
    "        write_file.writelines(pvd_start)\n",
    "        for files in pvd_files:\n",
    "            print(files)\n",
    "            with files.open(\"r\") as f:\n",
    "                for line in f.readlines():\n",
    "                    line = line.replace(vis_folder_name, str(files)[:-4])\n",
    "                    if \"DataSet\" in line:\n",
    "                        write_file.writelines(line)\n",
    "        write_file.writelines(pvd_end)\n",
    "\n",
    "    print(output_path)\n",
    "\n",
    "\n",
    "def moving_average(array, avg_len):\n",
    "    return np.convolve(array, np.ones(avg_len)) / avg_len\n",
    "\n",
    "\n",
    "def moving_average_valid(array, avg_len):\n",
    "    return np.convolve(array, np.ones(avg_len), \"valid\") / avg_len\n",
    "\n",
    "\n",
    "def path_to_folder_name(folder_name):\n",
    "    return folder_name.replace(\"/\", \"_\")\n",
    "\n",
    "\n",
    "# Give a dict of {\"run_name\" = runs_path} and data_file_path to get {\"run_name\" = dat_file_data}\n",
    "def load_data_from_levs(runs_path, data_file_path):\n",
    "    data_dict = {}\n",
    "    column_list = \"\"\n",
    "    for run_name in runs_path.keys():\n",
    "        data_dict[run_name] = read_dat_file_across_AA(\n",
    "            runs_path[run_name] + data_file_path\n",
    "        )\n",
    "        column_list = data_dict[run_name].columns\n",
    "    return column_list, data_dict\n",
    "\n",
    "\n",
    "def add_diff_columns(runs_data_dict, x_axis, y_axis, diff_base):\n",
    "    if diff_base not in runs_data_dict.keys():\n",
    "        raise Exception(f\"{diff_base} not in {runs_data_dict.keys()}\")\n",
    "\n",
    "    unique_x_data, unique_indices = np.unique(\n",
    "        runs_data_dict[diff_base][x_axis], return_index=True\n",
    "    )\n",
    "    # sorted_indices = np.sort(unique_indices)\n",
    "    unique_y_data = runs_data_dict[diff_base][y_axis].iloc[unique_indices]\n",
    "    interpolated_data = sp.interpolate.CubicSpline(\n",
    "        unique_x_data, unique_y_data, extrapolate=False\n",
    "    )\n",
    "    # interpolated_data = sp.interpolate.PchipInterpolator(unique_x_data, unique_y_data, extrapolate=False)\n",
    "\n",
    "    for key in runs_data_dict:\n",
    "        if key == diff_base:\n",
    "            continue\n",
    "        df = runs_data_dict[key]\n",
    "        df[\"diff_abs_\" + y_axis] = np.abs(df[y_axis] - interpolated_data(df[x_axis]))\n",
    "        df[\"diff_\" + y_axis] = df[y_axis] - interpolated_data(df[x_axis])\n",
    "\n",
    "\n",
    "def plot_graph_for_runs_wrapper(\n",
    "    runs_data_dict,\n",
    "    x_axis,\n",
    "    y_axis_list,\n",
    "    minT,\n",
    "    maxT,\n",
    "    legend_dict=None,\n",
    "    save_path=None,\n",
    "    moving_avg_len=0,\n",
    "    plot_fun=lambda x, y, label: plt.plot(x, y, label=label),\n",
    "    sort_by=None,\n",
    "    diff_base=None,\n",
    "    title=None,\n",
    "    append_to_title=\"\",\n",
    "    plot_abs_diff=False,\n",
    "    constant_shift_val_time=None,\n",
    "    modification_function=None,\n",
    "    take_abs=False,\n",
    "):\n",
    "    # Do this better using columns of a pandas dataframe\n",
    "    for y_axis in y_axis_list[:-1]:\n",
    "        legend_dict = {}\n",
    "        for key in runs_data_dict:\n",
    "            legend_dict[key] = key + \"_\" + str(y_axis)\n",
    "        plot_graph_for_runs(\n",
    "            runs_data_dict,\n",
    "            x_axis,\n",
    "            y_axis,\n",
    "            minT,\n",
    "            maxT,\n",
    "            legend_dict=legend_dict,\n",
    "            save_path=None,\n",
    "            moving_avg_len=moving_avg_len,\n",
    "            plot_fun=plot_fun,\n",
    "            sort_by=sort_by,\n",
    "            diff_base=diff_base,\n",
    "            title=title,\n",
    "            append_to_title=append_to_title,\n",
    "            plot_abs_diff=plot_abs_diff,\n",
    "            constant_shift_val_time=constant_shift_val_time,\n",
    "            modification_function=modification_function,\n",
    "            take_abs=take_abs,\n",
    "        )\n",
    "\n",
    "    # Save when plotting the last y_axis.\n",
    "    y_axis = y_axis_list[-1]\n",
    "    legend_dict = {}\n",
    "    for key in runs_data_dict:\n",
    "        legend_dict[key] = key + \"_\" + str(y_axis)\n",
    "    plot_graph_for_runs(\n",
    "        runs_data_dict,\n",
    "        x_axis,\n",
    "        y_axis,\n",
    "        minT,\n",
    "        maxT,\n",
    "        legend_dict=legend_dict,\n",
    "        save_path=save_path,\n",
    "        moving_avg_len=moving_avg_len,\n",
    "        plot_fun=plot_fun,\n",
    "        sort_by=sort_by,\n",
    "        diff_base=diff_base,\n",
    "        title=title,\n",
    "        append_to_title=append_to_title,\n",
    "        plot_abs_diff=plot_abs_diff,\n",
    "        constant_shift_val_time=constant_shift_val_time,\n",
    "        modification_function=modification_function,\n",
    "        take_abs=take_abs,\n",
    "    )\n",
    "\n",
    "    plt.ylabel(\"\")\n",
    "    plt.title(\"\" + append_to_title)\n",
    "\n",
    "    if save_path is not None:\n",
    "        fig_x_label = \"\"\n",
    "        fig_y_label = \"\"\n",
    "\n",
    "        for y_axis in y_axis_list:\n",
    "            fig_x_label = fig_x_label + x_axis.replace(\"/\", \"_\").replace(\".\", \"_\")\n",
    "            fig_y_label = fig_y_label + y_axis.replace(\"/\", \"_\").replace(\".\", \"_\")\n",
    "        save_file_name = (\n",
    "            f\"{fig_y_label}_vs_{fig_x_label}_minT={minT}_maxT={maxT}\".replace(\".\", \"_\")\n",
    "        )\n",
    "        if moving_avg_len > 0:\n",
    "            save_file_name = save_file_name + f\"_moving_avg_len={moving_avg_len}\"\n",
    "        if diff_base is not None:\n",
    "            save_file_name = save_file_name + f\"_diff_base={diff_base}\"\n",
    "\n",
    "        if len(save_file_name) >= 251:  # <save_file_name>.png >=255\n",
    "            save_file_name = save_file_name[:245] + str(random.randint(10000, 99999))\n",
    "            print(f\"The filename was too long!! New filename is {save_file_name}\")\n",
    "\n",
    "        plt.savefig(save_path + save_file_name)\n",
    "\n",
    "\n",
    "def plot_graph_for_runs(\n",
    "    runs_data_dict_original,\n",
    "    x_axis,\n",
    "    y_axis,\n",
    "    minT,\n",
    "    maxT,\n",
    "    legend_dict=None,\n",
    "    save_path=None,\n",
    "    moving_avg_len=0,\n",
    "    plot_fun=lambda x, y, label: plt.plot(x, y, label=label),\n",
    "    sort_by=None,\n",
    "    diff_base=None,\n",
    "    title=None,\n",
    "    append_to_title=\"\",\n",
    "    plot_abs_diff=False,\n",
    "    constant_shift_val_time=None,\n",
    "    modification_function=None,\n",
    "    take_abs=False,\n",
    "):\n",
    "    runs_data_dict = runs_data_dict_original\n",
    "    if modification_function is not None:\n",
    "        runs_data_dict = copy.deepcopy(runs_data_dict_original)\n",
    "        for key in runs_data_dict:\n",
    "            new_x, new_y, new_y_axis = modification_function(\n",
    "                runs_data_dict[key][x_axis],\n",
    "                runs_data_dict[key][y_axis],\n",
    "                runs_data_dict[key],\n",
    "                y_axis,\n",
    "            )\n",
    "            runs_data_dict[key][new_y_axis] = new_y\n",
    "            runs_data_dict[key][x_axis] = new_x\n",
    "        y_axis = new_y_axis\n",
    "\n",
    "    sort_run_data_dict(runs_data_dict, sort_by=sort_by)\n",
    "    current_runs_data_dict_keys = list(runs_data_dict.keys())\n",
    "\n",
    "    if diff_base is not None:\n",
    "        add_diff_columns(runs_data_dict, x_axis, y_axis, diff_base)\n",
    "        current_runs_data_dict_keys = []\n",
    "        for key in runs_data_dict:\n",
    "            if key == diff_base:\n",
    "                continue\n",
    "            else:\n",
    "                current_runs_data_dict_keys.append(key)\n",
    "        if plot_abs_diff:\n",
    "            y_axis = \"diff_abs_\" + y_axis\n",
    "        else:\n",
    "            y_axis = \"diff_\" + y_axis\n",
    "\n",
    "    # Find the indices corresponding to maxT and minT\n",
    "    minT_indx_list = {}\n",
    "    maxT_indx_list = {}\n",
    "\n",
    "    if legend_dict is None:\n",
    "        legend_dict = {}\n",
    "        for run_name in current_runs_data_dict_keys:\n",
    "            legend_dict[run_name] = None\n",
    "    else:\n",
    "        for run_name in current_runs_data_dict_keys:\n",
    "            if run_name not in legend_dict:\n",
    "                raise ValueError(f\"{run_name} not in {legend_dict=}\")\n",
    "\n",
    "    for run_name in current_runs_data_dict_keys:\n",
    "        minT_indx_list[run_name] = len(\n",
    "            runs_data_dict[run_name][x_axis][runs_data_dict[run_name][x_axis] < minT]\n",
    "        )\n",
    "        maxT_indx_list[run_name] = len(\n",
    "            runs_data_dict[run_name][x_axis][runs_data_dict[run_name][x_axis] < maxT]\n",
    "        )\n",
    "\n",
    "    if moving_avg_len == 0:\n",
    "        for run_name in current_runs_data_dict_keys:\n",
    "            x_data = runs_data_dict[run_name][x_axis][\n",
    "                minT_indx_list[run_name] : maxT_indx_list[run_name]\n",
    "            ]\n",
    "            y_data = runs_data_dict[run_name][y_axis][\n",
    "                minT_indx_list[run_name] : maxT_indx_list[run_name]\n",
    "            ]\n",
    "\n",
    "            if constant_shift_val_time is not None:\n",
    "                shift_label_val = np.abs(x_data.iloc[-1] - x_data.iloc[0]) / 4\n",
    "                unique_x_data, unique_indices = np.unique(x_data, return_index=True)\n",
    "                # sorted_indices = np.sort(unique_indices)\n",
    "                unique_y_data = y_data.iloc[unique_indices]\n",
    "                try:\n",
    "                    interpolated_data = CubicSpline(\n",
    "                        unique_x_data, unique_y_data, extrapolate=False\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(run_name, unique_y_data)\n",
    "                y_data = y_data - interpolated_data(constant_shift_val_time)\n",
    "                if plot_abs_diff:\n",
    "                    y_data = np.abs(y_data)\n",
    "\n",
    "            #   print(f\"{len(x_data)=},{len(y_data)=},{len(np.argsort(x_data))=},{type(x_data)=}\")\n",
    "\n",
    "            #   sorted_indices = x_data.argsort()\n",
    "            #   x_data = x_data.iloc[sorted_indices]\n",
    "            #   y_data = y_data.iloc[sorted_indices]\n",
    "            legend = legend_dict[run_name]\n",
    "            if legend is None:\n",
    "                legend = run_name\n",
    "            if take_abs:\n",
    "                y_data = np.abs(y_data)\n",
    "            plot_fun(x_data, y_data, legend)\n",
    "\n",
    "            if constant_shift_val_time is not None:\n",
    "                plt.axhline(y=y_data.iloc[-1], linestyle=\":\")\n",
    "                plt.text(\n",
    "                    x=np.random.rand() * shift_label_val + x_data.iloc[0],\n",
    "                    y=y_data.iloc[-1],\n",
    "                    s=f\"{y_data.iloc[-1]:.2e}\",\n",
    "                    verticalalignment=\"bottom\",\n",
    "                )\n",
    "\n",
    "        plt.xlabel(x_axis)\n",
    "        plt.ylabel(y_axis)\n",
    "        if constant_shift_val_time is not None:\n",
    "            plt.axvline(x=constant_shift_val_time, linestyle=\":\", color=\"red\")\n",
    "        if title is None:\n",
    "            title = '\"' + y_axis + '\" vs \"' + x_axis + '\"'\n",
    "            if constant_shift_val_time is not None:\n",
    "                title = title + f\" constant_shift_val_time={constant_shift_val_time}\"\n",
    "            if diff_base is not None:\n",
    "                title = title + f\" diff_base={diff_base}\"\n",
    "            if plot_abs_diff:\n",
    "                title = title + \" (abs_diff)\"\n",
    "        plt.title(title + append_to_title)\n",
    "        plt.legend()\n",
    "\n",
    "    else:\n",
    "        for run_name in current_runs_data_dict_keys:\n",
    "            x_data = np.array(\n",
    "                runs_data_dict[run_name][x_axis][\n",
    "                    minT_indx_list[run_name] + moving_avg_len - 1 : maxT_indx_list[\n",
    "                        run_name\n",
    "                    ]\n",
    "                ]\n",
    "            )\n",
    "            y_data = np.array(\n",
    "                moving_average_valid(\n",
    "                    runs_data_dict[run_name][y_axis][\n",
    "                        minT_indx_list[run_name] : maxT_indx_list[run_name]\n",
    "                    ],\n",
    "                    moving_avg_len,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if constant_shift_val_time is not None:\n",
    "                shift_label_val = np.abs(x_data.iloc[-1] - x_data.iloc[0]) / 4\n",
    "                unique_x_data, unique_indices = np.unique(x_data, return_index=True)\n",
    "                # sorted_indices = np.sort(unique_indices)\n",
    "                unique_y_data = y_data.iloc[unique_indices]\n",
    "\n",
    "                interpolated_data = CubicSpline(\n",
    "                    unique_x_data, unique_y_data, extrapolate=False\n",
    "                )\n",
    "                y_data = y_data - interpolated_data(constant_shift_val_time)\n",
    "                if plot_abs_diff:\n",
    "                    y_data = np.abs(y_data)\n",
    "\n",
    "            #   sorted_indices = np.argsort(x_data)\n",
    "            #   x_data = x_data[sorted_indices]\n",
    "            #   y_data = y_data[sorted_indices]\n",
    "            legend = legend_dict[run_name]\n",
    "            if legend is None:\n",
    "                legend = run_name\n",
    "            if take_abs:\n",
    "                y_data = np.abs(y_data)\n",
    "            plot_fun(x_data, y_data, legend)\n",
    "\n",
    "            if constant_shift_val_time is not None:\n",
    "                plt.axhline(y=y_data.iloc[-1], linestyle=\":\")\n",
    "                plt.text(\n",
    "                    x=np.random.rand() * shift_label_val + x_data.iloc[0],\n",
    "                    y=y_data.iloc[-1],\n",
    "                    s=f\"{y_data.iloc[-1]:.1f}\",\n",
    "                    verticalalignment=\"bottom\",\n",
    "                )\n",
    "\n",
    "        plt.xlabel(x_axis)\n",
    "        plt.ylabel(y_axis)\n",
    "        if constant_shift_val_time is not None:\n",
    "            plt.axvline(x=constant_shift_val_time, linestyle=\":\", color=\"red\")\n",
    "        if title is None:\n",
    "            title = (\n",
    "                '\"'\n",
    "                + y_axis\n",
    "                + '\" vs \"'\n",
    "                + x_axis\n",
    "                + '\"  '\n",
    "                + f\"avg_window_len={moving_avg_len}\"\n",
    "            )\n",
    "            if constant_shift_val_time is not None:\n",
    "                title = title + f\" constant_shift_val_time={constant_shift_val_time}\"\n",
    "            if diff_base is not None:\n",
    "                title = title + f\" diff_base={diff_base}\"\n",
    "            if plot_abs_diff:\n",
    "                title = title + \" (abs_diff)\"\n",
    "        plt.title(title + append_to_title)\n",
    "        plt.legend()\n",
    "\n",
    "    if save_path is not None:\n",
    "        fig_x_label = x_axis.replace(\"/\", \"_\").replace(\".\", \"_\")\n",
    "        fig_y_label = y_axis.replace(\"/\", \"_\").replace(\".\", \"_\")\n",
    "        save_file_name = (\n",
    "            f\"{fig_y_label}_vs_{fig_x_label}_minT={minT}_maxT={maxT}\".replace(\".\", \"_\")\n",
    "        )\n",
    "        if moving_avg_len > 0:\n",
    "            save_file_name = save_file_name + f\"_moving_avg_len={moving_avg_len}\"\n",
    "        if diff_base is not None:\n",
    "            save_file_name = save_file_name + f\"_diff_base={diff_base}\"\n",
    "\n",
    "        for run_name in current_runs_data_dict_keys:\n",
    "            save_file_name = (\n",
    "                save_file_name + \"__\" + run_name.replace(\"/\", \"_\").replace(\".\", \"_\")\n",
    "            )\n",
    "\n",
    "        if len(save_file_name) >= 251:  # <save_file_name>.png >=255\n",
    "            save_file_name = save_file_name[:245] + str(random.randint(10000, 99999))\n",
    "            print(f\"The filename was too long!! New filename is {save_file_name}\")\n",
    "\n",
    "        plt.savefig(save_path + save_file_name)\n",
    "\n",
    "\n",
    "def find_file(pattern):\n",
    "    return glob.glob(pattern, recursive=True)[0]\n",
    "\n",
    "\n",
    "def is_the_current_run_going_on(run_folder):\n",
    "    if len(find_file(run_folder + \"/**/\" + \"TerminationReason.txt\")) > 0:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def plot_min_grid_spacing(runs_data_dict):\n",
    "    \"\"\"\n",
    "    runs_data_dict should have dataframes with MinimumGridSpacing.dat data.\n",
    "    The function will compute the min grid spacing along all domains and plot it.\n",
    "    \"\"\"\n",
    "    keys = runs_data_dict.keys()\n",
    "    if len(keys) == 0:\n",
    "        print(\"There are no dataframes in the dict\")\n",
    "\n",
    "    for key in keys:\n",
    "        t_step = runs_data_dict[key][\"t\"]\n",
    "        min_val = runs_data_dict[key].drop(columns=[\"t\"]).min(axis=\"columns\")\n",
    "        plt.plot(t_step, min_val, label=key)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"Min Grid Spacing\")\n",
    "    plt.title(\"Min grid spacing in all domains\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_GrAdjustSubChunksToDampingTimes(runs_data_dict):\n",
    "    keys = runs_data_dict.keys()\n",
    "    if len(keys) > 1:\n",
    "        print(\n",
    "            \"To plot the Tdamp for various quantities only put one dataframe in the runs_data_dict\"\n",
    "        )\n",
    "\n",
    "    data: pd.DataFrame = runs_data_dict[list(keys)[0]]\n",
    "    tdamp_keys = []\n",
    "    for key in data.keys():\n",
    "        if \"Tdamp\" in key:\n",
    "            tdamp_keys.append(key)\n",
    "\n",
    "    # Get a colormap\n",
    "    cmap = plt.get_cmap(\"tab10\")\n",
    "    colors = cmap(np.linspace(0, 1, len(tdamp_keys)))\n",
    "\n",
    "    t_vals = data[\"time\"]\n",
    "    for i, color, key in zip(range(len(tdamp_keys)), colors, tdamp_keys):\n",
    "        if i % 2 == 0:\n",
    "            plt.plot(t_vals, data[key], label=key, color=color)\n",
    "        else:\n",
    "            plt.plot(t_vals, data[key], label=key, color=color, linestyle=\"--\")\n",
    "\n",
    "    min_tdamp = data[tdamp_keys].min(axis=\"columns\")\n",
    "    plt.plot(\n",
    "        t_vals,\n",
    "        min_tdamp,\n",
    "        label=\"min_tdamp\",\n",
    "        linewidth=3,\n",
    "        linestyle=\"dotted\",\n",
    "        color=\"red\",\n",
    "    )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"time\")\n",
    "    plt.title(list(keys)[0])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def add_max_and_min_val(runs_data_dict):\n",
    "    # If we load a file with 5 columns with first being time, then find max and min values for all the other columns, at all times and add it to the dataframe.\n",
    "    # Useful when you want to find like Linf across all domains at all times\n",
    "    for run_name in runs_data_dict.keys():\n",
    "        data_frame = runs_data_dict[run_name]\n",
    "        t = data_frame.iloc[:, 0]\n",
    "        max_val = np.zeros_like(t)\n",
    "        min_val = np.zeros_like(t)\n",
    "        for i in range(len(t)):\n",
    "            max_val[i] = data_frame.iloc[i, 1:].max()\n",
    "            min_val[i] = data_frame.iloc[i, 1:].max()\n",
    "\n",
    "        # Add the values to the dataframe\n",
    "        data_frame[\"max_val\"] = max_val\n",
    "        data_frame[\"min_val\"] = min_val\n",
    "\n",
    "\n",
    "def sort_run_data_dict(runs_data_dict: dict, sort_by=None):\n",
    "    for run_name in runs_data_dict.keys():\n",
    "        run_df = runs_data_dict[run_name]\n",
    "        if sort_by is None:\n",
    "            sort_by = run_df.keys()[0]\n",
    "        runs_data_dict[run_name] = run_df.sort_values(by=sort_by)\n",
    "\n",
    "\n",
    "# =================================================================================================\n",
    "# power_diag.ipynb\n",
    "# =================================================================================================\n",
    "\n",
    "\n",
    "def join_str_with_underscore(str_list):\n",
    "    a = str_list[0]\n",
    "    for i in str_list[1:]:\n",
    "        a = a + f\"_{i}\"\n",
    "    return a\n",
    "\n",
    "\n",
    "def get_top_name_from_number(top_number: int, subdomain_name: str) -> str:\n",
    "    if re.match(r\"Sphere\", subdomain_name):\n",
    "        return [\"Bf0I1\", \"Bf1S2\", \"Bf1S2\"][top_number]\n",
    "    elif re.match(r\"Cylinder\", subdomain_name):\n",
    "        return [\"Bf0I1\", \"Bf1S1\", \"Bf2I1\"][top_number]\n",
    "    elif re.match(r\"FilledCylinder\", subdomain_name):\n",
    "        return [\"Bf0I1\", \"Bf1B2Radial\", \"Bf1B2\"][top_number]\n",
    "    else:\n",
    "        raise Exception(f\"{subdomain_name=} not recognized!\")\n",
    "\n",
    "\n",
    "def get_domain_name(col_name):\n",
    "    def AMR_domains_to_decimal(subdoamin_name):\n",
    "        # SphereC28.0.1\n",
    "        a = subdoamin_name.split(\".\")\n",
    "        # a = [SphereC28,0,1]\n",
    "        decimal_rep = a[0] + \".\"\n",
    "        # decimal_rep = SphereC28.\n",
    "        for i in a[1:]:\n",
    "            decimal_rep = decimal_rep + i\n",
    "        # decimal_rep = SphereC28.01\n",
    "        return decimal_rep\n",
    "\n",
    "    if \"on\" in col_name:\n",
    "        return AMR_domains_to_decimal(col_name.split(\" \")[-1])\n",
    "    if \".\" in col_name:\n",
    "        return AMR_domains_to_decimal(col_name.split(\" \")[-1])\n",
    "    elif \"_\" in col_name:\n",
    "        return col_name.split(\"_\")[0]\n",
    "    elif \"MinimumGridSpacing\" in col_name:\n",
    "        return col_name.split(\"[\")[-1][:-1]\n",
    "    else:\n",
    "        return col_name\n",
    "        # raise Exception(f\"{col_name} type not implemented in return_sorted_domain_names\")\n",
    "\n",
    "\n",
    "def filtered_domain_names(domain_names, filter):\n",
    "    return [i for i in domain_names if re.match(filter, get_domain_name(i))]\n",
    "\n",
    "\n",
    "def sort_spheres(sphere_list, reverse=False):\n",
    "    if len(sphere_list) == 0:\n",
    "        return []\n",
    "    if \"SphereA\" in sphere_list[0]:\n",
    "        return sorted(\n",
    "            sphere_list,\n",
    "            key=lambda x: float(get_domain_name(x).lstrip(\"SphereA\")),\n",
    "            reverse=reverse,\n",
    "        )\n",
    "    elif \"SphereB\" in sphere_list[0]:\n",
    "        return sorted(\n",
    "            sphere_list,\n",
    "            key=lambda x: float(get_domain_name(x).lstrip(\"SphereB\")),\n",
    "            reverse=reverse,\n",
    "        )\n",
    "    elif \"SphereC\" in sphere_list[0]:\n",
    "        return sorted(\n",
    "            sphere_list,\n",
    "            key=lambda x: float(get_domain_name(x).lstrip(\"SphereC\")),\n",
    "            reverse=reverse,\n",
    "        )\n",
    "    elif \"SphereD\" in sphere_list[0]:\n",
    "        return sorted(\n",
    "            sphere_list,\n",
    "            key=lambda x: float(get_domain_name(x).lstrip(\"SphereD\")),\n",
    "            reverse=reverse,\n",
    "        )\n",
    "    elif \"SphereE\" in sphere_list[0]:\n",
    "        return sorted(\n",
    "            sphere_list,\n",
    "            key=lambda x: float(get_domain_name(x).lstrip(\"SphereE\")),\n",
    "            reverse=reverse,\n",
    "        )\n",
    "\n",
    "\n",
    "def return_sorted_domain_names(domain_names):\n",
    "    FilledCylinderCA = filtered_domain_names(domain_names, r\"FilledCylinder.{0,2}CA\")\n",
    "    CylinderCA = filtered_domain_names(domain_names, r\"Cylinder.{0,2}CA\")\n",
    "    FilledCylinderEA = filtered_domain_names(domain_names, r\"FilledCylinder.{0,2}EA\")\n",
    "    CylinderEA = filtered_domain_names(domain_names, r\"Cylinder.{0,2}EA\")\n",
    "    SphereA = sort_spheres(filtered_domain_names(domain_names, \"SphereA\"), reverse=True)\n",
    "    CylinderSMA = filtered_domain_names(domain_names, r\"CylinderS.{0,2}MA\")\n",
    "    FilledCylinderMA = filtered_domain_names(domain_names, r\"FilledCylinder.{0,2}MA\")\n",
    "\n",
    "    FilledCylinderMB = filtered_domain_names(domain_names, r\"FilledCylinder.{0,2}MB\")\n",
    "    CylinderSMB = filtered_domain_names(domain_names, r\"CylinderS.{0,2}MB\")\n",
    "    SphereB = sort_spheres(filtered_domain_names(domain_names, \"SphereB\"), reverse=True)\n",
    "    CylinderEB = filtered_domain_names(domain_names, r\"Cylinder.{0,2}EB\")\n",
    "    FilledCylinderEB = filtered_domain_names(domain_names, r\"FilledCylinder.{0,2}EB\")\n",
    "    CylinderCB = filtered_domain_names(domain_names, r\"Cylinder.{0,2}CB\")\n",
    "    FilledCylinderCB = filtered_domain_names(domain_names, r\"FilledCylinder.{0,2}CB\")\n",
    "\n",
    "    SphereC = sort_spheres(\n",
    "        filtered_domain_names(domain_names, \"SphereC\"), reverse=False\n",
    "    )\n",
    "    SphereD = sort_spheres(\n",
    "        filtered_domain_names(domain_names, \"SphereD\"), reverse=False\n",
    "    )\n",
    "    SphereE = sort_spheres(\n",
    "        filtered_domain_names(domain_names, \"SphereE\"), reverse=False\n",
    "    )\n",
    "\n",
    "    combined_columns = [\n",
    "        FilledCylinderCA,\n",
    "        CylinderCA,\n",
    "        FilledCylinderEA,\n",
    "        CylinderEA,\n",
    "        SphereA,\n",
    "        CylinderSMA,\n",
    "        FilledCylinderMA,\n",
    "        FilledCylinderMB,\n",
    "        CylinderSMB,\n",
    "        SphereB,\n",
    "        CylinderEB,\n",
    "        FilledCylinderEB,\n",
    "        CylinderCB,\n",
    "        FilledCylinderCB,\n",
    "        SphereC,\n",
    "        SphereD,\n",
    "        SphereE,\n",
    "    ]\n",
    "    combined_columns = [item for sublist in combined_columns for item in sublist]\n",
    "\n",
    "    # Just append the domains not following any patterns in the front. Mostly domains surrounding sphereA for high spin and mass ratios\n",
    "    combined_columns_set = set(combined_columns)\n",
    "    domain_names_set = set()\n",
    "    for i in domain_names:\n",
    "        domain_names_set.add(i)\n",
    "    subdomains_not_sorted = list(domain_names_set - combined_columns_set)\n",
    "    return subdomains_not_sorted + combined_columns\n",
    "\n",
    "\n",
    "def limit_by_col_val(min_val, max_val, col_name, df):\n",
    "    filter = (df[col_name] >= min_val) & (df[col_name] <= max_val)\n",
    "    return df[filter]\n",
    "\n",
    "\n",
    "def read_dat_file_single_bh(file_name):\n",
    "    # Find the max number of columns\n",
    "    with open(file_name, \"r\") as f:\n",
    "        max_columns = max(len(line.split()) for line in f if not line.startswith(\"#\"))\n",
    "    return pd.read_csv(\n",
    "        file_name,\n",
    "        sep=\"\\s+\",\n",
    "        comment=\"#\",\n",
    "        header=None,\n",
    "        names=[str(i) for i in np.arange(-1, max_columns)],\n",
    "    ).rename(columns={\"-1\": \"t\"})\n",
    "\n",
    "\n",
    "def find_subdomains(path: Path):\n",
    "    subdomain_set = set()\n",
    "    for i in path.iterdir():\n",
    "        if i.is_dir():\n",
    "            subdomain_set.add(i.stem)\n",
    "\n",
    "    return list(subdomain_set)\n",
    "\n",
    "\n",
    "def find_topologies(path: Path):\n",
    "    topologies_set = set()\n",
    "    for i in path.iterdir():\n",
    "        if i.is_file():\n",
    "            topologies_set.add(i.stem.split(\"_\")[0])\n",
    "\n",
    "    return list(topologies_set)\n",
    "\n",
    "\n",
    "def find_dat_file_names(path: Path):\n",
    "    file_name_set = set()\n",
    "    for i in path.iterdir():\n",
    "        if i.is_file():\n",
    "            file_name_set.add(i.stem.split(\"_\")[1])\n",
    "\n",
    "    return list(file_name_set)\n",
    "\n",
    "\n",
    "def get_top_name_and_mode(name):\n",
    "    # Bf0I1(12 modes).dat -> Bf0I1, 12\n",
    "    top_name = name.split(\"(\")[0]\n",
    "    mode = int(name.split(\"(\")[-1].split(\" \")[0])\n",
    "    return top_name, mode\n",
    "\n",
    "\n",
    "def find_highest_modes_for_topologies(path: Path):\n",
    "    highest_mode_dict = {}\n",
    "    for i in path.iterdir():\n",
    "        if i.is_file():\n",
    "            top_name, mode = get_top_name_and_mode(i.stem)\n",
    "            if top_name in highest_mode_dict:\n",
    "                if highest_mode_dict[top_name] < mode:\n",
    "                    highest_mode_dict[top_name] = mode\n",
    "            else:\n",
    "                highest_mode_dict[top_name] = mode\n",
    "\n",
    "    return highest_mode_dict\n",
    "\n",
    "\n",
    "def make_mode_dataframe(path: Path):\n",
    "    highest_mode_dict = find_highest_modes_for_topologies(path)\n",
    "    top_dataframe_list = {i: [] for i in highest_mode_dict}\n",
    "\n",
    "    for i in path.iterdir():\n",
    "        for top_name in highest_mode_dict:\n",
    "            if (top_name + \"(\") in i.stem:\n",
    "                top_dataframe_list[top_name].append(read_dat_file(i))\n",
    "\n",
    "    top_mode_df_dict = {}\n",
    "    for i, df_list in top_dataframe_list.items():\n",
    "        result = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "        # Remove duplicates based on 't' column (keep first occurrence)\n",
    "        # result = result.drop_duplicates(subset='t', keep='first')\n",
    "\n",
    "        # Sort by 't' and reset index\n",
    "        top_mode_df_dict[i] = result.sort_values(\"t\").reset_index(drop=True)\n",
    "    return top_mode_df_dict\n",
    "\n",
    "\n",
    "def filter_columns(\n",
    "    cols: List[str],\n",
    "    include_patterns: List[str] = None,\n",
    "    exclude_patterns: List[str] = None,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Filter a list of column names using include and exclude regex patterns.\n",
    "\n",
    "    Args:\n",
    "        cols: List of column names to filter\n",
    "        include_patterns: List of regex patterns to include (if None, includes all)\n",
    "        exclude_patterns: List of regex patterns to exclude (if None, excludes none)\n",
    "\n",
    "    Returns:\n",
    "        List of filtered column names\n",
    "\n",
    "    Examples:\n",
    "        >>> cols = ['age_2020', 'age_2021', 'height_2020', 'weight_2021']\n",
    "        >>> filter_columns(cols, ['age_.*'], ['.*2021'])\n",
    "        ['age_2020']\n",
    "    \"\"\"\n",
    "    # Handle None inputs\n",
    "    include_patterns = include_patterns or [\".*\"]\n",
    "    exclude_patterns = exclude_patterns or []\n",
    "\n",
    "    # First, get columns that match any include pattern\n",
    "    included_cols = set()\n",
    "    for pattern in include_patterns:\n",
    "        included_cols.update(col for col in cols if re.search(pattern, col))\n",
    "\n",
    "    # Then remove any columns that match exclude patterns\n",
    "    for pattern in exclude_patterns:\n",
    "        included_cols = {col for col in included_cols if not re.search(pattern, col)}\n",
    "\n",
    "    return sorted(list(included_cols))\n",
    "\n",
    "\n",
    "def chain_filter_columns(\n",
    "    cols: List[str],\n",
    "    include_patterns: List[str] = None,\n",
    "    exclude_patterns: List[str] = None,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Filter columns sequentially using chained include and exclude regex patterns.\n",
    "    Each pattern filters from the result of the previous pattern.\n",
    "\n",
    "    Args:\n",
    "        cols: List of column names to filter\n",
    "        include_patterns: List of regex patterns to include (if None, includes all)\n",
    "        exclude_patterns: List of regex patterns to exclude (if None, excludes none)\n",
    "\n",
    "    Returns:\n",
    "        List of filtered column names\n",
    "\n",
    "    Examples:\n",
    "        >>> cols = ['age_2020_q1', 'age_2020_q2', 'age_2021_q1', 'height_2020_q1']\n",
    "        >>> chain_filter_columns(cols, ['age_.*', '.*q1'], ['.*2021.*'])\n",
    "        ['age_2020_q1']\n",
    "    \"\"\"\n",
    "    # Handle None inputs\n",
    "    include_patterns = include_patterns or [\".*\"]\n",
    "    exclude_patterns = exclude_patterns or []\n",
    "\n",
    "    # Start with all columns\n",
    "    filtered_cols = set(cols)\n",
    "\n",
    "    # Apply include patterns sequentially\n",
    "    for pattern in include_patterns:\n",
    "        filtered_cols = {col for col in filtered_cols if re.search(pattern, col)}\n",
    "\n",
    "    # Apply exclude patterns sequentially\n",
    "    for pattern in exclude_patterns:\n",
    "        filtered_cols = {col for col in filtered_cols if not re.search(pattern, col)}\n",
    "\n",
    "    return sorted(list(filtered_cols))\n",
    "\n",
    "\n",
    "def sort_by_coefs_numbers(col_list: List[str]):\n",
    "    with_coef_list = []\n",
    "    without_coef_list = []\n",
    "    for col in col_list:\n",
    "        if \"coef\" not in col:\n",
    "            without_coef_list.append(col)\n",
    "        else:\n",
    "            with_coef_list.append(col)\n",
    "    return without_coef_list + sorted(\n",
    "        with_coef_list, key=lambda x: int(x.split(\"_\")[-1][4:])\n",
    "    )\n",
    "\n",
    "\n",
    "def get_extreme_coef_for_each_domain(df, min_or_max=\"min\"):\n",
    "    col_names = df.columns\n",
    "    subdomains = set([i.split(\"_\")[0] for i in col_names]) - set([\"t(M)\"])\n",
    "    exterme_coef = {\"t(M)\": df[\"t(M)\"]}\n",
    "    for sd in subdomains:\n",
    "        sd_cols = [i for i in col_names if f\"{sd}_\" in i]\n",
    "        if min_or_max == \"max\":\n",
    "            exterme_coef[sd] = df[sd_cols].max(axis=1)\n",
    "        elif min_or_max == \"min\":\n",
    "            exterme_coef[sd] = df[sd_cols].min(axis=1)\n",
    "        else:\n",
    "            raise Exception(\n",
    "                f\"Only supported values of min_or_max are min and max and not {min_or_max=}\"\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(exterme_coef)\n",
    "\n",
    "\n",
    "def load_power_diagonistics(PowDiag_path: Path):\n",
    "    pow_diag_dict = {}\n",
    "    for sd in find_subdomains(PowDiag_path):\n",
    "        pow_diag_dict[sd] = {}\n",
    "        sd_path = PowDiag_path / f\"{sd}.dir\"\n",
    "\n",
    "        psi_pd = make_mode_dataframe(sd_path / f\"Powerpsi.dir\")\n",
    "        kappa_pd = make_mode_dataframe(sd_path / f\"Powerkappa.dir\")\n",
    "        # For each subdomain save things by topology\n",
    "        for top in find_topologies(sd_path):\n",
    "            pow_diag_dict[sd][top] = {}\n",
    "            psi_pd_sorted_cols = sort_by_coefs_numbers(psi_pd[top].columns.to_list())\n",
    "            pow_diag_dict[sd][top][f\"psi_ps\"] = psi_pd[top][psi_pd_sorted_cols]\n",
    "\n",
    "            kappa_pd_sorted_cols = sort_by_coefs_numbers(\n",
    "                kappa_pd[top].columns.to_list()\n",
    "            )\n",
    "            pow_diag_dict[sd][top][f\"kappa_ps\"] = kappa_pd[top][kappa_pd_sorted_cols]\n",
    "\n",
    "            for dat_file in find_dat_file_names(sd_path):\n",
    "                pow_diag_dict[sd][top][f\"{dat_file}\"] = read_dat_file(\n",
    "                    sd_path / f\"{top}_{dat_file}.dat\"\n",
    "                )\n",
    "\n",
    "    return pow_diag_dict\n",
    "\n",
    "\n",
    "def load_power_diagonistics_flat(\n",
    "    PowDiag_path: Path,\n",
    "    reload: bool = False,\n",
    "    return_df: bool = True,\n",
    "    load_dat_files_only: list[str] = None,\n",
    "):\n",
    "    cache_data = PowDiag_path / \"pandas.pkl\"\n",
    "    if cache_data.exists():\n",
    "        if not reload:\n",
    "            with open(cache_data, \"rb\") as f:\n",
    "                pow_diag_dict = pickle.load(f)\n",
    "                print(f\"Loaded from cache {cache_data}\")\n",
    "            return pow_diag_dict\n",
    "\n",
    "    # Same as load_power_diagonistics but no nested dicts. This makes it easy to filter\n",
    "    pow_diag_dict = {}\n",
    "    for sd in find_subdomains(PowDiag_path):\n",
    "        sd_path = PowDiag_path / f\"{sd}.dir\"\n",
    "\n",
    "        for top in find_topologies(sd_path):\n",
    "            for dat_file in find_dat_file_names(sd_path):\n",
    "                if load_dat_files_only is not None:\n",
    "                    for allowed_dat_files in load_dat_files_only:\n",
    "                        if re.search(allowed_dat_files, dat_file) is None:\n",
    "                            continue\n",
    "                        else:\n",
    "                            print(dat_file)\n",
    "                pow_diag_dict[f\"{sd}_{top}_{dat_file}\"] = read_dat_file(\n",
    "                    sd_path / f\"{top}_{dat_file}.dat\"\n",
    "                )\n",
    "        if load_dat_files_only is not None:\n",
    "            print(sd_path)\n",
    "            continue\n",
    "\n",
    "        psi_pd = make_mode_dataframe(sd_path / f\"Powerpsi.dir\")\n",
    "        kappa_pd = make_mode_dataframe(sd_path / f\"Powerkappa.dir\")\n",
    "        # For each subdomain save things by topology\n",
    "        for top in find_topologies(sd_path):\n",
    "            psi_pd_sorted_cols = sort_by_coefs_numbers(psi_pd[top].columns.to_list())\n",
    "            pow_diag_dict[f\"{sd}_{top}_psi_ps\"] = psi_pd[top][psi_pd_sorted_cols]\n",
    "\n",
    "            kappa_pd_sorted_cols = sort_by_coefs_numbers(\n",
    "                kappa_pd[top].columns.to_list()\n",
    "            )\n",
    "            pow_diag_dict[f\"{sd}_{top}_kappa_ps\"] = kappa_pd[top][kappa_pd_sorted_cols]\n",
    "\n",
    "        print(sd_path)\n",
    "\n",
    "    if return_df:\n",
    "        # This can be definitely merged with the stuff above but it's fast enough anyways\n",
    "        flat_dict = {}\n",
    "        flat_dict[\"t\"] = pow_diag_dict[rc(list(pow_diag_dict.keys()))][\"t\"]\n",
    "        for key, item in pow_diag_dict.items():\n",
    "            for col in item.columns:\n",
    "                if \"t\" == col:\n",
    "                    continue\n",
    "                else:\n",
    "                    flat_dict[f\"{key}_{col}\"] = item[col]\n",
    "\n",
    "        flat_df = pd.DataFrame(flat_dict)\n",
    "        with open(cache_data, \"wb\") as f:\n",
    "            pickle.dump(flat_df, f)\n",
    "            print(f\"Cached at {cache_data}\")\n",
    "\n",
    "        return flat_df\n",
    "\n",
    "    return pow_diag_dict\n",
    "\n",
    "\n",
    "def convert_series_to_coeff_df(data, top_num):\n",
    "    irr_top_regex = 0\n",
    "    match top_num:\n",
    "        case 0:\n",
    "            irr_top_regex = r\"Bf0\"  # First top\n",
    "        case 1:\n",
    "            irr_top_regex = (\n",
    "                r\"Bf1(S\\d|B2R)\"  # Second top, S2 for spheres, B2 for filled cylinders\n",
    "            )\n",
    "        case 2:\n",
    "            irr_top_regex = r\"((Bf1S2|Bf1B2_)|Bf2)\"  # Thrid top S2 for spheres, B2 radial for filled cylinders\n",
    "        case _:\n",
    "            raise Exception(f\"{top_num=} should be one of [0,1,2]\")\n",
    "\n",
    "    indices = set(data.index) - set([\"t(M)\"])\n",
    "    indices = set([i for i in indices if re.search(irr_top_regex, i)])\n",
    "\n",
    "    subdomains = set([i.split(\"_\")[0] for i in indices])\n",
    "    # The coefs are sorted?? Do not assume but they are\n",
    "\n",
    "    data_dict = {sd: {} for sd in subdomains}\n",
    "\n",
    "    def coef_number(x):\n",
    "        return int(x.split(\"_\")[-1][4:])\n",
    "\n",
    "    max_coef = 0\n",
    "    for sd in subdomains:\n",
    "        for idx in indices:\n",
    "            if f\"{sd}_\" in idx:\n",
    "                if pd.notna(data[idx]):\n",
    "                    data_dict[sd][coef_number(idx)] = data[idx]\n",
    "        data_dict[sd] = dict(sorted(data_dict[sd].items()))\n",
    "        if len(data_dict[sd]) > max_coef:\n",
    "            max_coef = len(data_dict[sd])\n",
    "\n",
    "    pd_data = pd.DataFrame(data_dict)\n",
    "    return pd_data\n",
    "\n",
    "\n",
    "def series_closest_to_time(t, df):\n",
    "    time_index = np.where(df[\"t(M)\"] > t)[0][0]\n",
    "    time = df[\"t(M)\"][time_index]\n",
    "    return time, df.iloc[time_index].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =================================================================================================\n",
    "# =================================================================================================\n",
    "# PLOTTING\n",
    "# =================================================================================================\n",
    "# =================================================================================================\n",
    "\n",
    "\n",
    "save_folder_path = Path(\"./plots/\").resolve()\n",
    "if not save_folder_path.exists():\n",
    "    raise Exception(f\"Save folder {save_folder_path} does not exist\")\n",
    "\n",
    "# =================================================================================================\n",
    "# Constraints\n",
    "# =================================================================================================\n",
    "\n",
    "L15_main_legend = {\n",
    "    \"high_accuracy_main_L1\": \"Old Level 1\",\n",
    "    \"high_accuracy_main_L2\": \"Old Level 2\",\n",
    "    \"high_accuracy_main_L3\": \"Old Level 3\",\n",
    "    \"high_accuracy_main_L4\": \"Old Level 4\",\n",
    "    \"high_accuracy_main_L5\": \"Old Level 5\",\n",
    "}\n",
    "\n",
    "L15_main_runs = {\n",
    "    \"high_accuracy_main_L1\": \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/Ev/Lev1_A?/Run/\",\n",
    "    \"high_accuracy_main_L2\": \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/Ev/Lev2_A?/Run/\",\n",
    "    \"high_accuracy_main_L3\": \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/Ev/Lev3_A?/Run/\",\n",
    "    \"high_accuracy_main_L4\": \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/Ev/Lev4_A?/Run/\",\n",
    "    \"high_accuracy_main_L5\": \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/Ev/Lev5_A?/Run/\",\n",
    "}\n",
    "\n",
    "L15_ode_fix_legend = {\n",
    "    \"high_accuracy_L1\": \"Ode Fix Level 1\",\n",
    "    \"high_accuracy_L2\": \"Ode Fix Level 2\",\n",
    "    \"high_accuracy_L3\": \"Ode Fix Level 3\",\n",
    "    \"high_accuracy_L4\": \"Ode Fix Level 4\",\n",
    "    \"high_accuracy_L5\": \"Ode Fix Level 5\",\n",
    "}\n",
    "L15_ode_fix_runs = {\n",
    "    \"high_accuracy_L1\": \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev1_A?/Run/\",\n",
    "    \"high_accuracy_L2\": \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev2_A?/Run/\",\n",
    "    \"high_accuracy_L3\": \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev3_A?/Run/\",\n",
    "    \"high_accuracy_L4\": \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev4_A?/Run/\",\n",
    "    \"high_accuracy_L5\": \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/Lev5_A?/Run/\",\n",
    "}\n",
    "\n",
    "\n",
    "L16_set1_legend = {\n",
    "    \"6_set1_L6s1\": \"Set1 Level 1\",\n",
    "    \"6_set1_L6s2\": \"Set1 Level 2\",\n",
    "    \"6_set1_L6s3\": \"Set1 Level 3\",\n",
    "    \"6_set1_L6s4\": \"Set1 Level 4\",\n",
    "    \"6_set1_L6s5\": \"Set1 Level 5\",\n",
    "    \"6_set1_L6s6\": \"Set1 Level 6\",\n",
    "}\n",
    "L16_set1_runs = {\n",
    "    \"6_set1_L6s1\": \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/Ev/Lev1_A?/Run/\",\n",
    "    \"6_set1_L6s2\": \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/Ev/Lev2_A?/Run/\",\n",
    "    \"6_set1_L6s3\": \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/Ev/Lev3_A?/Run/\",\n",
    "    \"6_set1_L6s4\": \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/Ev/Lev4_A?/Run/\",\n",
    "    \"6_set1_L6s5\": \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/Ev/Lev5_A?/Run/\",\n",
    "    \"6_set1_L6s6\": \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/Ev/Lev6_A?/Run/\",\n",
    "}\n",
    "\n",
    "joined_runs = {**L15_main_runs, **L15_ode_fix_runs, **L16_set1_runs}\n",
    "joined_legend = {**L15_main_legend, **L15_ode_fix_legend, **L16_set1_legend}\n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "SKIP_THIS = True\n",
    "\n",
    "runs_to_plot_list = [L15_main_runs, L15_ode_fix_runs, L16_set1_runs]\n",
    "legend_dict_list = [L15_main_legend, L15_ode_fix_legend, L16_set1_legend]\n",
    "runs_set_name_list = [\"L15_main\", \"L15_ode_fix\", \"L16_set1\"]\n",
    "\n",
    "for runs_to_plot, legend_dict, runs_set_name in zip(\n",
    "    runs_to_plot_list, legend_dict_list, runs_set_name_list\n",
    "):\n",
    "    if SKIP_THIS:\n",
    "        continue\n",
    "\n",
    "    data_file_path = \"ConstraintNorms/GhCe_Linf.dat\"\n",
    "    column_names, runs_data_dict = load_data_from_levs(runs_to_plot, data_file_path)\n",
    "\n",
    "    moving_avg_len = 0\n",
    "    save_path = None\n",
    "    diff_base = None\n",
    "    constant_shift_val_time = None\n",
    "    plot_abs_diff = True\n",
    "    y_axis_list = None\n",
    "    x_axis = \"t(M)\"\n",
    "\n",
    "    plot_abs_diff = False\n",
    "\n",
    "    minT = 1205\n",
    "    maxT = 4000\n",
    "\n",
    "    def plot_fun(x, y, label):\n",
    "        return plt.semilogy(x, y, label=label)\n",
    "\n",
    "    append_to_title = \"\"\n",
    "    if \"@\" in data_file_path:\n",
    "        append_to_title = \" HorizonBH=\" + data_file_path.split(\"@\")[-1]\n",
    "\n",
    "    with plt.style.context(\"ggplot\"):\n",
    "        plt.rcParams[\"figure.figsize\"] = (5, 5)\n",
    "        plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "        y_axis = \"Linf(GhCe) on SphereA0\"\n",
    "        plot_graph_for_runs(\n",
    "            runs_data_dict,\n",
    "            x_axis,\n",
    "            y_axis,\n",
    "            minT,\n",
    "            maxT,\n",
    "            legend_dict=legend_dict,\n",
    "            save_path=save_path,\n",
    "            moving_avg_len=moving_avg_len,\n",
    "            plot_fun=plot_fun,\n",
    "            diff_base=diff_base,\n",
    "            plot_abs_diff=plot_abs_diff,\n",
    "            constant_shift_val_time=constant_shift_val_time,\n",
    "            append_to_title=append_to_title,\n",
    "        )\n",
    "\n",
    "        plt.title(\"\")\n",
    "        plt.ylabel(y_axis)\n",
    "        plt.xlabel(\"t(M)\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        #   plt.ylim(1e-8, 1e-5)\n",
    "        #   plt.ylim(1e-12, 1e-6)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_name = save_folder_path / f\"{runs_set_name}_SphereA0_Linf_GhCe.pdf\"\n",
    "        plt.savefig(save_name, dpi=300)\n",
    "        plt.clf()\n",
    "        print(f\"Saved {save_name}!\\n\")\n",
    "\n",
    "        y_axis = \"Linf(GhCe) on SphereC6\"\n",
    "        plot_graph_for_runs(\n",
    "            runs_data_dict,\n",
    "            x_axis,\n",
    "            y_axis,\n",
    "            minT,\n",
    "            maxT,\n",
    "            legend_dict=legend_dict,\n",
    "            save_path=save_path,\n",
    "            moving_avg_len=moving_avg_len,\n",
    "            plot_fun=plot_fun,\n",
    "            diff_base=diff_base,\n",
    "            plot_abs_diff=plot_abs_diff,\n",
    "            constant_shift_val_time=constant_shift_val_time,\n",
    "            append_to_title=append_to_title,\n",
    "        )\n",
    "\n",
    "        plt.title(\"\")\n",
    "        plt.ylabel(y_axis)\n",
    "        plt.xlabel(\"t(M)\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        #   plt.ylim(1e-8, 1e-5)\n",
    "        #   plt.ylim(1e-12, 1e-6)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_name = save_folder_path / f\"{runs_set_name}_SphereC6_Linf_GhCe.pdf\"\n",
    "        plt.savefig(save_name, dpi=300)\n",
    "        print(f\"Saved {save_name}!\\n\")\n",
    "        plt.clf()\n",
    "\n",
    "    # ==============================================================================\n",
    "\n",
    "    data_file_path = \"ConstraintNorms/NormalizedGhCe_Linf.dat\"\n",
    "    column_names, runs_data_dict = load_data_from_levs(runs_to_plot, data_file_path)\n",
    "\n",
    "    moving_avg_len = 0\n",
    "    save_path = None\n",
    "    diff_base = None\n",
    "    constant_shift_val_time = None\n",
    "    plot_abs_diff = True\n",
    "    y_axis_list = None\n",
    "    x_axis = \"t(M)\"\n",
    "\n",
    "    plot_abs_diff = False\n",
    "\n",
    "    minT = 1205\n",
    "    maxT = 4000\n",
    "\n",
    "    def plot_fun(x, y, label):\n",
    "        return plt.semilogy(x, y, label=label)\n",
    "\n",
    "    append_to_title = \"\"\n",
    "    if \"@\" in data_file_path:\n",
    "        append_to_title = \" HorizonBH=\" + data_file_path.split(\"@\")[-1]\n",
    "\n",
    "    with plt.style.context(\"ggplot\"):\n",
    "        plt.rcParams[\"figure.figsize\"] = (5, 5)\n",
    "        plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "        y_axis = \"Linf(NormalizedGhCe) on SphereA0\"\n",
    "        plot_graph_for_runs(\n",
    "            runs_data_dict,\n",
    "            x_axis,\n",
    "            y_axis,\n",
    "            minT,\n",
    "            maxT,\n",
    "            legend_dict=legend_dict,\n",
    "            save_path=save_path,\n",
    "            moving_avg_len=moving_avg_len,\n",
    "            plot_fun=plot_fun,\n",
    "            diff_base=diff_base,\n",
    "            plot_abs_diff=plot_abs_diff,\n",
    "            constant_shift_val_time=constant_shift_val_time,\n",
    "            append_to_title=append_to_title,\n",
    "        )\n",
    "\n",
    "        plt.title(\"\")\n",
    "        plt.ylabel(y_axis)\n",
    "        plt.xlabel(\"t(M)\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        #   plt.ylim(1e-8, 1e-5)\n",
    "        #   plt.ylim(1e-12, 1e-6)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_name = (\n",
    "            save_folder_path / f\"{runs_set_name}_SphereA0_Linf_NormalizedGhCe.pdf\"\n",
    "        )\n",
    "        plt.savefig(save_name, dpi=300)\n",
    "        plt.clf()\n",
    "        print(f\"Saved {save_name}!\\n\")\n",
    "\n",
    "        y_axis = \"Linf(NormalizedGhCe) on SphereC6\"\n",
    "        plot_graph_for_runs(\n",
    "            runs_data_dict,\n",
    "            x_axis,\n",
    "            y_axis,\n",
    "            minT,\n",
    "            maxT,\n",
    "            legend_dict=legend_dict,\n",
    "            save_path=save_path,\n",
    "            moving_avg_len=moving_avg_len,\n",
    "            plot_fun=plot_fun,\n",
    "            diff_base=diff_base,\n",
    "            plot_abs_diff=plot_abs_diff,\n",
    "            constant_shift_val_time=constant_shift_val_time,\n",
    "            append_to_title=append_to_title,\n",
    "        )\n",
    "\n",
    "        plt.title(\"\")\n",
    "        plt.ylabel(y_axis)\n",
    "        plt.xlabel(\"t(M)\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        #   plt.ylim(1e-8, 1e-5)\n",
    "        #   plt.ylim(1e-12, 1e-6)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_name = (\n",
    "            save_folder_path / f\"{runs_set_name}_SphereC6_Linf_NormalizedGhCe.pdf\"\n",
    "        )\n",
    "        plt.savefig(save_name, dpi=300)\n",
    "        print(f\"Saved {save_name}!\\n\")\n",
    "        plt.clf()\n",
    "\n",
    "    # ==============================================================================\n",
    "\n",
    "    data_file_path = \"ConstraintNorms/GhCe_Norms.dat\"\n",
    "    column_names, runs_data_dict = load_data_from_levs(runs_to_plot, data_file_path)\n",
    "\n",
    "    moving_avg_len = 0\n",
    "    save_path = None\n",
    "    diff_base = None\n",
    "    constant_shift_val_time = None\n",
    "    plot_abs_diff = True\n",
    "    y_axis_list = None\n",
    "    x_axis = \"t(M)\"\n",
    "\n",
    "    plot_abs_diff = False\n",
    "\n",
    "    minT = 1205\n",
    "    maxT = 4000\n",
    "\n",
    "    def plot_fun(x, y, label):\n",
    "        return plt.semilogy(x, y, label=label)\n",
    "\n",
    "    append_to_title = \"\"\n",
    "    if \"@\" in data_file_path:\n",
    "        append_to_title = \" HorizonBH=\" + data_file_path.split(\"@\")[-1]\n",
    "\n",
    "    with plt.style.context(\"ggplot\"):\n",
    "        plt.rcParams[\"figure.figsize\"] = (5, 5)\n",
    "        plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "        y_axis = \"L2(GhCe)\"\n",
    "        plot_graph_for_runs(\n",
    "            runs_data_dict,\n",
    "            x_axis,\n",
    "            y_axis,\n",
    "            minT,\n",
    "            maxT,\n",
    "            legend_dict=legend_dict,\n",
    "            save_path=save_path,\n",
    "            moving_avg_len=moving_avg_len,\n",
    "            plot_fun=plot_fun,\n",
    "            diff_base=diff_base,\n",
    "            plot_abs_diff=plot_abs_diff,\n",
    "            constant_shift_val_time=constant_shift_val_time,\n",
    "            append_to_title=append_to_title,\n",
    "        )\n",
    "\n",
    "        plt.title(\"\")\n",
    "        plt.ylabel(y_axis)\n",
    "        plt.xlabel(\"t(M)\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        #   plt.ylim(1e-8, 1e-5)\n",
    "        #   plt.ylim(1e-12, 1e-6)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_name = save_folder_path / f\"{runs_set_name}_L2(GhCe).pdf\"\n",
    "        plt.savefig(save_name, dpi=300)\n",
    "        plt.clf()\n",
    "        print(f\"Saved {save_name}!\\n\")\n",
    "\n",
    "        y_axis = \"Linf(GhCe)\"\n",
    "        plot_graph_for_runs(\n",
    "            runs_data_dict,\n",
    "            x_axis,\n",
    "            y_axis,\n",
    "            minT,\n",
    "            maxT,\n",
    "            legend_dict=legend_dict,\n",
    "            save_path=save_path,\n",
    "            moving_avg_len=moving_avg_len,\n",
    "            plot_fun=plot_fun,\n",
    "            diff_base=diff_base,\n",
    "            plot_abs_diff=plot_abs_diff,\n",
    "            constant_shift_val_time=constant_shift_val_time,\n",
    "            append_to_title=append_to_title,\n",
    "        )\n",
    "\n",
    "        plt.title(\"\")\n",
    "        plt.ylabel(y_axis)\n",
    "        plt.xlabel(\"t(M)\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        #   plt.ylim(1e-8, 1e-5)\n",
    "        #   plt.ylim(1e-12, 1e-6)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_name = save_folder_path / f\"{runs_set_name}_Linf(GhCe).pdf\"\n",
    "        plt.savefig(save_name, dpi=300)\n",
    "        print(f\"Saved {save_name}!\\n\")\n",
    "        plt.clf()\n",
    "\n",
    "        y_axis = \"VolLp(GhCe)\"\n",
    "        plot_graph_for_runs(\n",
    "            runs_data_dict,\n",
    "            x_axis,\n",
    "            y_axis,\n",
    "            minT,\n",
    "            maxT,\n",
    "            legend_dict=legend_dict,\n",
    "            save_path=save_path,\n",
    "            moving_avg_len=moving_avg_len,\n",
    "            plot_fun=plot_fun,\n",
    "            diff_base=diff_base,\n",
    "            plot_abs_diff=plot_abs_diff,\n",
    "            constant_shift_val_time=constant_shift_val_time,\n",
    "            append_to_title=append_to_title,\n",
    "        )\n",
    "\n",
    "        plt.title(\"\")\n",
    "        plt.ylabel(y_axis)\n",
    "        plt.xlabel(\"t(M)\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        #   plt.ylim(1e-8, 1e-5)\n",
    "        #   plt.ylim(1e-12, 1e-6)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_name = save_folder_path / f\"{runs_set_name}_VolLp(GhCe).pdf\"\n",
    "        plt.savefig(save_name, dpi=300)\n",
    "        print(f\"Saved {save_name}!\\n\")\n",
    "        plt.clf()\n",
    "\n",
    "    # ==============================================================================\n",
    "\n",
    "    data_file_path = \"ConstraintNorms/NormalizedGhCe_Norms.dat\"\n",
    "    column_names, runs_data_dict = load_data_from_levs(runs_to_plot, data_file_path)\n",
    "\n",
    "    moving_avg_len = 0\n",
    "    save_path = None\n",
    "    diff_base = None\n",
    "    constant_shift_val_time = None\n",
    "    plot_abs_diff = True\n",
    "    y_axis_list = None\n",
    "    x_axis = \"t(M)\"\n",
    "\n",
    "    plot_abs_diff = False\n",
    "\n",
    "    minT = 1205\n",
    "    maxT = 4000\n",
    "\n",
    "    def plot_fun(x, y, label):\n",
    "        return plt.semilogy(x, y, label=label)\n",
    "\n",
    "    append_to_title = \"\"\n",
    "    if \"@\" in data_file_path:\n",
    "        append_to_title = \" HorizonBH=\" + data_file_path.split(\"@\")[-1]\n",
    "\n",
    "    with plt.style.context(\"ggplot\"):\n",
    "        plt.rcParams[\"figure.figsize\"] = (5, 5)\n",
    "        plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "        y_axis = \"L2(NormalizedGhCe)\"\n",
    "        plot_graph_for_runs(\n",
    "            runs_data_dict,\n",
    "            x_axis,\n",
    "            y_axis,\n",
    "            minT,\n",
    "            maxT,\n",
    "            legend_dict=legend_dict,\n",
    "            save_path=save_path,\n",
    "            moving_avg_len=moving_avg_len,\n",
    "            plot_fun=plot_fun,\n",
    "            diff_base=diff_base,\n",
    "            plot_abs_diff=plot_abs_diff,\n",
    "            constant_shift_val_time=constant_shift_val_time,\n",
    "            append_to_title=append_to_title,\n",
    "        )\n",
    "\n",
    "        plt.title(\"\")\n",
    "        plt.ylabel(y_axis)\n",
    "        plt.xlabel(\"t(M)\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        #   plt.ylim(1e-8, 1e-5)\n",
    "        #   plt.ylim(1e-12, 1e-6)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_name = save_folder_path / f\"{runs_set_name}_L2(NormalizedGhCe).pdf\"\n",
    "        plt.savefig(save_name, dpi=300)\n",
    "        plt.clf()\n",
    "        print(f\"Saved {save_name}!\\n\")\n",
    "\n",
    "        y_axis = \"Linf(NormalizedGhCe)\"\n",
    "        plot_graph_for_runs(\n",
    "            runs_data_dict,\n",
    "            x_axis,\n",
    "            y_axis,\n",
    "            minT,\n",
    "            maxT,\n",
    "            legend_dict=legend_dict,\n",
    "            save_path=save_path,\n",
    "            moving_avg_len=moving_avg_len,\n",
    "            plot_fun=plot_fun,\n",
    "            diff_base=diff_base,\n",
    "            plot_abs_diff=plot_abs_diff,\n",
    "            constant_shift_val_time=constant_shift_val_time,\n",
    "            append_to_title=append_to_title,\n",
    "        )\n",
    "\n",
    "        plt.title(\"\")\n",
    "        plt.ylabel(y_axis)\n",
    "        plt.xlabel(\"t(M)\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        #   plt.ylim(1e-8, 1e-5)\n",
    "        #   plt.ylim(1e-12, 1e-6)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_name = save_folder_path / f\"{runs_set_name}_Linf(NormalizedGhCe).pdf\"\n",
    "        plt.savefig(save_name, dpi=300)\n",
    "        print(f\"Saved {save_name}!\\n\")\n",
    "        plt.clf()\n",
    "\n",
    "        y_axis = \"VolLp(NormalizedGhCe)\"\n",
    "        plot_graph_for_runs(\n",
    "            runs_data_dict,\n",
    "            x_axis,\n",
    "            y_axis,\n",
    "            minT,\n",
    "            maxT,\n",
    "            legend_dict=legend_dict,\n",
    "            save_path=save_path,\n",
    "            moving_avg_len=moving_avg_len,\n",
    "            plot_fun=plot_fun,\n",
    "            diff_base=diff_base,\n",
    "            plot_abs_diff=plot_abs_diff,\n",
    "            constant_shift_val_time=constant_shift_val_time,\n",
    "            append_to_title=append_to_title,\n",
    "        )\n",
    "\n",
    "        plt.title(\"\")\n",
    "        plt.ylabel(y_axis)\n",
    "        plt.xlabel(\"t(M)\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        #   plt.ylim(1e-8, 1e-5)\n",
    "        #   plt.ylim(1e-12, 1e-6)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_name = save_folder_path / f\"{runs_set_name}_VolLp(NormalizedGhCe).pdf\"\n",
    "        plt.savefig(save_name, dpi=300)\n",
    "        print(f\"Saved {save_name}!\\n\")\n",
    "        plt.clf()\n",
    "\n",
    "# =================================================================================================\n",
    "# Individual plots\n",
    "# =================================================================================================\n",
    "\n",
    "SKIP_THIS = False\n",
    "\n",
    "if not SKIP_THIS:\n",
    "    data_file_path = \"ConstraintNorms/NormalizedGhCe_Linf.dat\"\n",
    "    runs_to_plot = {}\n",
    "    runs_to_plot[\"high_accuracy_L5\"] = joined_runs[\"high_accuracy_L5\"]\n",
    "    runs_to_plot[\"6_set1_L6s5\"] = joined_runs[\"6_set1_L6s5\"]\n",
    "    column_names, runs_data_dict = load_data_from_levs(runs_to_plot, data_file_path)\n",
    "\n",
    "    moving_avg_len = 0\n",
    "    save_path = None\n",
    "    diff_base = None\n",
    "    constant_shift_val_time = None\n",
    "    plot_abs_diff = True\n",
    "    y_axis_list = None\n",
    "    x_axis = \"t(M)\"\n",
    "\n",
    "    plot_abs_diff = False\n",
    "\n",
    "    minT = 1205\n",
    "    maxT = 8000\n",
    "\n",
    "    def plot_fun(x, y, label):\n",
    "        return plt.semilogy(x, y, label=label)\n",
    "\n",
    "    append_to_title = \"\"\n",
    "    if \"@\" in data_file_path:\n",
    "        append_to_title = \" HorizonBH=\" + data_file_path.split(\"@\")[-1]\n",
    "\n",
    "    with plt.style.context(\"ggplot\"):\n",
    "        plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "        plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "        y_axis = \"Linf(NormalizedGhCe) on SphereC28\"\n",
    "        plot_graph_for_runs(\n",
    "            runs_data_dict,\n",
    "            x_axis,\n",
    "            y_axis,\n",
    "            minT,\n",
    "            maxT,\n",
    "            legend_dict=joined_legend,\n",
    "            save_path=save_path,\n",
    "            moving_avg_len=moving_avg_len,\n",
    "            plot_fun=plot_fun,\n",
    "            diff_base=diff_base,\n",
    "            plot_abs_diff=plot_abs_diff,\n",
    "            constant_shift_val_time=constant_shift_val_time,\n",
    "            append_to_title=append_to_title,\n",
    "        )\n",
    "\n",
    "        plt.title(\"\")\n",
    "        plt.ylabel(y_axis)\n",
    "        plt.xlabel(\"t(M)\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        #   plt.ylim(1e-8, 1e-5)\n",
    "        #   plt.ylim(1e-12, 1e-6)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_name = (\n",
    "            save_folder_path / \"joined_ML_5_S1_L5_SphereC28_Linf_NormalizedGhCe.pdf\"\n",
    "        )\n",
    "        plt.savefig(save_name, dpi=300)\n",
    "        print(f\"Saved {save_name}!\\n\")\n",
    "        plt.clf()\n",
    "\n",
    "    # ==============================================================================\n",
    "\n",
    "    minT = 1205\n",
    "    maxT = 4000\n",
    "\n",
    "    def plot_fun(x, y, label):\n",
    "        return plt.semilogy(x, y, label=label)\n",
    "\n",
    "    append_to_title = \"\"\n",
    "    if \"@\" in data_file_path:\n",
    "        append_to_title = \" HorizonBH=\" + data_file_path.split(\"@\")[-1]\n",
    "\n",
    "    with plt.style.context(\"ggplot\"):\n",
    "        plt.rcParams[\"figure.figsize\"] = (5, 5)\n",
    "        plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "        y_axis = \"Linf(NormalizedGhCe) on SphereC0\"\n",
    "        plot_graph_for_runs(\n",
    "            runs_data_dict,\n",
    "            x_axis,\n",
    "            y_axis,\n",
    "            minT,\n",
    "            maxT,\n",
    "            legend_dict=joined_legend,\n",
    "            save_path=save_path,\n",
    "            moving_avg_len=moving_avg_len,\n",
    "            plot_fun=plot_fun,\n",
    "            diff_base=diff_base,\n",
    "            plot_abs_diff=plot_abs_diff,\n",
    "            constant_shift_val_time=constant_shift_val_time,\n",
    "            append_to_title=append_to_title,\n",
    "        )\n",
    "\n",
    "        plt.title(\"\")\n",
    "        plt.ylabel(y_axis)\n",
    "        plt.xlabel(\"t(M)\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        #   plt.ylim(1e-8, 1e-5)\n",
    "        #   plt.ylim(1e-12, 1e-6)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_name = (\n",
    "            save_folder_path / \"joined_ML_5_S1_L5_SphereC0_Linf_NormalizedGhCe.pdf\"\n",
    "        )\n",
    "        plt.savefig(save_name, dpi=300)\n",
    "        print(f\"Saved {save_name}!\\n\")\n",
    "        plt.clf()\n",
    "\n",
    "    # ==============================================================================\n",
    "\n",
    "    with plt.style.context(\"ggplot\"):\n",
    "        plt.rcParams[\"figure.figsize\"] = (5, 5)\n",
    "        plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "        y_axis = \"Linf(NormalizedGhCe) on SphereC1\"\n",
    "        plot_graph_for_runs(\n",
    "            runs_data_dict,\n",
    "            x_axis,\n",
    "            y_axis,\n",
    "            minT,\n",
    "            maxT,\n",
    "            legend_dict=joined_legend,\n",
    "            save_path=save_path,\n",
    "            moving_avg_len=moving_avg_len,\n",
    "            plot_fun=plot_fun,\n",
    "            diff_base=diff_base,\n",
    "            plot_abs_diff=plot_abs_diff,\n",
    "            constant_shift_val_time=constant_shift_val_time,\n",
    "            append_to_title=append_to_title,\n",
    "        )\n",
    "\n",
    "        plt.title(\"\")\n",
    "        plt.ylabel(y_axis)\n",
    "        plt.xlabel(\"t(M)\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        #   plt.ylim(1e-8, 1e-5)\n",
    "        #   plt.ylim(1e-12, 1e-6)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_name = (\n",
    "            save_folder_path / \"joined_ML_5_S1_L5_SphereC1_Linf_NormalizedGhCe.pdf\"\n",
    "        )\n",
    "        plt.savefig(save_name, dpi=300)\n",
    "        print(f\"Saved {save_name}!\\n\")\n",
    "        plt.clf()\n",
    "\n",
    "    # ==============================================================================\n",
    "\n",
    "    data_file_path = \"ConstraintNorms/GhCe_Linf.dat\"\n",
    "    runs_to_plot = {}\n",
    "    runs_to_plot[\"high_accuracy_L5\"] = joined_runs[\"high_accuracy_L5\"]\n",
    "    runs_to_plot[\"6_set1_L6s5\"] = joined_runs[\"6_set1_L6s5\"]\n",
    "    column_names, runs_data_dict = load_data_from_levs(runs_to_plot, data_file_path)\n",
    "\n",
    "    moving_avg_len = 0\n",
    "    save_path = None\n",
    "    diff_base = None\n",
    "    constant_shift_val_time = None\n",
    "    plot_abs_diff = True\n",
    "    y_axis_list = None\n",
    "    x_axis = \"t(M)\"\n",
    "\n",
    "    plot_abs_diff = False\n",
    "\n",
    "    minT = 1205\n",
    "    maxT = 8000\n",
    "\n",
    "    def plot_fun(x, y, label):\n",
    "        return plt.semilogy(x, y, label=label)\n",
    "\n",
    "    append_to_title = \"\"\n",
    "    if \"@\" in data_file_path:\n",
    "        append_to_title = \" HorizonBH=\" + data_file_path.split(\"@\")[-1]\n",
    "\n",
    "    with plt.style.context(\"ggplot\"):\n",
    "        plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "        plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "        y_axis = \"Linf(GhCe) on SphereC28\"\n",
    "        plot_graph_for_runs(\n",
    "            runs_data_dict,\n",
    "            x_axis,\n",
    "            y_axis,\n",
    "            minT,\n",
    "            maxT,\n",
    "            legend_dict=joined_legend,\n",
    "            save_path=save_path,\n",
    "            moving_avg_len=moving_avg_len,\n",
    "            plot_fun=plot_fun,\n",
    "            diff_base=diff_base,\n",
    "            plot_abs_diff=plot_abs_diff,\n",
    "            constant_shift_val_time=constant_shift_val_time,\n",
    "            append_to_title=append_to_title,\n",
    "        )\n",
    "\n",
    "        plt.title(\"\")\n",
    "        plt.ylabel(y_axis)\n",
    "        plt.xlabel(\"t(M)\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        #   plt.ylim(1e-8, 1e-5)\n",
    "        #   plt.ylim(1e-12, 1e-6)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_name = save_folder_path / \"joined_ML_5_S1_L5_SphereC28_Linf_GhCe.pdf\"\n",
    "        plt.savefig(save_name, dpi=300)\n",
    "        print(f\"Saved {save_name}!\\n\")\n",
    "        plt.clf()\n",
    "\n",
    "\n",
    "# =================================================================================================\n",
    "# Power spectrum\n",
    "# =================================================================================================\n",
    "\n",
    "L15_main_legend = {\n",
    "    \"high_accuracy_main_L1\": \"Old Level 1\",\n",
    "    \"high_accuracy_main_L2\": \"Old Level 2\",\n",
    "    \"high_accuracy_main_L3\": \"Old Level 3\",\n",
    "    \"high_accuracy_main_L4\": \"Old Level 4\",\n",
    "    \"high_accuracy_main_L5\": \"Old Level 5\",\n",
    "}\n",
    "\n",
    "L15_main_h5_files = {\n",
    "    \"high_accuracy_main_L1\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/h5_files_Lev1\"\n",
    "    ),\n",
    "    \"high_accuracy_main_L2\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/h5_files_Lev2\"\n",
    "    ),\n",
    "    \"high_accuracy_main_L3\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/h5_files_Lev3\"\n",
    "    ),\n",
    "    \"high_accuracy_main_L4\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/h5_files_Lev4\"\n",
    "    ),\n",
    "    \"high_accuracy_main_L5\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35_master/h5_files_Lev5\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "L15_ode_fix_legend = {\n",
    "    \"high_accuracy_L1\": \"Ode Fix Level 1\",\n",
    "    \"high_accuracy_L2\": \"Ode Fix Level 2\",\n",
    "    \"high_accuracy_L3\": \"Ode Fix Level 3\",\n",
    "    \"high_accuracy_L4\": \"Ode Fix Level 4\",\n",
    "    \"high_accuracy_L5\": \"Ode Fix Level 5\",\n",
    "}\n",
    "\n",
    "L15_ode_fix_h5_files = {\n",
    "    \"high_accuracy_L1\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/h5_files_Lev1\"\n",
    "    ),\n",
    "    \"high_accuracy_L2\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/h5_files_Lev2\"\n",
    "    ),\n",
    "    \"high_accuracy_L3\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/h5_files_Lev3\"\n",
    "    ),\n",
    "    \"high_accuracy_L4\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/h5_files_Lev4\"\n",
    "    ),\n",
    "    \"high_accuracy_L5\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/h5_files_Lev5\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "L16_set1_legend = {\n",
    "    \"6_set1_L6s1\": \"Set1 Level 1\",\n",
    "    \"6_set1_L6s2\": \"Set1 Level 2\",\n",
    "    \"6_set1_L6s3\": \"Set1 Level 3\",\n",
    "    \"6_set1_L6s4\": \"Set1 Level 4\",\n",
    "    \"6_set1_L6s5\": \"Set1 Level 5\",\n",
    "    \"6_set1_L6s6\": \"Set1 Level 6\",\n",
    "}\n",
    "\n",
    "L16_set1_h5_files = {\n",
    "    \"6_set1_L6s1\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/h5_files_Lev1\"\n",
    "    ),\n",
    "    \"6_set1_L6s2\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/h5_files_Lev2\"\n",
    "    ),\n",
    "    \"6_set1_L6s3\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/h5_files_Lev3\"\n",
    "    ),\n",
    "    \"6_set1_L6s4\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/h5_files_Lev4\"\n",
    "    ),\n",
    "    \"6_set1_L6s5\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/h5_files_Lev5\"\n",
    "    ),\n",
    "    \"6_set1_L6s6\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/h5_files_Lev6\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "SKIP_THIS = True\n",
    "\n",
    "levs_to_plot_list = [5, 6]\n",
    "domains_to_plot_list = [\"SphereA0\", \"SphereC6\"]\n",
    "vars_to_plot_list = [\"psi\", \"kappa\"]\n",
    "topologies_to_plot_list = [0, 1]\n",
    "runs_set_name_list = [\"L15_main\", \"L15_ode_fix\", \"L16_set1\"]\n",
    "\n",
    "runs_to_plot_list = [L15_main_h5_files, L15_ode_fix_h5_files, L16_set1_h5_files]\n",
    "runs_legend_list = [L15_main_legend, L15_ode_fix_legend, L16_set1_legend]\n",
    "\n",
    "for runs_to_plot, runs_legend, runs_set_name in zip(\n",
    "    # runs_to_plot_list, runs_legend_list, runs_set_name_list\n",
    "    runs_to_plot_list,\n",
    "    runs_legend_list,\n",
    "    runs_set_name_list,\n",
    "):\n",
    "    if SKIP_THIS:\n",
    "        continue\n",
    "\n",
    "    with plt.style.context(\"ggplot\"):\n",
    "        plt.rcParams[\"figure.figsize\"] = (5, 5)\n",
    "        plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "        for h5_path_key, domain, top_num, var in itertools.product(\n",
    "            runs_to_plot,\n",
    "            domains_to_plot_list,\n",
    "            topologies_to_plot_list,\n",
    "            vars_to_plot_list,\n",
    "        ):\n",
    "            h5_path = runs_to_plot[h5_path_key]\n",
    "            current_lev = int(str(h5_path)[-1])\n",
    "            if current_lev not in levs_to_plot_list:\n",
    "                continue\n",
    "\n",
    "            folder_paths = [\n",
    "                Path(\n",
    "                    f\"{h5_path}/extracted-PowerDiagnostics/{domain}.dir/Power{var}.dir\"\n",
    "                )\n",
    "            ]\n",
    "            top_data_dict = {\n",
    "                join_str_with_underscore(\n",
    "                    str(folder_path).split(\"/\")[-5:-3] + [domain]\n",
    "                ): make_mode_dataframe(folder_path)\n",
    "                for folder_path in folder_paths\n",
    "            }\n",
    "\n",
    "            top_name = get_top_name_from_number(top_num, domain)\n",
    "\n",
    "            t_min = 1210\n",
    "            t_max = 4000\n",
    "\n",
    "            min_coef = -1\n",
    "            max_coef = 100\n",
    "            plot_slice = slice(0, None)\n",
    "\n",
    "            title = \"\"\n",
    "            has_more_than_one = len(list(top_data_dict.keys())) > 1\n",
    "            style_list = [\"-\", \":\", \"--\", \"-.\"]\n",
    "            num_legends = 0\n",
    "            single_legend, max_col = True, -1\n",
    "            for key_num, A, key in zip(\n",
    "                range(100), string.ascii_uppercase, top_data_dict\n",
    "            ):\n",
    "                plt.gca().set_prop_cycle(None)\n",
    "                data = top_data_dict[key][top_name]\n",
    "                data = limit_by_col_val(t_min, t_max, \"t\", data)\n",
    "                data = data.dropna(\n",
    "                    axis=1, how=\"all\"\n",
    "                )  # Some columns will have just nans remove those\n",
    "                column_names = data.columns[1:]\n",
    "                visual_data = data[column_names]\n",
    "\n",
    "                cols_to_use = [i for i in data.columns if \"t\" not in i]\n",
    "                # df = np.log10(data[cols_to_use])\n",
    "                df = data[cols_to_use]\n",
    "                df[\"row_min\"] = df.min(axis=1)\n",
    "                df[\"row_max\"] = df.max(axis=1)\n",
    "                df[\"row_mean\"] = df.mean(axis=1)\n",
    "                df[\"row_std\"] = df.std(axis=1)\n",
    "\n",
    "                for i in cols_to_use[plot_slice]:\n",
    "                    coef_num = int(i[4:])\n",
    "                    if coef_num < min_coef or coef_num > max_coef:\n",
    "                        continue\n",
    "                    # plt.plot(data['t'], df[f'{i}'])\n",
    "                    label = f\"$a_{{{i[4:]}}}$\"\n",
    "                    if has_more_than_one:\n",
    "                        if max_col < coef_num:\n",
    "                            label = f\"{A}: {i}\"\n",
    "                            num_legends = num_legends + 1\n",
    "                        else:\n",
    "                            label = None\n",
    "                        max_col = max(coef_num, max_col)\n",
    "                    plt.plot(\n",
    "                        data[\"t\"],\n",
    "                        df[f\"{i}\"],\n",
    "                        label=label,\n",
    "                        linestyle=style_list[key_num % len(style_list)],\n",
    "                    )\n",
    "\n",
    "                if has_more_than_one:\n",
    "                    title = title + f\"{style_list[key_num % len(style_list)]}  {A}: \"\n",
    "                title = title + f\"{domain} of {runs_legend[h5_path_key]} : {top_name}\\n\"\n",
    "\n",
    "            if num_legends > 20:\n",
    "                plt.legend(ncol=int(np.ceil(num_legends / 20)))\n",
    "            else:\n",
    "                plt.legend()\n",
    "\n",
    "            plt.legend(loc=\"upper right\")\n",
    "            plt.title(title[:-1])\n",
    "            plt.xlabel(\"t(M)\")\n",
    "            plt.ylabel(f\"Power {var}\")\n",
    "            # plt.ylim(-16.5, 0.5)\n",
    "            plt.yscale(\"log\")\n",
    "            plt.ylim(5e-17, 5)\n",
    "            plt.grid(False)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            save_name = (\n",
    "                save_folder_path\n",
    "                / f\"{runs_set_name}_L{current_lev}_PS_{var}_{domain}_{top_num}.pdf\"\n",
    "            )\n",
    "            plt.savefig(save_name, dpi=300)\n",
    "            print(f\"Saved {save_name}!\\n\")\n",
    "            plt.clf()\n",
    "\n",
    "\n",
    "# =================================================================================================\n",
    "# CCE bondi constraints\n",
    "# =================================================================================================\n",
    "\n",
    "L15_ode_fix_legend = {\n",
    "    \"high_accuracy_L1\": \"Ode Fix Level 1\",\n",
    "    \"high_accuracy_L2\": \"Ode Fix Level 2\",\n",
    "    \"high_accuracy_L3\": \"Ode Fix Level 3\",\n",
    "    \"high_accuracy_L4\": \"Ode Fix Level 4\",\n",
    "    \"high_accuracy_L5\": \"Ode Fix Level 5\",\n",
    "}\n",
    "\n",
    "L15_ode_fix_cce_files = {\n",
    "    \"high_accuracy_L1\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/Lev01_test/new_ode_tol/high_accuracy_L35/Ev/GW_data_lev1/BondiCceR0257/red_cce.h5\"\n",
    "    ),\n",
    "    \"high_accuracy_L2\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/Lev01_test/new_ode_tol/high_accuracy_L35/Ev/GW_data_lev2/BondiCceR0257/red_cce.h5\"\n",
    "    ),\n",
    "    \"high_accuracy_L3\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/GW_data_lev3/BondiCceR0258/red_cce.h5\"\n",
    "    ),\n",
    "    \"high_accuracy_L4\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/GW_data_lev4/BondiCceR0258/red_cce.h5\"\n",
    "    ),\n",
    "    \"high_accuracy_L5\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/high_accuracy_L35/Ev/GW_data_lev5/BondiCceR0258/red_cce.h5\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "L16_set1_legend = {\n",
    "    \"6_set1_L6s1\": \"Set1 Level 1\",\n",
    "    \"6_set1_L6s2\": \"Set1 Level 2\",\n",
    "    \"6_set1_L6s3\": \"Set1 Level 3\",\n",
    "    \"6_set1_L6s4\": \"Set1 Level 4\",\n",
    "    \"6_set1_L6s5\": \"Set1 Level 5\",\n",
    "    \"6_set1_L6s6\": \"Set1 Level 6\",\n",
    "}\n",
    "\n",
    "L16_set1_cce_files = {\n",
    "    \"6_set1_L6s1\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/GW_data_lev1/BondiCceR0250/red_cce.h5\"\n",
    "    ),\n",
    "    \"6_set1_L6s2\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/GW_data_lev2/BondiCceR0250/red_cce.h5\"\n",
    "    ),\n",
    "    \"6_set1_L6s3\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/GW_data_lev3/BondiCceR0250/red_cce.h5\"\n",
    "    ),\n",
    "    \"6_set1_L6s4\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/GW_data_lev4/BondiCceR0250/red_cce.h5\"\n",
    "    ),\n",
    "    \"6_set1_L6s5\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/GW_data_lev5/BondiCceR0250/red_cce.h5\"\n",
    "    ),\n",
    "    \"6_set1_L6s6\": Path(\n",
    "        \"/groups/sxs/hchaudha/spec_runs/6_segs/6_set1_L6/GW_data_lev6/BondiCceR0250/red_cce.h5\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "SKIP_THIS = True\n",
    "\n",
    "bondi_norms_to_plot = [2, 4, 5]\n",
    "runs_set_name_list = [\"L15_ode_fix\", \"L16_set1\"]\n",
    "\n",
    "runs_to_plot_list = [L15_ode_fix_cce_files, L16_set1_cce_files]\n",
    "runs_legend_list = [L15_ode_fix_legend, L16_set1_legend]\n",
    "\n",
    "for runs_to_plot, runs_legend, runs_set_name in zip(\n",
    "    runs_to_plot_list,\n",
    "    runs_legend_list,\n",
    "    runs_set_name_list,\n",
    "):\n",
    "    if SKIP_THIS:\n",
    "        continue\n",
    "\n",
    "    t_interpolate = np.linspace(-1000, 100000, num=2000)\n",
    "    abd_data = {}\n",
    "    for key in runs_to_plot:\n",
    "        abd_data[key] = load_and_pickle(\n",
    "            runs_to_plot[key], options={\"t_interpolate\": t_interpolate}\n",
    "        )\n",
    "        abd_data[key] = load_bondi_constraints(runs_to_plot[key])\n",
    "    print(abd_data.keys())\n",
    "\n",
    "    with plt.style.context(\"ggplot\"):\n",
    "        plt.rcParams[\"figure.figsize\"] = (5, 5)\n",
    "        plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "        for bondi_norm in bondi_norms_to_plot:\n",
    "            t_min = 1210 - 260\n",
    "            t_max = 4000 - 260\n",
    "\n",
    "            for key in abd_data:\n",
    "                violation_dict = abd_data[key][\"bondi_violation_norms\"]\n",
    "\n",
    "                t_arr = abd_data[key][\"abd\"].t\n",
    "                trimmed_indices = (t_arr > t_min) & (t_arr < t_max)\n",
    "                t_arr = t_arr[trimmed_indices]\n",
    "\n",
    "                plt.semilogy(\n",
    "                    t_arr,\n",
    "                    violation_dict[bondi_norm][trimmed_indices],\n",
    "                    label=f\"{runs_legend[key]}\",\n",
    "                )\n",
    "\n",
    "            plt.legend(loc=\"upper right\")\n",
    "            plt.xlabel(\"t(M)\")\n",
    "            plt.ylabel(f\"Bondi violations {bondi_norm}\")\n",
    "            # plt.grid(False)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            save_name = (\n",
    "                save_folder_path / f\"{runs_set_name}_cce_boncon_{bondi_norm}.pdf\"\n",
    "            )\n",
    "            plt.savefig(save_name, dpi=300)\n",
    "            print(f\"Saved {save_name}!\\n\")\n",
    "            plt.clf()\n",
    "\n",
    "\n",
    "# =================================================================================================\n",
    "# CCE bondi constraints radius dependence\n",
    "# =================================================================================================\n",
    "cce_data = {}\n",
    "levs = [6]\n",
    "# levs = [0,1,2,3]\n",
    "# levs = [5]\n",
    "# levs = [5,6]\n",
    "run_sets = [1]\n",
    "# radius = [250]\n",
    "radius = [100, 150, 200, 250, 300, 350, 500, 700, 900]\n",
    "# radius = [150,200,250,300,350,500,700]\n",
    "# radius = [200,250,300,350,500]\n",
    "for l, s, r in itertools.product(levs, run_sets, radius):\n",
    "    if s == 2 and (l == 0 or l == 1):\n",
    "        continue\n",
    "    if l <= 3:\n",
    "        if s == 1:\n",
    "            cce_data[f\"6_set{s}_L6s{l}_{r}\"] = Path(\n",
    "                f\"/groups/sxs/hchaudha/spec_runs/6_segs/6_set{s}_L6/GW_data_lev{l}/BondiCceR0{r}/red_cce.h5\"\n",
    "            )\n",
    "        # cce_data[f\"6_set{s}_L3s{l}_{r}\"] = Path(f\"/groups/sxs/hchaudha/spec_runs/6_segs/6_set{s}_L3/GW_data_lev{l}/BondiCceR0{r}/red_cce.h5\")\n",
    "    else:\n",
    "        cce_data[f\"6_set{s}_L6s{l}_{r}\"] = Path(\n",
    "            f\"/groups/sxs/hchaudha/spec_runs/6_segs/6_set{s}_L6/GW_data_lev{l}/BondiCceR0{r}/red_cce.h5\"\n",
    "        )\n",
    "        pass\n",
    "# ==============================================================================\n",
    "\n",
    "SKIP_THIS = True\n",
    "\n",
    "bondi_norms_to_plot = [2, 4, 5]\n",
    "runs_set_name_list = [\"L15_ode_fix\", \"L16_set1\"]\n",
    "\n",
    "runs_to_plot_list = [L15_ode_fix_cce_files, L16_set1_cce_files]\n",
    "runs_legend_list = [L15_ode_fix_legend, L16_set1_legend]\n",
    "\n",
    "for runs_to_plot, runs_legend, runs_set_name in zip(\n",
    "    runs_to_plot_list,\n",
    "    runs_legend_list,\n",
    "    runs_set_name_list,\n",
    "):\n",
    "    if SKIP_THIS:\n",
    "        continue\n",
    "\n",
    "    t_interpolate = np.linspace(-1000, 100000, num=2000)\n",
    "    abd_data = {}\n",
    "    for key in runs_to_plot:\n",
    "        abd_data[key] = load_and_pickle(\n",
    "            runs_to_plot[key], options={\"t_interpolate\": t_interpolate}\n",
    "        )\n",
    "        abd_data[key] = load_bondi_constraints(runs_to_plot[key])\n",
    "    print(abd_data.keys())\n",
    "\n",
    "    with plt.style.context(\"ggplot\"):\n",
    "        plt.rcParams[\"figure.figsize\"] = (5, 5)\n",
    "        plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "        for bondi_norm in bondi_norms_to_plot:\n",
    "            t_min = 1210 - 260\n",
    "            t_max = 4000 - 260\n",
    "\n",
    "            for key in abd_data:\n",
    "                violation_dict = abd_data[key][\"bondi_violation_norms\"]\n",
    "\n",
    "                t_arr = abd_data[key][\"abd\"].t\n",
    "                trimmed_indices = (t_arr > t_min) & (t_arr < t_max)\n",
    "                t_arr = t_arr[trimmed_indices]\n",
    "\n",
    "                plt.semilogy(\n",
    "                    t_arr,\n",
    "                    violation_dict[bondi_norm][trimmed_indices],\n",
    "                    label=f\"{runs_legend[key]}\",\n",
    "                )\n",
    "\n",
    "            plt.legend(loc=\"upper right\")\n",
    "            plt.xlabel(\"t(M)\")\n",
    "            plt.ylabel(f\"Bondi violations {bondi_norm}\")\n",
    "            # plt.grid(False)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            save_name = (\n",
    "                save_folder_path / f\"{runs_set_name}_cce_boncon_{bondi_norm}.pdf\"\n",
    "            )\n",
    "            plt.savefig(save_name, dpi=300)\n",
    "            print(f\"Saved {save_name}!\\n\")\n",
    "            plt.clf()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sxs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
